// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v3.21.12
// source: yandex/cloud/ai/batch_inference/v1/batch_inference_service.proto

package fomo

import (
	_ "github.com/yandex-cloud/go-genproto/yandex/cloud"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type BatchInferenceMetadata struct {
	state            protoimpl.MessageState    `protogen:"open.v1"`
	TaskId           string                    `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	TaskStatus       BatchInferenceTask_Status `protobuf:"varint,2,opt,name=task_status,json=taskStatus,proto3,enum=yandex.cloud.ai.batch_inference.v1.BatchInferenceTask_Status" json:"task_status,omitempty"`
	CompletedBatches int64                     `protobuf:"varint,3,opt,name=completed_batches,json=completedBatches,proto3" json:"completed_batches,omitempty"`
	TotalBatches     int64                     `protobuf:"varint,4,opt,name=total_batches,json=totalBatches,proto3" json:"total_batches,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *BatchInferenceMetadata) Reset() {
	*x = BatchInferenceMetadata{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceMetadata) ProtoMessage() {}

func (x *BatchInferenceMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceMetadata.ProtoReflect.Descriptor instead.
func (*BatchInferenceMetadata) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{0}
}

func (x *BatchInferenceMetadata) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

func (x *BatchInferenceMetadata) GetTaskStatus() BatchInferenceTask_Status {
	if x != nil {
		return x.TaskStatus
	}
	return BatchInferenceTask_STATUS_UNSPECIFIED
}

func (x *BatchInferenceMetadata) GetCompletedBatches() int64 {
	if x != nil {
		return x.CompletedBatches
	}
	return 0
}

func (x *BatchInferenceMetadata) GetTotalBatches() int64 {
	if x != nil {
		return x.TotalBatches
	}
	return 0
}

type BatchInferenceResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Task          *BatchInferenceTask    `protobuf:"bytes,1,opt,name=task,proto3" json:"task,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchInferenceResponse) Reset() {
	*x = BatchInferenceResponse{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceResponse) ProtoMessage() {}

func (x *BatchInferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceResponse.ProtoReflect.Descriptor instead.
func (*BatchInferenceResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{1}
}

func (x *BatchInferenceResponse) GetTask() *BatchInferenceTask {
	if x != nil {
		return x.Task
	}
	return nil
}

type DescribeBatchInferenceRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required task id
	TaskId        string `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DescribeBatchInferenceRequest) Reset() {
	*x = DescribeBatchInferenceRequest{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DescribeBatchInferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DescribeBatchInferenceRequest) ProtoMessage() {}

func (x *DescribeBatchInferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DescribeBatchInferenceRequest.ProtoReflect.Descriptor instead.
func (*DescribeBatchInferenceRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{2}
}

func (x *DescribeBatchInferenceRequest) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

type DescribeBatchInferenceResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Task          *BatchInferenceTask    `protobuf:"bytes,1,opt,name=task,proto3" json:"task,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DescribeBatchInferenceResponse) Reset() {
	*x = DescribeBatchInferenceResponse{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DescribeBatchInferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DescribeBatchInferenceResponse) ProtoMessage() {}

func (x *DescribeBatchInferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DescribeBatchInferenceResponse.ProtoReflect.Descriptor instead.
func (*DescribeBatchInferenceResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{3}
}

func (x *DescribeBatchInferenceResponse) GetTask() *BatchInferenceTask {
	if x != nil {
		return x.Task
	}
	return nil
}

type ListBatchInferencesRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Folder ID for which the list of tasks will be provided.
	FolderId  string `protobuf:"bytes,1,opt,name=folder_id,json=folderId,proto3" json:"folder_id,omitempty"`
	PageSize  int64  `protobuf:"varint,2,opt,name=page_size,json=pageSize,proto3" json:"page_size,omitempty"`
	PageToken string `protobuf:"bytes,3,opt,name=page_token,json=pageToken,proto3" json:"page_token,omitempty"`
	// Batch inference status for filtering
	Status        BatchInferenceTask_Status `protobuf:"varint,4,opt,name=status,proto3,enum=yandex.cloud.ai.batch_inference.v1.BatchInferenceTask_Status" json:"status,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListBatchInferencesRequest) Reset() {
	*x = ListBatchInferencesRequest{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListBatchInferencesRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListBatchInferencesRequest) ProtoMessage() {}

func (x *ListBatchInferencesRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListBatchInferencesRequest.ProtoReflect.Descriptor instead.
func (*ListBatchInferencesRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{4}
}

func (x *ListBatchInferencesRequest) GetFolderId() string {
	if x != nil {
		return x.FolderId
	}
	return ""
}

func (x *ListBatchInferencesRequest) GetPageSize() int64 {
	if x != nil {
		return x.PageSize
	}
	return 0
}

func (x *ListBatchInferencesRequest) GetPageToken() string {
	if x != nil {
		return x.PageToken
	}
	return ""
}

func (x *ListBatchInferencesRequest) GetStatus() BatchInferenceTask_Status {
	if x != nil {
		return x.Status
	}
	return BatchInferenceTask_STATUS_UNSPECIFIED
}

type ListBatchInferencesResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Tasks         []*BatchInferenceTask  `protobuf:"bytes,1,rep,name=tasks,proto3" json:"tasks,omitempty"`
	NextPageToken string                 `protobuf:"bytes,2,opt,name=next_page_token,json=nextPageToken,proto3" json:"next_page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListBatchInferencesResponse) Reset() {
	*x = ListBatchInferencesResponse{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListBatchInferencesResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListBatchInferencesResponse) ProtoMessage() {}

func (x *ListBatchInferencesResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListBatchInferencesResponse.ProtoReflect.Descriptor instead.
func (*ListBatchInferencesResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{5}
}

func (x *ListBatchInferencesResponse) GetTasks() []*BatchInferenceTask {
	if x != nil {
		return x.Tasks
	}
	return nil
}

func (x *ListBatchInferencesResponse) GetNextPageToken() string {
	if x != nil {
		return x.NextPageToken
	}
	return ""
}

type CancelBatchInferenceRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Task ID that should be canceled.
	TaskId        string `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CancelBatchInferenceRequest) Reset() {
	*x = CancelBatchInferenceRequest{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CancelBatchInferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CancelBatchInferenceRequest) ProtoMessage() {}

func (x *CancelBatchInferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CancelBatchInferenceRequest.ProtoReflect.Descriptor instead.
func (*CancelBatchInferenceRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{6}
}

func (x *CancelBatchInferenceRequest) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

type CancelBatchInferenceResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	TaskId        string                 `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CancelBatchInferenceResponse) Reset() {
	*x = CancelBatchInferenceResponse{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CancelBatchInferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CancelBatchInferenceResponse) ProtoMessage() {}

func (x *CancelBatchInferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CancelBatchInferenceResponse.ProtoReflect.Descriptor instead.
func (*CancelBatchInferenceResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{7}
}

func (x *CancelBatchInferenceResponse) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

type DeleteBatchInferenceRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Task ID that should be deleted.
	TaskId        string `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DeleteBatchInferenceRequest) Reset() {
	*x = DeleteBatchInferenceRequest{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeleteBatchInferenceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeleteBatchInferenceRequest) ProtoMessage() {}

func (x *DeleteBatchInferenceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeleteBatchInferenceRequest.ProtoReflect.Descriptor instead.
func (*DeleteBatchInferenceRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{8}
}

func (x *DeleteBatchInferenceRequest) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

type DeleteBatchInferenceResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	TaskId        string                 `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DeleteBatchInferenceResponse) Reset() {
	*x = DeleteBatchInferenceResponse{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeleteBatchInferenceResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeleteBatchInferenceResponse) ProtoMessage() {}

func (x *DeleteBatchInferenceResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeleteBatchInferenceResponse.ProtoReflect.Descriptor instead.
func (*DeleteBatchInferenceResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP(), []int{9}
}

func (x *DeleteBatchInferenceResponse) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

var File_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto protoreflect.FileDescriptor

const file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDesc = "" +
	"\n" +
	"@yandex/cloud/ai/batch_inference/v1/batch_inference_service.proto\x12\"yandex.cloud.ai.batch_inference.v1\x1a\x1dyandex/cloud/validation.proto\x1a=yandex/cloud/ai/batch_inference/v1/batch_inference_task.proto\"\xe3\x01\n" +
	"\x16BatchInferenceMetadata\x12\x17\n" +
	"\atask_id\x18\x01 \x01(\tR\x06taskId\x12^\n" +
	"\vtask_status\x18\x02 \x01(\x0e2=.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.StatusR\n" +
	"taskStatus\x12+\n" +
	"\x11completed_batches\x18\x03 \x01(\x03R\x10completedBatches\x12#\n" +
	"\rtotal_batches\x18\x04 \x01(\x03R\ftotalBatches\"d\n" +
	"\x16BatchInferenceResponse\x12J\n" +
	"\x04task\x18\x01 \x01(\v26.yandex.cloud.ai.batch_inference.v1.BatchInferenceTaskR\x04task\">\n" +
	"\x1dDescribeBatchInferenceRequest\x12\x1d\n" +
	"\atask_id\x18\x01 \x01(\tB\x04\xe8\xc71\x01R\x06taskId\"l\n" +
	"\x1eDescribeBatchInferenceResponse\x12J\n" +
	"\x04task\x18\x01 \x01(\v26.yandex.cloud.ai.batch_inference.v1.BatchInferenceTaskR\x04task\"\xd2\x01\n" +
	"\x1aListBatchInferencesRequest\x12!\n" +
	"\tfolder_id\x18\x01 \x01(\tB\x04\xe8\xc71\x01R\bfolderId\x12\x1b\n" +
	"\tpage_size\x18\x02 \x01(\x03R\bpageSize\x12\x1d\n" +
	"\n" +
	"page_token\x18\x03 \x01(\tR\tpageToken\x12U\n" +
	"\x06status\x18\x04 \x01(\x0e2=.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.StatusR\x06status\"\x93\x01\n" +
	"\x1bListBatchInferencesResponse\x12L\n" +
	"\x05tasks\x18\x01 \x03(\v26.yandex.cloud.ai.batch_inference.v1.BatchInferenceTaskR\x05tasks\x12&\n" +
	"\x0fnext_page_token\x18\x02 \x01(\tR\rnextPageToken\"<\n" +
	"\x1bCancelBatchInferenceRequest\x12\x1d\n" +
	"\atask_id\x18\x01 \x01(\tB\x04\xe8\xc71\x01R\x06taskId\"7\n" +
	"\x1cCancelBatchInferenceResponse\x12\x17\n" +
	"\atask_id\x18\x01 \x01(\tR\x06taskId\"<\n" +
	"\x1bDeleteBatchInferenceRequest\x12\x1d\n" +
	"\atask_id\x18\x01 \x01(\tB\x04\xe8\xc71\x01R\x06taskId\"7\n" +
	"\x1cDeleteBatchInferenceResponse\x12\x17\n" +
	"\atask_id\x18\x01 \x01(\tR\x06taskId2\xd1\x04\n" +
	"\x15BatchInferenceService\x12\x91\x01\n" +
	"\bDescribe\x12A.yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceRequest\x1aB.yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceResponse\x12\x87\x01\n" +
	"\x04List\x12>.yandex.cloud.ai.batch_inference.v1.ListBatchInferencesRequest\x1a?.yandex.cloud.ai.batch_inference.v1.ListBatchInferencesResponse\x12\x8b\x01\n" +
	"\x06Cancel\x12?.yandex.cloud.ai.batch_inference.v1.CancelBatchInferenceRequest\x1a@.yandex.cloud.ai.batch_inference.v1.CancelBatchInferenceResponse\x12\x8b\x01\n" +
	"\x06Delete\x12?.yandex.cloud.ai.batch_inference.v1.DeleteBatchInferenceRequest\x1a@.yandex.cloud.ai.batch_inference.v1.DeleteBatchInferenceResponseBu\n" +
	"&yandex.cloud.api.ai.batch_inference.v1ZKgithub.com/yandex-cloud/go-genproto/yandex/cloud/ai/batch_inference/v1;fomob\x06proto3"

var (
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescOnce sync.Once
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescData []byte
)

func file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescGZIP() []byte {
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescOnce.Do(func() {
		file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDesc), len(file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDesc)))
	})
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDescData
}

var file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes = make([]protoimpl.MessageInfo, 10)
var file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_goTypes = []any{
	(*BatchInferenceMetadata)(nil),         // 0: yandex.cloud.ai.batch_inference.v1.BatchInferenceMetadata
	(*BatchInferenceResponse)(nil),         // 1: yandex.cloud.ai.batch_inference.v1.BatchInferenceResponse
	(*DescribeBatchInferenceRequest)(nil),  // 2: yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceRequest
	(*DescribeBatchInferenceResponse)(nil), // 3: yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceResponse
	(*ListBatchInferencesRequest)(nil),     // 4: yandex.cloud.ai.batch_inference.v1.ListBatchInferencesRequest
	(*ListBatchInferencesResponse)(nil),    // 5: yandex.cloud.ai.batch_inference.v1.ListBatchInferencesResponse
	(*CancelBatchInferenceRequest)(nil),    // 6: yandex.cloud.ai.batch_inference.v1.CancelBatchInferenceRequest
	(*CancelBatchInferenceResponse)(nil),   // 7: yandex.cloud.ai.batch_inference.v1.CancelBatchInferenceResponse
	(*DeleteBatchInferenceRequest)(nil),    // 8: yandex.cloud.ai.batch_inference.v1.DeleteBatchInferenceRequest
	(*DeleteBatchInferenceResponse)(nil),   // 9: yandex.cloud.ai.batch_inference.v1.DeleteBatchInferenceResponse
	(BatchInferenceTask_Status)(0),         // 10: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.Status
	(*BatchInferenceTask)(nil),             // 11: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask
}
var file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_depIdxs = []int32{
	10, // 0: yandex.cloud.ai.batch_inference.v1.BatchInferenceMetadata.task_status:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.Status
	11, // 1: yandex.cloud.ai.batch_inference.v1.BatchInferenceResponse.task:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask
	11, // 2: yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceResponse.task:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask
	10, // 3: yandex.cloud.ai.batch_inference.v1.ListBatchInferencesRequest.status:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.Status
	11, // 4: yandex.cloud.ai.batch_inference.v1.ListBatchInferencesResponse.tasks:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask
	2,  // 5: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.Describe:input_type -> yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceRequest
	4,  // 6: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.List:input_type -> yandex.cloud.ai.batch_inference.v1.ListBatchInferencesRequest
	6,  // 7: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.Cancel:input_type -> yandex.cloud.ai.batch_inference.v1.CancelBatchInferenceRequest
	8,  // 8: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.Delete:input_type -> yandex.cloud.ai.batch_inference.v1.DeleteBatchInferenceRequest
	3,  // 9: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.Describe:output_type -> yandex.cloud.ai.batch_inference.v1.DescribeBatchInferenceResponse
	5,  // 10: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.List:output_type -> yandex.cloud.ai.batch_inference.v1.ListBatchInferencesResponse
	7,  // 11: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.Cancel:output_type -> yandex.cloud.ai.batch_inference.v1.CancelBatchInferenceResponse
	9,  // 12: yandex.cloud.ai.batch_inference.v1.BatchInferenceService.Delete:output_type -> yandex.cloud.ai.batch_inference.v1.DeleteBatchInferenceResponse
	9,  // [9:13] is the sub-list for method output_type
	5,  // [5:9] is the sub-list for method input_type
	5,  // [5:5] is the sub-list for extension type_name
	5,  // [5:5] is the sub-list for extension extendee
	0,  // [0:5] is the sub-list for field type_name
}

func init() { file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_init() }
func file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_init() {
	if File_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto != nil {
		return
	}
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_init()
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDesc), len(file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   10,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_goTypes,
		DependencyIndexes: file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_depIdxs,
		MessageInfos:      file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_msgTypes,
	}.Build()
	File_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto = out.File
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_goTypes = nil
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_service_proto_depIdxs = nil
}

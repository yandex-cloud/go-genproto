// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v3.21.12
// source: yandex/cloud/ai/batch_inference/v1/batch_inference_task.proto

package fomo

import (
	status "google.golang.org/genproto/googleapis/rpc/status"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type BatchInferenceTask_Status int32

const (
	BatchInferenceTask_STATUS_UNSPECIFIED BatchInferenceTask_Status = 0
	BatchInferenceTask_CREATED            BatchInferenceTask_Status = 1
	BatchInferenceTask_PENDING            BatchInferenceTask_Status = 2
	BatchInferenceTask_IN_PROGRESS        BatchInferenceTask_Status = 3
	BatchInferenceTask_COMPLETED          BatchInferenceTask_Status = 4
	BatchInferenceTask_FAILED             BatchInferenceTask_Status = 5
	BatchInferenceTask_CANCELED           BatchInferenceTask_Status = 6
)

// Enum value maps for BatchInferenceTask_Status.
var (
	BatchInferenceTask_Status_name = map[int32]string{
		0: "STATUS_UNSPECIFIED",
		1: "CREATED",
		2: "PENDING",
		3: "IN_PROGRESS",
		4: "COMPLETED",
		5: "FAILED",
		6: "CANCELED",
	}
	BatchInferenceTask_Status_value = map[string]int32{
		"STATUS_UNSPECIFIED": 0,
		"CREATED":            1,
		"PENDING":            2,
		"IN_PROGRESS":        3,
		"COMPLETED":          4,
		"FAILED":             5,
		"CANCELED":           6,
	}
)

func (x BatchInferenceTask_Status) Enum() *BatchInferenceTask_Status {
	p := new(BatchInferenceTask_Status)
	*p = x
	return p
}

func (x BatchInferenceTask_Status) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (BatchInferenceTask_Status) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_enumTypes[0].Descriptor()
}

func (BatchInferenceTask_Status) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_enumTypes[0]
}

func (x BatchInferenceTask_Status) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use BatchInferenceTask_Status.Descriptor instead.
func (BatchInferenceTask_Status) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescGZIP(), []int{0, 0}
}

type BatchInferenceTask struct {
	state           protoimpl.MessageState `protogen:"open.v1"`
	TaskId          string                 `protobuf:"bytes,1,opt,name=task_id,json=taskId,proto3" json:"task_id,omitempty"`
	OperationId     string                 `protobuf:"bytes,2,opt,name=operation_id,json=operationId,proto3" json:"operation_id,omitempty"`
	FolderId        string                 `protobuf:"bytes,3,opt,name=folder_id,json=folderId,proto3" json:"folder_id,omitempty"`
	ModelUri        string                 `protobuf:"bytes,4,opt,name=model_uri,json=modelUri,proto3" json:"model_uri,omitempty"`
	SourceDatasetId string                 `protobuf:"bytes,5,opt,name=source_dataset_id,json=sourceDatasetId,proto3" json:"source_dataset_id,omitempty"`
	// Types that are valid to be assigned to Request:
	//
	//	*BatchInferenceTask_CompletionRequest
	Request         isBatchInferenceTask_Request   `protobuf_oneof:"request"`
	Status          BatchInferenceTask_Status      `protobuf:"varint,7,opt,name=status,proto3,enum=yandex.cloud.ai.batch_inference.v1.BatchInferenceTask_Status" json:"status,omitempty"`
	ResultDatasetId string                         `protobuf:"bytes,8,opt,name=result_dataset_id,json=resultDatasetId,proto3" json:"result_dataset_id,omitempty"`
	Labels          map[string]string              `protobuf:"bytes,9,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	CreatedBy       string                         `protobuf:"bytes,10,opt,name=created_by,json=createdBy,proto3" json:"created_by,omitempty"`
	CreatedAt       *timestamppb.Timestamp         `protobuf:"bytes,11,opt,name=created_at,json=createdAt,proto3" json:"created_at,omitempty"`
	StartedAt       *timestamppb.Timestamp         `protobuf:"bytes,12,opt,name=started_at,json=startedAt,proto3" json:"started_at,omitempty"`
	FinishedAt      *timestamppb.Timestamp         `protobuf:"bytes,13,opt,name=finished_at,json=finishedAt,proto3" json:"finished_at,omitempty"`
	Errors          *BatchInferenceTask_ErrorsInfo `protobuf:"bytes,14,opt,name=errors,proto3" json:"errors,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *BatchInferenceTask) Reset() {
	*x = BatchInferenceTask{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceTask) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceTask) ProtoMessage() {}

func (x *BatchInferenceTask) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceTask.ProtoReflect.Descriptor instead.
func (*BatchInferenceTask) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescGZIP(), []int{0}
}

func (x *BatchInferenceTask) GetTaskId() string {
	if x != nil {
		return x.TaskId
	}
	return ""
}

func (x *BatchInferenceTask) GetOperationId() string {
	if x != nil {
		return x.OperationId
	}
	return ""
}

func (x *BatchInferenceTask) GetFolderId() string {
	if x != nil {
		return x.FolderId
	}
	return ""
}

func (x *BatchInferenceTask) GetModelUri() string {
	if x != nil {
		return x.ModelUri
	}
	return ""
}

func (x *BatchInferenceTask) GetSourceDatasetId() string {
	if x != nil {
		return x.SourceDatasetId
	}
	return ""
}

func (x *BatchInferenceTask) GetRequest() isBatchInferenceTask_Request {
	if x != nil {
		return x.Request
	}
	return nil
}

func (x *BatchInferenceTask) GetCompletionRequest() *BatchCompletionRequest {
	if x != nil {
		if x, ok := x.Request.(*BatchInferenceTask_CompletionRequest); ok {
			return x.CompletionRequest
		}
	}
	return nil
}

func (x *BatchInferenceTask) GetStatus() BatchInferenceTask_Status {
	if x != nil {
		return x.Status
	}
	return BatchInferenceTask_STATUS_UNSPECIFIED
}

func (x *BatchInferenceTask) GetResultDatasetId() string {
	if x != nil {
		return x.ResultDatasetId
	}
	return ""
}

func (x *BatchInferenceTask) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *BatchInferenceTask) GetCreatedBy() string {
	if x != nil {
		return x.CreatedBy
	}
	return ""
}

func (x *BatchInferenceTask) GetCreatedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.CreatedAt
	}
	return nil
}

func (x *BatchInferenceTask) GetStartedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.StartedAt
	}
	return nil
}

func (x *BatchInferenceTask) GetFinishedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.FinishedAt
	}
	return nil
}

func (x *BatchInferenceTask) GetErrors() *BatchInferenceTask_ErrorsInfo {
	if x != nil {
		return x.Errors
	}
	return nil
}

type isBatchInferenceTask_Request interface {
	isBatchInferenceTask_Request()
}

type BatchInferenceTask_CompletionRequest struct {
	CompletionRequest *BatchCompletionRequest `protobuf:"bytes,6,opt,name=completion_request,json=completionRequest,proto3,oneof"`
}

func (*BatchInferenceTask_CompletionRequest) isBatchInferenceTask_Request() {}

type BatchInferenceTask_ErrorsInfo struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// If set and not OK - task failed
	Status *status.Status `protobuf:"bytes,1,opt,name=status,proto3" json:"status,omitempty"`
	// Errors by lines
	LineErrors []*BatchInferenceTask_ErrorsInfo_LineError `protobuf:"bytes,2,rep,name=line_errors,json=lineErrors,proto3" json:"line_errors,omitempty"`
	// Errors by batches
	BatchErrors   []*BatchInferenceTask_ErrorsInfo_BatchError `protobuf:"bytes,3,rep,name=batch_errors,json=batchErrors,proto3" json:"batch_errors,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchInferenceTask_ErrorsInfo) Reset() {
	*x = BatchInferenceTask_ErrorsInfo{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceTask_ErrorsInfo) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceTask_ErrorsInfo) ProtoMessage() {}

func (x *BatchInferenceTask_ErrorsInfo) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceTask_ErrorsInfo.ProtoReflect.Descriptor instead.
func (*BatchInferenceTask_ErrorsInfo) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescGZIP(), []int{0, 0}
}

func (x *BatchInferenceTask_ErrorsInfo) GetStatus() *status.Status {
	if x != nil {
		return x.Status
	}
	return nil
}

func (x *BatchInferenceTask_ErrorsInfo) GetLineErrors() []*BatchInferenceTask_ErrorsInfo_LineError {
	if x != nil {
		return x.LineErrors
	}
	return nil
}

func (x *BatchInferenceTask_ErrorsInfo) GetBatchErrors() []*BatchInferenceTask_ErrorsInfo_BatchError {
	if x != nil {
		return x.BatchErrors
	}
	return nil
}

type BatchInferenceTask_ErrorsInfo_LineError struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	LineNumber    int64                  `protobuf:"varint,1,opt,name=line_number,json=lineNumber,proto3" json:"line_number,omitempty"`
	Message       string                 `protobuf:"bytes,2,opt,name=message,proto3" json:"message,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchInferenceTask_ErrorsInfo_LineError) Reset() {
	*x = BatchInferenceTask_ErrorsInfo_LineError{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceTask_ErrorsInfo_LineError) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceTask_ErrorsInfo_LineError) ProtoMessage() {}

func (x *BatchInferenceTask_ErrorsInfo_LineError) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceTask_ErrorsInfo_LineError.ProtoReflect.Descriptor instead.
func (*BatchInferenceTask_ErrorsInfo_LineError) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescGZIP(), []int{0, 0, 0}
}

func (x *BatchInferenceTask_ErrorsInfo_LineError) GetLineNumber() int64 {
	if x != nil {
		return x.LineNumber
	}
	return 0
}

func (x *BatchInferenceTask_ErrorsInfo_LineError) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

type BatchInferenceTask_ErrorsInfo_BatchError struct {
	state       protoimpl.MessageState `protogen:"open.v1"`
	BatchNumber int64                  `protobuf:"varint,1,opt,name=batch_number,json=batchNumber,proto3" json:"batch_number,omitempty"`
	// Range of lines in batch
	FirstLine     int64  `protobuf:"varint,2,opt,name=first_line,json=firstLine,proto3" json:"first_line,omitempty"`
	LastLine      int64  `protobuf:"varint,3,opt,name=last_line,json=lastLine,proto3" json:"last_line,omitempty"`
	Message       string `protobuf:"bytes,4,opt,name=message,proto3" json:"message,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) Reset() {
	*x = BatchInferenceTask_ErrorsInfo_BatchError{}
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchInferenceTask_ErrorsInfo_BatchError) ProtoMessage() {}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchInferenceTask_ErrorsInfo_BatchError.ProtoReflect.Descriptor instead.
func (*BatchInferenceTask_ErrorsInfo_BatchError) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescGZIP(), []int{0, 0, 1}
}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) GetBatchNumber() int64 {
	if x != nil {
		return x.BatchNumber
	}
	return 0
}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) GetFirstLine() int64 {
	if x != nil {
		return x.FirstLine
	}
	return 0
}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) GetLastLine() int64 {
	if x != nil {
		return x.LastLine
	}
	return 0
}

func (x *BatchInferenceTask_ErrorsInfo_BatchError) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

var File_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto protoreflect.FileDescriptor

const file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDesc = "" +
	"\n" +
	"=yandex/cloud/ai/batch_inference/v1/batch_inference_task.proto\x12\"yandex.cloud.ai.batch_inference.v1\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x17google/rpc/status.proto\x1a:yandex/cloud/ai/batch_inference/v1/inference_options.proto\"\xd5\v\n" +
	"\x12BatchInferenceTask\x12\x17\n" +
	"\atask_id\x18\x01 \x01(\tR\x06taskId\x12!\n" +
	"\foperation_id\x18\x02 \x01(\tR\voperationId\x12\x1b\n" +
	"\tfolder_id\x18\x03 \x01(\tR\bfolderId\x12\x1b\n" +
	"\tmodel_uri\x18\x04 \x01(\tR\bmodelUri\x12*\n" +
	"\x11source_dataset_id\x18\x05 \x01(\tR\x0fsourceDatasetId\x12k\n" +
	"\x12completion_request\x18\x06 \x01(\v2:.yandex.cloud.ai.batch_inference.v1.BatchCompletionRequestH\x00R\x11completionRequest\x12U\n" +
	"\x06status\x18\a \x01(\x0e2=.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.StatusR\x06status\x12*\n" +
	"\x11result_dataset_id\x18\b \x01(\tR\x0fresultDatasetId\x12Z\n" +
	"\x06labels\x18\t \x03(\v2B.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.LabelsEntryR\x06labels\x12\x1d\n" +
	"\n" +
	"created_by\x18\n" +
	" \x01(\tR\tcreatedBy\x129\n" +
	"\n" +
	"created_at\x18\v \x01(\v2\x1a.google.protobuf.TimestampR\tcreatedAt\x129\n" +
	"\n" +
	"started_at\x18\f \x01(\v2\x1a.google.protobuf.TimestampR\tstartedAt\x12;\n" +
	"\vfinished_at\x18\r \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"finishedAt\x12Y\n" +
	"\x06errors\x18\x0e \x01(\v2A.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfoR\x06errors\x1a\xe7\x03\n" +
	"\n" +
	"ErrorsInfo\x12*\n" +
	"\x06status\x18\x01 \x01(\v2\x12.google.rpc.StatusR\x06status\x12l\n" +
	"\vline_errors\x18\x02 \x03(\v2K.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.LineErrorR\n" +
	"lineErrors\x12o\n" +
	"\fbatch_errors\x18\x03 \x03(\v2L.yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.BatchErrorR\vbatchErrors\x1aF\n" +
	"\tLineError\x12\x1f\n" +
	"\vline_number\x18\x01 \x01(\x03R\n" +
	"lineNumber\x12\x18\n" +
	"\amessage\x18\x02 \x01(\tR\amessage\x1a\x85\x01\n" +
	"\n" +
	"BatchError\x12!\n" +
	"\fbatch_number\x18\x01 \x01(\x03R\vbatchNumber\x12\x1d\n" +
	"\n" +
	"first_line\x18\x02 \x01(\x03R\tfirstLine\x12\x1b\n" +
	"\tlast_line\x18\x03 \x01(\x03R\blastLine\x12\x18\n" +
	"\amessage\x18\x04 \x01(\tR\amessage\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"t\n" +
	"\x06Status\x12\x16\n" +
	"\x12STATUS_UNSPECIFIED\x10\x00\x12\v\n" +
	"\aCREATED\x10\x01\x12\v\n" +
	"\aPENDING\x10\x02\x12\x0f\n" +
	"\vIN_PROGRESS\x10\x03\x12\r\n" +
	"\tCOMPLETED\x10\x04\x12\n" +
	"\n" +
	"\x06FAILED\x10\x05\x12\f\n" +
	"\bCANCELED\x10\x06B\t\n" +
	"\arequestBu\n" +
	"&yandex.cloud.api.ai.batch_inference.v1ZKgithub.com/yandex-cloud/go-genproto/yandex/cloud/ai/batch_inference/v1;fomob\x06proto3"

var (
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescOnce sync.Once
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescData []byte
)

func file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescGZIP() []byte {
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescOnce.Do(func() {
		file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDesc), len(file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDesc)))
	})
	return file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDescData
}

var file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes = make([]protoimpl.MessageInfo, 5)
var file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_goTypes = []any{
	(BatchInferenceTask_Status)(0),        // 0: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.Status
	(*BatchInferenceTask)(nil),            // 1: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask
	(*BatchInferenceTask_ErrorsInfo)(nil), // 2: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo
	nil,                                   // 3: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.LabelsEntry
	(*BatchInferenceTask_ErrorsInfo_LineError)(nil),  // 4: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.LineError
	(*BatchInferenceTask_ErrorsInfo_BatchError)(nil), // 5: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.BatchError
	(*BatchCompletionRequest)(nil),                   // 6: yandex.cloud.ai.batch_inference.v1.BatchCompletionRequest
	(*timestamppb.Timestamp)(nil),                    // 7: google.protobuf.Timestamp
	(*status.Status)(nil),                            // 8: google.rpc.Status
}
var file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_depIdxs = []int32{
	6,  // 0: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.completion_request:type_name -> yandex.cloud.ai.batch_inference.v1.BatchCompletionRequest
	0,  // 1: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.status:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.Status
	3,  // 2: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.labels:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.LabelsEntry
	7,  // 3: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.created_at:type_name -> google.protobuf.Timestamp
	7,  // 4: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.started_at:type_name -> google.protobuf.Timestamp
	7,  // 5: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.finished_at:type_name -> google.protobuf.Timestamp
	2,  // 6: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.errors:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo
	8,  // 7: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.status:type_name -> google.rpc.Status
	4,  // 8: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.line_errors:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.LineError
	5,  // 9: yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.batch_errors:type_name -> yandex.cloud.ai.batch_inference.v1.BatchInferenceTask.ErrorsInfo.BatchError
	10, // [10:10] is the sub-list for method output_type
	10, // [10:10] is the sub-list for method input_type
	10, // [10:10] is the sub-list for extension type_name
	10, // [10:10] is the sub-list for extension extendee
	0,  // [0:10] is the sub-list for field type_name
}

func init() { file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_init() }
func file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_init() {
	if File_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto != nil {
		return
	}
	file_yandex_cloud_ai_batch_inference_v1_inference_options_proto_init()
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes[0].OneofWrappers = []any{
		(*BatchInferenceTask_CompletionRequest)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDesc), len(file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   5,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_goTypes,
		DependencyIndexes: file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_depIdxs,
		EnumInfos:         file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_enumTypes,
		MessageInfos:      file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_msgTypes,
	}.Build()
	File_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto = out.File
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_goTypes = nil
	file_yandex_cloud_ai_batch_inference_v1_batch_inference_task_proto_depIdxs = nil
}

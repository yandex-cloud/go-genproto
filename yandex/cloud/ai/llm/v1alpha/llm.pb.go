// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v3.21.12
// source: yandex/cloud/ai/llm/v1alpha/llm.proto

package llm

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	wrapperspb "google.golang.org/protobuf/types/known/wrapperspb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Defines the options for text generation.
type GenerationOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Enables streaming of partially generated text.
	PartialResults bool `protobuf:"varint,1,opt,name=partial_results,json=partialResults,proto3" json:"partial_results,omitempty"`
	// Affects creativity and randomness of responses. Should be a double number between 0 (inclusive) and 1 (inclusive).
	// Lower values produce more straightforward responses, while higher values lead to increased creativity and randomness.
	Temperature *wrapperspb.DoubleValue `protobuf:"bytes,2,opt,name=temperature,proto3" json:"temperature,omitempty"`
	// Sets the maximum limit on the total number of tokens used for both the input prompt and the generated response.
	// Must be greater than zero and not exceed 7400 tokens.
	MaxTokens     *wrapperspb.Int64Value `protobuf:"bytes,3,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerationOptions) Reset() {
	*x = GenerationOptions{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerationOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerationOptions) ProtoMessage() {}

func (x *GenerationOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerationOptions.ProtoReflect.Descriptor instead.
func (*GenerationOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescGZIP(), []int{0}
}

func (x *GenerationOptions) GetPartialResults() bool {
	if x != nil {
		return x.PartialResults
	}
	return false
}

func (x *GenerationOptions) GetTemperature() *wrapperspb.DoubleValue {
	if x != nil {
		return x.Temperature
	}
	return nil
}

func (x *GenerationOptions) GetMaxTokens() *wrapperspb.Int64Value {
	if x != nil {
		return x.MaxTokens
	}
	return nil
}

// Represents an alternative generated response, including its score and token count.
type Alternative struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The generated text response.
	Text string `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	// The score or confidence of the generated text.
	Score float64 `protobuf:"fixed64,2,opt,name=score,proto3" json:"score,omitempty"`
	// The number of tokens in the generated response.
	NumTokens     int64 `protobuf:"varint,3,opt,name=num_tokens,json=numTokens,proto3" json:"num_tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Alternative) Reset() {
	*x = Alternative{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Alternative) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Alternative) ProtoMessage() {}

func (x *Alternative) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Alternative.ProtoReflect.Descriptor instead.
func (*Alternative) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescGZIP(), []int{1}
}

func (x *Alternative) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *Alternative) GetScore() float64 {
	if x != nil {
		return x.Score
	}
	return 0
}

func (x *Alternative) GetNumTokens() int64 {
	if x != nil {
		return x.NumTokens
	}
	return 0
}

// Represents a message within a chat.
type Message struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Identifies the sender of the message.
	Role string `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`
	// The text content of the message.
	Text          string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Message) Reset() {
	*x = Message{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Message) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Message) ProtoMessage() {}

func (x *Message) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Message.ProtoReflect.Descriptor instead.
func (*Message) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescGZIP(), []int{2}
}

func (x *Message) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *Message) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// Represents a token, the basic unit of text, used by the LLM.
type Token struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// An internal token identifier.
	Id int64 `protobuf:"varint,1,opt,name=id,proto3" json:"id,omitempty"`
	// The textual representation of the token.
	Text string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	// Indicates whether the token is special or not. Special tokens define the model's behavior and are not visible to users.
	Special       bool `protobuf:"varint,3,opt,name=special,proto3" json:"special,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Token) Reset() {
	*x = Token{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Token) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Token) ProtoMessage() {}

func (x *Token) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Token.ProtoReflect.Descriptor instead.
func (*Token) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescGZIP(), []int{3}
}

func (x *Token) GetId() int64 {
	if x != nil {
		return x.Id
	}
	return 0
}

func (x *Token) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *Token) GetSpecial() bool {
	if x != nil {
		return x.Special
	}
	return false
}

var File_yandex_cloud_ai_llm_v1alpha_llm_proto protoreflect.FileDescriptor

const file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDesc = "" +
	"\n" +
	"%yandex/cloud/ai/llm/v1alpha/llm.proto\x12\x1byandex.cloud.ai.llm.v1alpha\x1a\x1egoogle/protobuf/wrappers.proto\"\xb8\x01\n" +
	"\x11GenerationOptions\x12'\n" +
	"\x0fpartial_results\x18\x01 \x01(\bR\x0epartialResults\x12>\n" +
	"\vtemperature\x18\x02 \x01(\v2\x1c.google.protobuf.DoubleValueR\vtemperature\x12:\n" +
	"\n" +
	"max_tokens\x18\x03 \x01(\v2\x1b.google.protobuf.Int64ValueR\tmaxTokens\"V\n" +
	"\vAlternative\x12\x12\n" +
	"\x04text\x18\x01 \x01(\tR\x04text\x12\x14\n" +
	"\x05score\x18\x02 \x01(\x01R\x05score\x12\x1d\n" +
	"\n" +
	"num_tokens\x18\x03 \x01(\x03R\tnumTokens\"1\n" +
	"\aMessage\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\"E\n" +
	"\x05Token\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\x03R\x02id\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\x12\x18\n" +
	"\aspecial\x18\x03 \x01(\bR\aspecialBf\n" +
	"\x1fyandex.cloud.api.ai.llm.v1alphaZCgithub.com/yandex-cloud/go-genproto/yandex/cloud/ai/llm/v1alpha;llmb\x06proto3"

var (
	file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescOnce sync.Once
	file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescData []byte
)

func file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescGZIP() []byte {
	file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescOnce.Do(func() {
		file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDesc), len(file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDesc)))
	})
	return file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDescData
}

var file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
var file_yandex_cloud_ai_llm_v1alpha_llm_proto_goTypes = []any{
	(*GenerationOptions)(nil),      // 0: yandex.cloud.ai.llm.v1alpha.GenerationOptions
	(*Alternative)(nil),            // 1: yandex.cloud.ai.llm.v1alpha.Alternative
	(*Message)(nil),                // 2: yandex.cloud.ai.llm.v1alpha.Message
	(*Token)(nil),                  // 3: yandex.cloud.ai.llm.v1alpha.Token
	(*wrapperspb.DoubleValue)(nil), // 4: google.protobuf.DoubleValue
	(*wrapperspb.Int64Value)(nil),  // 5: google.protobuf.Int64Value
}
var file_yandex_cloud_ai_llm_v1alpha_llm_proto_depIdxs = []int32{
	4, // 0: yandex.cloud.ai.llm.v1alpha.GenerationOptions.temperature:type_name -> google.protobuf.DoubleValue
	5, // 1: yandex.cloud.ai.llm.v1alpha.GenerationOptions.max_tokens:type_name -> google.protobuf.Int64Value
	2, // [2:2] is the sub-list for method output_type
	2, // [2:2] is the sub-list for method input_type
	2, // [2:2] is the sub-list for extension type_name
	2, // [2:2] is the sub-list for extension extendee
	0, // [0:2] is the sub-list for field type_name
}

func init() { file_yandex_cloud_ai_llm_v1alpha_llm_proto_init() }
func file_yandex_cloud_ai_llm_v1alpha_llm_proto_init() {
	if File_yandex_cloud_ai_llm_v1alpha_llm_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDesc), len(file_yandex_cloud_ai_llm_v1alpha_llm_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   4,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_yandex_cloud_ai_llm_v1alpha_llm_proto_goTypes,
		DependencyIndexes: file_yandex_cloud_ai_llm_v1alpha_llm_proto_depIdxs,
		MessageInfos:      file_yandex_cloud_ai_llm_v1alpha_llm_proto_msgTypes,
	}.Build()
	File_yandex_cloud_ai_llm_v1alpha_llm_proto = out.File
	file_yandex_cloud_ai_llm_v1alpha_llm_proto_goTypes = nil
	file_yandex_cloud_ai_llm_v1alpha_llm_proto_depIdxs = nil
}

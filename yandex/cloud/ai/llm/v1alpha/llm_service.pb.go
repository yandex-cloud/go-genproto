// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v3.21.12
// source: yandex/cloud/ai/llm/v1alpha/llm_service.proto

package llm

import (
	_ "github.com/yandex-cloud/go-genproto/yandex/cloud"
	_ "github.com/yandex-cloud/go-genproto/yandex/cloud/api"
	operation "github.com/yandex-cloud/go-genproto/yandex/cloud/operation"
	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Enum to specify the type of embedding to be generated.
type EmbeddingRequest_EmbeddingType int32

const (
	// Unspecified embedding type.
	EmbeddingRequest_EMBEDDING_TYPE_UNSPECIFIED EmbeddingRequest_EmbeddingType = 0
	// Embedding for a query. Use this when you have a short query or search term
	// that you want to obtain an embedding for. Query embeddings are typically
	// used in information retrieval and search applications.
	EmbeddingRequest_EMBEDDING_TYPE_QUERY EmbeddingRequest_EmbeddingType = 1
	// Embedding for a document. Use this when you have a longer document or a piece
	// of text that you want to obtain an embedding for. Document embeddings are often
	// used in natural language understanding and document similarity tasks.
	EmbeddingRequest_EMBEDDING_TYPE_DOCUMENT EmbeddingRequest_EmbeddingType = 2
)

// Enum value maps for EmbeddingRequest_EmbeddingType.
var (
	EmbeddingRequest_EmbeddingType_name = map[int32]string{
		0: "EMBEDDING_TYPE_UNSPECIFIED",
		1: "EMBEDDING_TYPE_QUERY",
		2: "EMBEDDING_TYPE_DOCUMENT",
	}
	EmbeddingRequest_EmbeddingType_value = map[string]int32{
		"EMBEDDING_TYPE_UNSPECIFIED": 0,
		"EMBEDDING_TYPE_QUERY":       1,
		"EMBEDDING_TYPE_DOCUMENT":    2,
	}
)

func (x EmbeddingRequest_EmbeddingType) Enum() *EmbeddingRequest_EmbeddingType {
	p := new(EmbeddingRequest_EmbeddingType)
	*p = x
	return p
}

func (x EmbeddingRequest_EmbeddingType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (EmbeddingRequest_EmbeddingType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_enumTypes[0].Descriptor()
}

func (EmbeddingRequest_EmbeddingType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_enumTypes[0]
}

func (x EmbeddingRequest_EmbeddingType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use EmbeddingRequest_EmbeddingType.Descriptor instead.
func (EmbeddingRequest_EmbeddingType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{6, 0}
}

// Request for instructing the model to generate text.
type InstructRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name or identifier of the model to be used for text generation.
	// Possible value for now: `general`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Configuration options for text generation.
	GenerationOptions *GenerationOptions `protobuf:"bytes,2,opt,name=generation_options,json=generationOptions,proto3" json:"generation_options,omitempty"`
	// Text precondition or context of the request.
	// For example, if the instruction is "You are the youngest Nobel laureate",
	// the request text might be "Tell us about your daily routine".
	//
	// Types that are valid to be assigned to Instruction:
	//
	//	*InstructRequest_InstructionText
	//	*InstructRequest_InstructionUri
	Instruction isInstructRequest_Instruction `protobuf_oneof:"Instruction"`
	// Request for text generation.
	//
	// Types that are valid to be assigned to Request:
	//
	//	*InstructRequest_RequestText
	Request       isInstructRequest_Request `protobuf_oneof:"Request"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InstructRequest) Reset() {
	*x = InstructRequest{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InstructRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InstructRequest) ProtoMessage() {}

func (x *InstructRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InstructRequest.ProtoReflect.Descriptor instead.
func (*InstructRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{0}
}

func (x *InstructRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *InstructRequest) GetGenerationOptions() *GenerationOptions {
	if x != nil {
		return x.GenerationOptions
	}
	return nil
}

func (x *InstructRequest) GetInstruction() isInstructRequest_Instruction {
	if x != nil {
		return x.Instruction
	}
	return nil
}

func (x *InstructRequest) GetInstructionText() string {
	if x != nil {
		if x, ok := x.Instruction.(*InstructRequest_InstructionText); ok {
			return x.InstructionText
		}
	}
	return ""
}

func (x *InstructRequest) GetInstructionUri() string {
	if x != nil {
		if x, ok := x.Instruction.(*InstructRequest_InstructionUri); ok {
			return x.InstructionUri
		}
	}
	return ""
}

func (x *InstructRequest) GetRequest() isInstructRequest_Request {
	if x != nil {
		return x.Request
	}
	return nil
}

func (x *InstructRequest) GetRequestText() string {
	if x != nil {
		if x, ok := x.Request.(*InstructRequest_RequestText); ok {
			return x.RequestText
		}
	}
	return ""
}

type isInstructRequest_Instruction interface {
	isInstructRequest_Instruction()
}

type InstructRequest_InstructionText struct {
	// The text-based instruction for text generation.
	InstructionText string `protobuf:"bytes,3,opt,name=instruction_text,json=instructionText,proto3,oneof"`
}

type InstructRequest_InstructionUri struct {
	// A URI containing instructions for text generation.
	InstructionUri string `protobuf:"bytes,5,opt,name=instruction_uri,json=instructionUri,proto3,oneof"`
}

func (*InstructRequest_InstructionText) isInstructRequest_Instruction() {}

func (*InstructRequest_InstructionUri) isInstructRequest_Instruction() {}

type isInstructRequest_Request interface {
	isInstructRequest_Request()
}

type InstructRequest_RequestText struct {
	// The text-based request for text generation.
	RequestText string `protobuf:"bytes,4,opt,name=request_text,json=requestText,proto3,oneof"`
}

func (*InstructRequest_RequestText) isInstructRequest_Request() {}

// Response containing generated text alternatives and token count.
type InstructResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A list of alternative text responses.
	Alternatives []*Alternative `protobuf:"bytes,1,rep,name=alternatives,proto3" json:"alternatives,omitempty"`
	// The number of tokens used in the prompt, including both the [instruction_text] and [request_text].
	NumPromptTokens int64 `protobuf:"varint,2,opt,name=num_prompt_tokens,json=numPromptTokens,proto3" json:"num_prompt_tokens,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *InstructResponse) Reset() {
	*x = InstructResponse{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InstructResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InstructResponse) ProtoMessage() {}

func (x *InstructResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InstructResponse.ProtoReflect.Descriptor instead.
func (*InstructResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{1}
}

func (x *InstructResponse) GetAlternatives() []*Alternative {
	if x != nil {
		return x.Alternatives
	}
	return nil
}

func (x *InstructResponse) GetNumPromptTokens() int64 {
	if x != nil {
		return x.NumPromptTokens
	}
	return 0
}

// Request to engage in a chat conversation with a text generation model.
type ChatRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name or identifier of the model to be used for the chat.
	// Possible value for now: `general`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Configuration options for text generation.
	GenerationOptions *GenerationOptions `protobuf:"bytes,2,opt,name=generation_options,json=generationOptions,proto3" json:"generation_options,omitempty"`
	// Text precondition or context of the request.
	// For example, the instruction may be "You are a helpful assistant".
	//
	// Types that are valid to be assigned to Instruction:
	//
	//	*ChatRequest_InstructionText
	Instruction isChatRequest_Instruction `protobuf_oneof:"Instruction"`
	// A list of messages in the conversation.
	Messages      []*Message `protobuf:"bytes,4,rep,name=messages,proto3" json:"messages,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatRequest) Reset() {
	*x = ChatRequest{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatRequest) ProtoMessage() {}

func (x *ChatRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatRequest.ProtoReflect.Descriptor instead.
func (*ChatRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{2}
}

func (x *ChatRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *ChatRequest) GetGenerationOptions() *GenerationOptions {
	if x != nil {
		return x.GenerationOptions
	}
	return nil
}

func (x *ChatRequest) GetInstruction() isChatRequest_Instruction {
	if x != nil {
		return x.Instruction
	}
	return nil
}

func (x *ChatRequest) GetInstructionText() string {
	if x != nil {
		if x, ok := x.Instruction.(*ChatRequest_InstructionText); ok {
			return x.InstructionText
		}
	}
	return ""
}

func (x *ChatRequest) GetMessages() []*Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

type isChatRequest_Instruction interface {
	isChatRequest_Instruction()
}

type ChatRequest_InstructionText struct {
	// The text-based instruction for the conversation.
	InstructionText string `protobuf:"bytes,3,opt,name=instruction_text,json=instructionText,proto3,oneof"`
}

func (*ChatRequest_InstructionText) isChatRequest_Instruction() {}

// Contains a model-generated response for a chat query.
type ChatResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The assistant's message in the chat conversation.
	Message *Message `protobuf:"bytes,1,opt,name=message,proto3" json:"message,omitempty"`
	// Total number of tokens used in both the chat request and chat response.
	NumTokens     int64 `protobuf:"varint,2,opt,name=num_tokens,json=numTokens,proto3" json:"num_tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatResponse) Reset() {
	*x = ChatResponse{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatResponse) ProtoMessage() {}

func (x *ChatResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatResponse.ProtoReflect.Descriptor instead.
func (*ChatResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{3}
}

func (x *ChatResponse) GetMessage() *Message {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *ChatResponse) GetNumTokens() int64 {
	if x != nil {
		return x.NumTokens
	}
	return 0
}

// Request to tokenize input text.
type TokenizeRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The name or identifier of the model to be used for tokenization.
	// Possible values for now: `general`, `general:embedding`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The input text to tokenize.
	Text          string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TokenizeRequest) Reset() {
	*x = TokenizeRequest{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TokenizeRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TokenizeRequest) ProtoMessage() {}

func (x *TokenizeRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TokenizeRequest.ProtoReflect.Descriptor instead.
func (*TokenizeRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{4}
}

func (x *TokenizeRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TokenizeRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// Tokenization response.
type TokenizeResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A list of tokens obtained from tokenization.
	Tokens        []*Token `protobuf:"bytes,1,rep,name=tokens,proto3" json:"tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TokenizeResponse) Reset() {
	*x = TokenizeResponse{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TokenizeResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TokenizeResponse) ProtoMessage() {}

func (x *TokenizeResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TokenizeResponse.ProtoReflect.Descriptor instead.
func (*TokenizeResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{5}
}

func (x *TokenizeResponse) GetTokens() []*Token {
	if x != nil {
		return x.Tokens
	}
	return nil
}

// Represents a request to obtain embeddings for text data.
type EmbeddingRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The type of embedding to be generated.
	EmbeddingType EmbeddingRequest_EmbeddingType `protobuf:"varint,1,opt,name=embedding_type,json=embeddingType,proto3,enum=yandex.cloud.ai.llm.v1alpha.EmbeddingRequest_EmbeddingType" json:"embedding_type,omitempty"`
	// The name or identifier of the model to be used for embedding. Possible value for now: `general:embedding`.
	Model string `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	// The input text for which the embedding is requested.
	Text          string `protobuf:"bytes,3,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbeddingRequest) Reset() {
	*x = EmbeddingRequest{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbeddingRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbeddingRequest) ProtoMessage() {}

func (x *EmbeddingRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbeddingRequest.ProtoReflect.Descriptor instead.
func (*EmbeddingRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{6}
}

func (x *EmbeddingRequest) GetEmbeddingType() EmbeddingRequest_EmbeddingType {
	if x != nil {
		return x.EmbeddingType
	}
	return EmbeddingRequest_EMBEDDING_TYPE_UNSPECIFIED
}

func (x *EmbeddingRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *EmbeddingRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// Represents a response containing embeddings for input text data.
type EmbeddingResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A repeated list of double values representing the embedding.
	Embedding []float64 `protobuf:"fixed64,1,rep,packed,name=embedding,proto3" json:"embedding,omitempty"`
	// The number of tokens in the input text.
	NumTokens     int64 `protobuf:"varint,2,opt,name=num_tokens,json=numTokens,proto3" json:"num_tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbeddingResponse) Reset() {
	*x = EmbeddingResponse{}
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbeddingResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbeddingResponse) ProtoMessage() {}

func (x *EmbeddingResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbeddingResponse.ProtoReflect.Descriptor instead.
func (*EmbeddingResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP(), []int{7}
}

func (x *EmbeddingResponse) GetEmbedding() []float64 {
	if x != nil {
		return x.Embedding
	}
	return nil
}

func (x *EmbeddingResponse) GetNumTokens() int64 {
	if x != nil {
		return x.NumTokens
	}
	return 0
}

var File_yandex_cloud_ai_llm_v1alpha_llm_service_proto protoreflect.FileDescriptor

const file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDesc = "" +
	"\n" +
	"-yandex/cloud/ai/llm/v1alpha/llm_service.proto\x12\x1byandex.cloud.ai.llm.v1alpha\x1a%yandex/cloud/ai/llm/v1alpha/llm.proto\x1a\x1cgoogle/api/annotations.proto\x1a\x1dyandex/cloud/validation.proto\x1a yandex/cloud/api/operation.proto\x1a&yandex/cloud/operation/operation.proto\"\xa7\x02\n" +
	"\x0fInstructRequest\x12\x1e\n" +
	"\x05model\x18\x01 \x01(\tB\b\x8a\xc81\x04<=50R\x05model\x12]\n" +
	"\x12generation_options\x18\x02 \x01(\v2..yandex.cloud.ai.llm.v1alpha.GenerationOptionsR\x11generationOptions\x12+\n" +
	"\x10instruction_text\x18\x03 \x01(\tH\x00R\x0finstructionText\x12)\n" +
	"\x0finstruction_uri\x18\x05 \x01(\tH\x00R\x0einstructionUri\x12#\n" +
	"\frequest_text\x18\x04 \x01(\tH\x01R\vrequestTextB\r\n" +
	"\vInstructionB\t\n" +
	"\aRequest\"\x8c\x01\n" +
	"\x10InstructResponse\x12L\n" +
	"\falternatives\x18\x01 \x03(\v2(.yandex.cloud.ai.llm.v1alpha.AlternativeR\falternatives\x12*\n" +
	"\x11num_prompt_tokens\x18\x02 \x01(\x03R\x0fnumPromptTokens\"\x8a\x02\n" +
	"\vChatRequest\x12\x1e\n" +
	"\x05model\x18\x01 \x01(\tB\b\x8a\xc81\x04<=50R\x05model\x12]\n" +
	"\x12generation_options\x18\x02 \x01(\v2..yandex.cloud.ai.llm.v1alpha.GenerationOptionsR\x11generationOptions\x12+\n" +
	"\x10instruction_text\x18\x03 \x01(\tH\x00R\x0finstructionText\x12@\n" +
	"\bmessages\x18\x04 \x03(\v2$.yandex.cloud.ai.llm.v1alpha.MessageR\bmessagesB\r\n" +
	"\vInstruction\"m\n" +
	"\fChatResponse\x12>\n" +
	"\amessage\x18\x01 \x01(\v2$.yandex.cloud.ai.llm.v1alpha.MessageR\amessage\x12\x1d\n" +
	"\n" +
	"num_tokens\x18\x02 \x01(\x03R\tnumTokens\"E\n" +
	"\x0fTokenizeRequest\x12\x1e\n" +
	"\x05model\x18\x01 \x01(\tB\b\x8a\xc81\x04<=50R\x05model\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\"N\n" +
	"\x10TokenizeResponse\x12:\n" +
	"\x06tokens\x18\x01 \x03(\v2\".yandex.cloud.ai.llm.v1alpha.TokenR\x06tokens\"\x92\x02\n" +
	"\x10EmbeddingRequest\x12b\n" +
	"\x0eembedding_type\x18\x01 \x01(\x0e2;.yandex.cloud.ai.llm.v1alpha.EmbeddingRequest.EmbeddingTypeR\rembeddingType\x12\x1e\n" +
	"\x05model\x18\x02 \x01(\tB\b\x8a\xc81\x04<=50R\x05model\x12\x12\n" +
	"\x04text\x18\x03 \x01(\tR\x04text\"f\n" +
	"\rEmbeddingType\x12\x1e\n" +
	"\x1aEMBEDDING_TYPE_UNSPECIFIED\x10\x00\x12\x18\n" +
	"\x14EMBEDDING_TYPE_QUERY\x10\x01\x12\x1b\n" +
	"\x17EMBEDDING_TYPE_DOCUMENT\x10\x02\"P\n" +
	"\x11EmbeddingResponse\x12\x1c\n" +
	"\tembedding\x18\x01 \x03(\x01R\tembedding\x12\x1d\n" +
	"\n" +
	"num_tokens\x18\x02 \x01(\x03R\tnumTokens2\xa2\x02\n" +
	"\x15TextGenerationService\x12\x8b\x01\n" +
	"\bInstruct\x12,.yandex.cloud.ai.llm.v1alpha.InstructRequest\x1a-.yandex.cloud.ai.llm.v1alpha.InstructResponse\" \x82\xd3\xe4\x93\x02\x1a:\x01*\"\x15/llm/v1alpha/instruct0\x01\x12{\n" +
	"\x04Chat\x12(.yandex.cloud.ai.llm.v1alpha.ChatRequest\x1a).yandex.cloud.ai.llm.v1alpha.ChatResponse\"\x1c\x82\xd3\xe4\x93\x02\x16:\x01*\"\x11/llm/v1alpha/chat0\x012\x9e\x01\n" +
	"\x10TokenizerService\x12\x89\x01\n" +
	"\bTokenize\x12,.yandex.cloud.ai.llm.v1alpha.TokenizeRequest\x1a-.yandex.cloud.ai.llm.v1alpha.TokenizeResponse\" \x82\xd3\xe4\x93\x02\x1a:\x01*\"\x15/llm/v1alpha/tokenize2\xa3\x01\n" +
	"\x11EmbeddingsService\x12\x8d\x01\n" +
	"\tEmbedding\x12-.yandex.cloud.ai.llm.v1alpha.EmbeddingRequest\x1a..yandex.cloud.ai.llm.v1alpha.EmbeddingResponse\"!\x82\xd3\xe4\x93\x02\x1b:\x01*\"\x16/llm/v1alpha/embedding2\xb7\x01\n" +
	"\x1aTextGenerationAsyncService\x12\x98\x01\n" +
	"\bInstruct\x12,.yandex.cloud.ai.llm.v1alpha.InstructRequest\x1a!.yandex.cloud.operation.Operation\";\xb2\xd2*\x12\x12\x10InstructResponse\x82\xd3\xe4\x93\x02\x1f:\x01*\"\x1a/llm/v1alpha/instructAsyncBf\n" +
	"\x1fyandex.cloud.api.ai.llm.v1alphaZCgithub.com/yandex-cloud/go-genproto/yandex/cloud/ai/llm/v1alpha;llmb\x06proto3"

var (
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescOnce sync.Once
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescData []byte
)

func file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescGZIP() []byte {
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescOnce.Do(func() {
		file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDesc), len(file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDesc)))
	})
	return file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDescData
}

var file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes = make([]protoimpl.MessageInfo, 8)
var file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_goTypes = []any{
	(EmbeddingRequest_EmbeddingType)(0), // 0: yandex.cloud.ai.llm.v1alpha.EmbeddingRequest.EmbeddingType
	(*InstructRequest)(nil),             // 1: yandex.cloud.ai.llm.v1alpha.InstructRequest
	(*InstructResponse)(nil),            // 2: yandex.cloud.ai.llm.v1alpha.InstructResponse
	(*ChatRequest)(nil),                 // 3: yandex.cloud.ai.llm.v1alpha.ChatRequest
	(*ChatResponse)(nil),                // 4: yandex.cloud.ai.llm.v1alpha.ChatResponse
	(*TokenizeRequest)(nil),             // 5: yandex.cloud.ai.llm.v1alpha.TokenizeRequest
	(*TokenizeResponse)(nil),            // 6: yandex.cloud.ai.llm.v1alpha.TokenizeResponse
	(*EmbeddingRequest)(nil),            // 7: yandex.cloud.ai.llm.v1alpha.EmbeddingRequest
	(*EmbeddingResponse)(nil),           // 8: yandex.cloud.ai.llm.v1alpha.EmbeddingResponse
	(*GenerationOptions)(nil),           // 9: yandex.cloud.ai.llm.v1alpha.GenerationOptions
	(*Alternative)(nil),                 // 10: yandex.cloud.ai.llm.v1alpha.Alternative
	(*Message)(nil),                     // 11: yandex.cloud.ai.llm.v1alpha.Message
	(*Token)(nil),                       // 12: yandex.cloud.ai.llm.v1alpha.Token
	(*operation.Operation)(nil),         // 13: yandex.cloud.operation.Operation
}
var file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_depIdxs = []int32{
	9,  // 0: yandex.cloud.ai.llm.v1alpha.InstructRequest.generation_options:type_name -> yandex.cloud.ai.llm.v1alpha.GenerationOptions
	10, // 1: yandex.cloud.ai.llm.v1alpha.InstructResponse.alternatives:type_name -> yandex.cloud.ai.llm.v1alpha.Alternative
	9,  // 2: yandex.cloud.ai.llm.v1alpha.ChatRequest.generation_options:type_name -> yandex.cloud.ai.llm.v1alpha.GenerationOptions
	11, // 3: yandex.cloud.ai.llm.v1alpha.ChatRequest.messages:type_name -> yandex.cloud.ai.llm.v1alpha.Message
	11, // 4: yandex.cloud.ai.llm.v1alpha.ChatResponse.message:type_name -> yandex.cloud.ai.llm.v1alpha.Message
	12, // 5: yandex.cloud.ai.llm.v1alpha.TokenizeResponse.tokens:type_name -> yandex.cloud.ai.llm.v1alpha.Token
	0,  // 6: yandex.cloud.ai.llm.v1alpha.EmbeddingRequest.embedding_type:type_name -> yandex.cloud.ai.llm.v1alpha.EmbeddingRequest.EmbeddingType
	1,  // 7: yandex.cloud.ai.llm.v1alpha.TextGenerationService.Instruct:input_type -> yandex.cloud.ai.llm.v1alpha.InstructRequest
	3,  // 8: yandex.cloud.ai.llm.v1alpha.TextGenerationService.Chat:input_type -> yandex.cloud.ai.llm.v1alpha.ChatRequest
	5,  // 9: yandex.cloud.ai.llm.v1alpha.TokenizerService.Tokenize:input_type -> yandex.cloud.ai.llm.v1alpha.TokenizeRequest
	7,  // 10: yandex.cloud.ai.llm.v1alpha.EmbeddingsService.Embedding:input_type -> yandex.cloud.ai.llm.v1alpha.EmbeddingRequest
	1,  // 11: yandex.cloud.ai.llm.v1alpha.TextGenerationAsyncService.Instruct:input_type -> yandex.cloud.ai.llm.v1alpha.InstructRequest
	2,  // 12: yandex.cloud.ai.llm.v1alpha.TextGenerationService.Instruct:output_type -> yandex.cloud.ai.llm.v1alpha.InstructResponse
	4,  // 13: yandex.cloud.ai.llm.v1alpha.TextGenerationService.Chat:output_type -> yandex.cloud.ai.llm.v1alpha.ChatResponse
	6,  // 14: yandex.cloud.ai.llm.v1alpha.TokenizerService.Tokenize:output_type -> yandex.cloud.ai.llm.v1alpha.TokenizeResponse
	8,  // 15: yandex.cloud.ai.llm.v1alpha.EmbeddingsService.Embedding:output_type -> yandex.cloud.ai.llm.v1alpha.EmbeddingResponse
	13, // 16: yandex.cloud.ai.llm.v1alpha.TextGenerationAsyncService.Instruct:output_type -> yandex.cloud.operation.Operation
	12, // [12:17] is the sub-list for method output_type
	7,  // [7:12] is the sub-list for method input_type
	7,  // [7:7] is the sub-list for extension type_name
	7,  // [7:7] is the sub-list for extension extendee
	0,  // [0:7] is the sub-list for field type_name
}

func init() { file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_init() }
func file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_init() {
	if File_yandex_cloud_ai_llm_v1alpha_llm_service_proto != nil {
		return
	}
	file_yandex_cloud_ai_llm_v1alpha_llm_proto_init()
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[0].OneofWrappers = []any{
		(*InstructRequest_InstructionText)(nil),
		(*InstructRequest_InstructionUri)(nil),
		(*InstructRequest_RequestText)(nil),
	}
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes[2].OneofWrappers = []any{
		(*ChatRequest_InstructionText)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDesc), len(file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   8,
			NumExtensions: 0,
			NumServices:   4,
		},
		GoTypes:           file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_goTypes,
		DependencyIndexes: file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_depIdxs,
		EnumInfos:         file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_enumTypes,
		MessageInfos:      file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_msgTypes,
	}.Build()
	File_yandex_cloud_ai_llm_v1alpha_llm_service_proto = out.File
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_goTypes = nil
	file_yandex_cloud_ai_llm_v1alpha_llm_service_proto_depIdxs = nil
}

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v3.21.12
// source: yandex/cloud/ai/stt/v3/stt.proto

package stt

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	structpb "google.golang.org/protobuf/types/known/structpb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type CodeType int32

const (
	// Deprecated: Marked as deprecated in yandex/cloud/ai/stt/v3/stt.proto.
	CodeType_CODE_TYPE_UNSPECIFIED CodeType = 0
	// All good.
	CodeType_WORKING CodeType = 1
	// For example, if speech is sent not in real-time or context is unknown and we've made fallback.
	CodeType_WARNING CodeType = 2
	// After session was closed.
	CodeType_CLOSED CodeType = 3
)

// Enum value maps for CodeType.
var (
	CodeType_name = map[int32]string{
		0: "CODE_TYPE_UNSPECIFIED",
		1: "WORKING",
		2: "WARNING",
		3: "CLOSED",
	}
	CodeType_value = map[string]int32{
		"CODE_TYPE_UNSPECIFIED": 0,
		"WORKING":               1,
		"WARNING":               2,
		"CLOSED":                3,
	}
)

func (x CodeType) Enum() *CodeType {
	p := new(CodeType)
	*p = x
	return p
}

func (x CodeType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (CodeType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[0].Descriptor()
}

func (CodeType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[0]
}

func (x CodeType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use CodeType.Descriptor instead.
func (CodeType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{0}
}

// Base-level normalization.
type TextNormalizationOptions_TextNormalization int32

const (
	TextNormalizationOptions_TEXT_NORMALIZATION_UNSPECIFIED TextNormalizationOptions_TextNormalization = 0
	// Enable converting numbers, dates and time from text to numeric format.
	TextNormalizationOptions_TEXT_NORMALIZATION_ENABLED TextNormalizationOptions_TextNormalization = 1
	// Disable all normalization. Default value.
	TextNormalizationOptions_TEXT_NORMALIZATION_DISABLED TextNormalizationOptions_TextNormalization = 2
)

// Enum value maps for TextNormalizationOptions_TextNormalization.
var (
	TextNormalizationOptions_TextNormalization_name = map[int32]string{
		0: "TEXT_NORMALIZATION_UNSPECIFIED",
		1: "TEXT_NORMALIZATION_ENABLED",
		2: "TEXT_NORMALIZATION_DISABLED",
	}
	TextNormalizationOptions_TextNormalization_value = map[string]int32{
		"TEXT_NORMALIZATION_UNSPECIFIED": 0,
		"TEXT_NORMALIZATION_ENABLED":     1,
		"TEXT_NORMALIZATION_DISABLED":    2,
	}
)

func (x TextNormalizationOptions_TextNormalization) Enum() *TextNormalizationOptions_TextNormalization {
	p := new(TextNormalizationOptions_TextNormalization)
	*p = x
	return p
}

func (x TextNormalizationOptions_TextNormalization) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TextNormalizationOptions_TextNormalization) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[1].Descriptor()
}

func (TextNormalizationOptions_TextNormalization) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[1]
}

func (x TextNormalizationOptions_TextNormalization) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use TextNormalizationOptions_TextNormalization.Descriptor instead.
func (TextNormalizationOptions_TextNormalization) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{0, 0}
}

type TextNormalizationOptions_PhoneFormattingMode int32

const (
	TextNormalizationOptions_PHONE_FORMATTING_MODE_UNSPECIFIED TextNormalizationOptions_PhoneFormattingMode = 0
	// Disable phone formatting
	TextNormalizationOptions_PHONE_FORMATTING_MODE_DISABLED TextNormalizationOptions_PhoneFormattingMode = 1
)

// Enum value maps for TextNormalizationOptions_PhoneFormattingMode.
var (
	TextNormalizationOptions_PhoneFormattingMode_name = map[int32]string{
		0: "PHONE_FORMATTING_MODE_UNSPECIFIED",
		1: "PHONE_FORMATTING_MODE_DISABLED",
	}
	TextNormalizationOptions_PhoneFormattingMode_value = map[string]int32{
		"PHONE_FORMATTING_MODE_UNSPECIFIED": 0,
		"PHONE_FORMATTING_MODE_DISABLED":    1,
	}
)

func (x TextNormalizationOptions_PhoneFormattingMode) Enum() *TextNormalizationOptions_PhoneFormattingMode {
	p := new(TextNormalizationOptions_PhoneFormattingMode)
	*p = x
	return p
}

func (x TextNormalizationOptions_PhoneFormattingMode) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TextNormalizationOptions_PhoneFormattingMode) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[2].Descriptor()
}

func (TextNormalizationOptions_PhoneFormattingMode) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[2]
}

func (x TextNormalizationOptions_PhoneFormattingMode) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use TextNormalizationOptions_PhoneFormattingMode.Descriptor instead.
func (TextNormalizationOptions_PhoneFormattingMode) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{0, 1}
}

type DefaultEouClassifier_EouSensitivity int32

const (
	DefaultEouClassifier_EOU_SENSITIVITY_UNSPECIFIED DefaultEouClassifier_EouSensitivity = 0
	// Default and more conservative EOU detector.
	DefaultEouClassifier_DEFAULT DefaultEouClassifier_EouSensitivity = 1
	// A high-sensitive and fast EOU detector, which may produce more false positives.
	DefaultEouClassifier_HIGH DefaultEouClassifier_EouSensitivity = 2
)

// Enum value maps for DefaultEouClassifier_EouSensitivity.
var (
	DefaultEouClassifier_EouSensitivity_name = map[int32]string{
		0: "EOU_SENSITIVITY_UNSPECIFIED",
		1: "DEFAULT",
		2: "HIGH",
	}
	DefaultEouClassifier_EouSensitivity_value = map[string]int32{
		"EOU_SENSITIVITY_UNSPECIFIED": 0,
		"DEFAULT":                     1,
		"HIGH":                        2,
	}
)

func (x DefaultEouClassifier_EouSensitivity) Enum() *DefaultEouClassifier_EouSensitivity {
	p := new(DefaultEouClassifier_EouSensitivity)
	*p = x
	return p
}

func (x DefaultEouClassifier_EouSensitivity) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (DefaultEouClassifier_EouSensitivity) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[3].Descriptor()
}

func (DefaultEouClassifier_EouSensitivity) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[3]
}

func (x DefaultEouClassifier_EouSensitivity) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use DefaultEouClassifier_EouSensitivity.Descriptor instead.
func (DefaultEouClassifier_EouSensitivity) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{1, 0}
}

// Type of recognition classifier trigger.
type RecognitionClassifier_TriggerType int32

const (
	// Deprecated: Marked as deprecated in yandex/cloud/ai/stt/v3/stt.proto.
	RecognitionClassifier_TRIGGER_TYPE_UNSPECIFIED RecognitionClassifier_TriggerType = 0
	// Apply classifier to utterance responses.
	RecognitionClassifier_ON_UTTERANCE RecognitionClassifier_TriggerType = 1
	// Apply classifier to final responses.
	RecognitionClassifier_ON_FINAL RecognitionClassifier_TriggerType = 2
	// Apply classifier to partial responses.
	RecognitionClassifier_ON_PARTIAL RecognitionClassifier_TriggerType = 3
)

// Enum value maps for RecognitionClassifier_TriggerType.
var (
	RecognitionClassifier_TriggerType_name = map[int32]string{
		0: "TRIGGER_TYPE_UNSPECIFIED",
		1: "ON_UTTERANCE",
		2: "ON_FINAL",
		3: "ON_PARTIAL",
	}
	RecognitionClassifier_TriggerType_value = map[string]int32{
		"TRIGGER_TYPE_UNSPECIFIED": 0,
		"ON_UTTERANCE":             1,
		"ON_FINAL":                 2,
		"ON_PARTIAL":               3,
	}
)

func (x RecognitionClassifier_TriggerType) Enum() *RecognitionClassifier_TriggerType {
	p := new(RecognitionClassifier_TriggerType)
	*p = x
	return p
}

func (x RecognitionClassifier_TriggerType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (RecognitionClassifier_TriggerType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[4].Descriptor()
}

func (RecognitionClassifier_TriggerType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[4]
}

func (x RecognitionClassifier_TriggerType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use RecognitionClassifier_TriggerType.Descriptor instead.
func (RecognitionClassifier_TriggerType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{4, 0}
}

type RawAudio_AudioEncoding int32

const (
	RawAudio_AUDIO_ENCODING_UNSPECIFIED RawAudio_AudioEncoding = 0
	// Audio bit depth 16-bit signed little-endian (Linear PCM).
	RawAudio_LINEAR16_PCM RawAudio_AudioEncoding = 1
)

// Enum value maps for RawAudio_AudioEncoding.
var (
	RawAudio_AudioEncoding_name = map[int32]string{
		0: "AUDIO_ENCODING_UNSPECIFIED",
		1: "LINEAR16_PCM",
	}
	RawAudio_AudioEncoding_value = map[string]int32{
		"AUDIO_ENCODING_UNSPECIFIED": 0,
		"LINEAR16_PCM":               1,
	}
)

func (x RawAudio_AudioEncoding) Enum() *RawAudio_AudioEncoding {
	p := new(RawAudio_AudioEncoding)
	*p = x
	return p
}

func (x RawAudio_AudioEncoding) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (RawAudio_AudioEncoding) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[5].Descriptor()
}

func (RawAudio_AudioEncoding) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[5]
}

func (x RawAudio_AudioEncoding) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use RawAudio_AudioEncoding.Descriptor instead.
func (RawAudio_AudioEncoding) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{7, 0}
}

type ContainerAudio_ContainerAudioType int32

const (
	ContainerAudio_CONTAINER_AUDIO_TYPE_UNSPECIFIED ContainerAudio_ContainerAudioType = 0
	// Audio bit depth 16-bit signed little-endian (Linear PCM).
	ContainerAudio_WAV ContainerAudio_ContainerAudioType = 1
	// Data is encoded using the OPUS audio codec and compressed using the OGG container format.
	ContainerAudio_OGG_OPUS ContainerAudio_ContainerAudioType = 2
	// Data is encoded using MPEG-1/2 Layer III and compressed using the MP3 container format.
	ContainerAudio_MP3 ContainerAudio_ContainerAudioType = 3
)

// Enum value maps for ContainerAudio_ContainerAudioType.
var (
	ContainerAudio_ContainerAudioType_name = map[int32]string{
		0: "CONTAINER_AUDIO_TYPE_UNSPECIFIED",
		1: "WAV",
		2: "OGG_OPUS",
		3: "MP3",
	}
	ContainerAudio_ContainerAudioType_value = map[string]int32{
		"CONTAINER_AUDIO_TYPE_UNSPECIFIED": 0,
		"WAV":                              1,
		"OGG_OPUS":                         2,
		"MP3":                              3,
	}
)

func (x ContainerAudio_ContainerAudioType) Enum() *ContainerAudio_ContainerAudioType {
	p := new(ContainerAudio_ContainerAudioType)
	*p = x
	return p
}

func (x ContainerAudio_ContainerAudioType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ContainerAudio_ContainerAudioType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[6].Descriptor()
}

func (ContainerAudio_ContainerAudioType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[6]
}

func (x ContainerAudio_ContainerAudioType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ContainerAudio_ContainerAudioType.Descriptor instead.
func (ContainerAudio_ContainerAudioType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{8, 0}
}

type LanguageRestrictionOptions_LanguageRestrictionType int32

const (
	LanguageRestrictionOptions_LANGUAGE_RESTRICTION_TYPE_UNSPECIFIED LanguageRestrictionOptions_LanguageRestrictionType = 0
	// The list of most possible languages in the incoming audio.
	LanguageRestrictionOptions_WHITELIST LanguageRestrictionOptions_LanguageRestrictionType = 1
	// The list of languages that are likely not to be included in the incoming audio.
	LanguageRestrictionOptions_BLACKLIST LanguageRestrictionOptions_LanguageRestrictionType = 2
)

// Enum value maps for LanguageRestrictionOptions_LanguageRestrictionType.
var (
	LanguageRestrictionOptions_LanguageRestrictionType_name = map[int32]string{
		0: "LANGUAGE_RESTRICTION_TYPE_UNSPECIFIED",
		1: "WHITELIST",
		2: "BLACKLIST",
	}
	LanguageRestrictionOptions_LanguageRestrictionType_value = map[string]int32{
		"LANGUAGE_RESTRICTION_TYPE_UNSPECIFIED": 0,
		"WHITELIST":                             1,
		"BLACKLIST":                             2,
	}
)

func (x LanguageRestrictionOptions_LanguageRestrictionType) Enum() *LanguageRestrictionOptions_LanguageRestrictionType {
	p := new(LanguageRestrictionOptions_LanguageRestrictionType)
	*p = x
	return p
}

func (x LanguageRestrictionOptions_LanguageRestrictionType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (LanguageRestrictionOptions_LanguageRestrictionType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[7].Descriptor()
}

func (LanguageRestrictionOptions_LanguageRestrictionType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[7]
}

func (x LanguageRestrictionOptions_LanguageRestrictionType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use LanguageRestrictionOptions_LanguageRestrictionType.Descriptor instead.
func (LanguageRestrictionOptions_LanguageRestrictionType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{10, 0}
}

type RecognitionModelOptions_AudioProcessingType int32

const (
	RecognitionModelOptions_AUDIO_PROCESSING_TYPE_UNSPECIFIED RecognitionModelOptions_AudioProcessingType = 0
	// Process audio in mode optimized for real-time recognition, i.e. send partials and final responses as soon as possible.
	RecognitionModelOptions_REAL_TIME RecognitionModelOptions_AudioProcessingType = 1
	// Process audio after all data was received.
	RecognitionModelOptions_FULL_DATA RecognitionModelOptions_AudioProcessingType = 2
)

// Enum value maps for RecognitionModelOptions_AudioProcessingType.
var (
	RecognitionModelOptions_AudioProcessingType_name = map[int32]string{
		0: "AUDIO_PROCESSING_TYPE_UNSPECIFIED",
		1: "REAL_TIME",
		2: "FULL_DATA",
	}
	RecognitionModelOptions_AudioProcessingType_value = map[string]int32{
		"AUDIO_PROCESSING_TYPE_UNSPECIFIED": 0,
		"REAL_TIME":                         1,
		"FULL_DATA":                         2,
	}
)

func (x RecognitionModelOptions_AudioProcessingType) Enum() *RecognitionModelOptions_AudioProcessingType {
	p := new(RecognitionModelOptions_AudioProcessingType)
	*p = x
	return p
}

func (x RecognitionModelOptions_AudioProcessingType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (RecognitionModelOptions_AudioProcessingType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[8].Descriptor()
}

func (RecognitionModelOptions_AudioProcessingType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[8]
}

func (x RecognitionModelOptions_AudioProcessingType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use RecognitionModelOptions_AudioProcessingType.Descriptor instead.
func (RecognitionModelOptions_AudioProcessingType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{15, 0}
}

type SpeakerLabelingOptions_SpeakerLabeling int32

const (
	SpeakerLabelingOptions_SPEAKER_LABELING_UNSPECIFIED SpeakerLabelingOptions_SpeakerLabeling = 0
	// Enable speaker labeling.
	SpeakerLabelingOptions_SPEAKER_LABELING_ENABLED SpeakerLabelingOptions_SpeakerLabeling = 1
	// Disable speaker labeling. Default value.
	SpeakerLabelingOptions_SPEAKER_LABELING_DISABLED SpeakerLabelingOptions_SpeakerLabeling = 2
)

// Enum value maps for SpeakerLabelingOptions_SpeakerLabeling.
var (
	SpeakerLabelingOptions_SpeakerLabeling_name = map[int32]string{
		0: "SPEAKER_LABELING_UNSPECIFIED",
		1: "SPEAKER_LABELING_ENABLED",
		2: "SPEAKER_LABELING_DISABLED",
	}
	SpeakerLabelingOptions_SpeakerLabeling_value = map[string]int32{
		"SPEAKER_LABELING_UNSPECIFIED": 0,
		"SPEAKER_LABELING_ENABLED":     1,
		"SPEAKER_LABELING_DISABLED":    2,
	}
)

func (x SpeakerLabelingOptions_SpeakerLabeling) Enum() *SpeakerLabelingOptions_SpeakerLabeling {
	p := new(SpeakerLabelingOptions_SpeakerLabeling)
	*p = x
	return p
}

func (x SpeakerLabelingOptions_SpeakerLabeling) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (SpeakerLabelingOptions_SpeakerLabeling) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[9].Descriptor()
}

func (SpeakerLabelingOptions_SpeakerLabeling) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[9]
}

func (x SpeakerLabelingOptions_SpeakerLabeling) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use SpeakerLabelingOptions_SpeakerLabeling.Descriptor instead.
func (SpeakerLabelingOptions_SpeakerLabeling) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{16, 0}
}

type RecognitionClassifierUpdate_WindowType int32

const (
	// Deprecated: Marked as deprecated in yandex/cloud/ai/stt/v3/stt.proto.
	RecognitionClassifierUpdate_WINDOW_TYPE_UNSPECIFIED RecognitionClassifierUpdate_WindowType = 0
	// The result of applying the classifier to the last utterance response.
	RecognitionClassifierUpdate_LAST_UTTERANCE RecognitionClassifierUpdate_WindowType = 1
	// The result of applying the classifier to the last final response.
	RecognitionClassifierUpdate_LAST_FINAL RecognitionClassifierUpdate_WindowType = 2
	// The result of applying the classifier to the last partial response.
	RecognitionClassifierUpdate_LAST_PARTIAL RecognitionClassifierUpdate_WindowType = 3
)

// Enum value maps for RecognitionClassifierUpdate_WindowType.
var (
	RecognitionClassifierUpdate_WindowType_name = map[int32]string{
		0: "WINDOW_TYPE_UNSPECIFIED",
		1: "LAST_UTTERANCE",
		2: "LAST_FINAL",
		3: "LAST_PARTIAL",
	}
	RecognitionClassifierUpdate_WindowType_value = map[string]int32{
		"WINDOW_TYPE_UNSPECIFIED": 0,
		"LAST_UTTERANCE":          1,
		"LAST_FINAL":              2,
		"LAST_PARTIAL":            3,
	}
)

func (x RecognitionClassifierUpdate_WindowType) Enum() *RecognitionClassifierUpdate_WindowType {
	p := new(RecognitionClassifierUpdate_WindowType)
	*p = x
	return p
}

func (x RecognitionClassifierUpdate_WindowType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (RecognitionClassifierUpdate_WindowType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[10].Descriptor()
}

func (RecognitionClassifierUpdate_WindowType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[10]
}

func (x RecognitionClassifierUpdate_WindowType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use RecognitionClassifierUpdate_WindowType.Descriptor instead.
func (RecognitionClassifierUpdate_WindowType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{35, 0}
}

type SpeakerAnalysis_WindowType int32

const (
	// Deprecated: Marked as deprecated in yandex/cloud/ai/stt/v3/stt.proto.
	SpeakerAnalysis_WINDOW_TYPE_UNSPECIFIED SpeakerAnalysis_WindowType = 0
	// Stats for all received audio.
	SpeakerAnalysis_TOTAL SpeakerAnalysis_WindowType = 1
	// Stats for last utterance.
	SpeakerAnalysis_LAST_UTTERANCE SpeakerAnalysis_WindowType = 2
)

// Enum value maps for SpeakerAnalysis_WindowType.
var (
	SpeakerAnalysis_WindowType_name = map[int32]string{
		0: "WINDOW_TYPE_UNSPECIFIED",
		1: "TOTAL",
		2: "LAST_UTTERANCE",
	}
	SpeakerAnalysis_WindowType_value = map[string]int32{
		"WINDOW_TYPE_UNSPECIFIED": 0,
		"TOTAL":                   1,
		"LAST_UTTERANCE":          2,
	}
)

func (x SpeakerAnalysis_WindowType) Enum() *SpeakerAnalysis_WindowType {
	p := new(SpeakerAnalysis_WindowType)
	*p = x
	return p
}

func (x SpeakerAnalysis_WindowType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (SpeakerAnalysis_WindowType) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[11].Descriptor()
}

func (SpeakerAnalysis_WindowType) Type() protoreflect.EnumType {
	return &file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes[11]
}

func (x SpeakerAnalysis_WindowType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use SpeakerAnalysis_WindowType.Descriptor instead.
func (SpeakerAnalysis_WindowType) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{38, 0}
}

// Options for post-processing text results. The normalization levels depend on the settings and the language.
// For detailed information, see [documentation](/docs/speechkit/stt/normalization).
type TextNormalizationOptions struct {
	state             protoimpl.MessageState                     `protogen:"open.v1"`
	TextNormalization TextNormalizationOptions_TextNormalization `protobuf:"varint,1,opt,name=text_normalization,json=textNormalization,proto3,enum=speechkit.stt.v3.TextNormalizationOptions_TextNormalization" json:"text_normalization,omitempty"`
	// Profanity filter (default: false).
	ProfanityFilter bool `protobuf:"varint,2,opt,name=profanity_filter,json=profanityFilter,proto3" json:"profanity_filter,omitempty"`
	// Rewrite text in literature style (default: false).
	LiteratureText bool `protobuf:"varint,3,opt,name=literature_text,json=literatureText,proto3" json:"literature_text,omitempty"`
	// Define phone formatting mode
	PhoneFormattingMode TextNormalizationOptions_PhoneFormattingMode `protobuf:"varint,4,opt,name=phone_formatting_mode,json=phoneFormattingMode,proto3,enum=speechkit.stt.v3.TextNormalizationOptions_PhoneFormattingMode" json:"phone_formatting_mode,omitempty"`
	unknownFields       protoimpl.UnknownFields
	sizeCache           protoimpl.SizeCache
}

func (x *TextNormalizationOptions) Reset() {
	*x = TextNormalizationOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextNormalizationOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextNormalizationOptions) ProtoMessage() {}

func (x *TextNormalizationOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextNormalizationOptions.ProtoReflect.Descriptor instead.
func (*TextNormalizationOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{0}
}

func (x *TextNormalizationOptions) GetTextNormalization() TextNormalizationOptions_TextNormalization {
	if x != nil {
		return x.TextNormalization
	}
	return TextNormalizationOptions_TEXT_NORMALIZATION_UNSPECIFIED
}

func (x *TextNormalizationOptions) GetProfanityFilter() bool {
	if x != nil {
		return x.ProfanityFilter
	}
	return false
}

func (x *TextNormalizationOptions) GetLiteratureText() bool {
	if x != nil {
		return x.LiteratureText
	}
	return false
}

func (x *TextNormalizationOptions) GetPhoneFormattingMode() TextNormalizationOptions_PhoneFormattingMode {
	if x != nil {
		return x.PhoneFormattingMode
	}
	return TextNormalizationOptions_PHONE_FORMATTING_MODE_UNSPECIFIED
}

type DefaultEouClassifier struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// EOU sensitivity.
	Type DefaultEouClassifier_EouSensitivity `protobuf:"varint,1,opt,name=type,proto3,enum=speechkit.stt.v3.DefaultEouClassifier_EouSensitivity" json:"type,omitempty"`
	// Hint for max pause between words. SpeechKit EOU detector could use this information to adjust the speed of the EOU detection.
	// For example, a long pause between words will help distinguish between the end of utterance from slow speech like `One <long pause> two <long pause> three`.
	// A short pause can be helpful if the speaker is speaking quickly and does not emphasize pauses between sentences.
	MaxPauseBetweenWordsHintMs int64 `protobuf:"varint,2,opt,name=max_pause_between_words_hint_ms,json=maxPauseBetweenWordsHintMs,proto3" json:"max_pause_between_words_hint_ms,omitempty"`
	unknownFields              protoimpl.UnknownFields
	sizeCache                  protoimpl.SizeCache
}

func (x *DefaultEouClassifier) Reset() {
	*x = DefaultEouClassifier{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DefaultEouClassifier) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DefaultEouClassifier) ProtoMessage() {}

func (x *DefaultEouClassifier) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DefaultEouClassifier.ProtoReflect.Descriptor instead.
func (*DefaultEouClassifier) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{1}
}

func (x *DefaultEouClassifier) GetType() DefaultEouClassifier_EouSensitivity {
	if x != nil {
		return x.Type
	}
	return DefaultEouClassifier_EOU_SENSITIVITY_UNSPECIFIED
}

func (x *DefaultEouClassifier) GetMaxPauseBetweenWordsHintMs() int64 {
	if x != nil {
		return x.MaxPauseBetweenWordsHintMs
	}
	return 0
}

// Use EOU provided by user.
type ExternalEouClassifier struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ExternalEouClassifier) Reset() {
	*x = ExternalEouClassifier{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExternalEouClassifier) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExternalEouClassifier) ProtoMessage() {}

func (x *ExternalEouClassifier) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExternalEouClassifier.ProtoReflect.Descriptor instead.
func (*ExternalEouClassifier) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{2}
}

type EouClassifierOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to Classifier:
	//
	//	*EouClassifierOptions_DefaultClassifier
	//	*EouClassifierOptions_ExternalClassifier
	Classifier    isEouClassifierOptions_Classifier `protobuf_oneof:"Classifier"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EouClassifierOptions) Reset() {
	*x = EouClassifierOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EouClassifierOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EouClassifierOptions) ProtoMessage() {}

func (x *EouClassifierOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EouClassifierOptions.ProtoReflect.Descriptor instead.
func (*EouClassifierOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{3}
}

func (x *EouClassifierOptions) GetClassifier() isEouClassifierOptions_Classifier {
	if x != nil {
		return x.Classifier
	}
	return nil
}

func (x *EouClassifierOptions) GetDefaultClassifier() *DefaultEouClassifier {
	if x != nil {
		if x, ok := x.Classifier.(*EouClassifierOptions_DefaultClassifier); ok {
			return x.DefaultClassifier
		}
	}
	return nil
}

func (x *EouClassifierOptions) GetExternalClassifier() *ExternalEouClassifier {
	if x != nil {
		if x, ok := x.Classifier.(*EouClassifierOptions_ExternalClassifier); ok {
			return x.ExternalClassifier
		}
	}
	return nil
}

type isEouClassifierOptions_Classifier interface {
	isEouClassifierOptions_Classifier()
}

type EouClassifierOptions_DefaultClassifier struct {
	// Default EOU classifier provided by SpeechKit.
	DefaultClassifier *DefaultEouClassifier `protobuf:"bytes,1,opt,name=default_classifier,json=defaultClassifier,proto3,oneof"`
}

type EouClassifierOptions_ExternalClassifier struct {
	// EOU classifier enforced by external messages from user.
	ExternalClassifier *ExternalEouClassifier `protobuf:"bytes,2,opt,name=external_classifier,json=externalClassifier,proto3,oneof"`
}

func (*EouClassifierOptions_DefaultClassifier) isEouClassifierOptions_Classifier() {}

func (*EouClassifierOptions_ExternalClassifier) isEouClassifierOptions_Classifier() {}

type RecognitionClassifier struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Classifier name
	Classifier string `protobuf:"bytes,1,opt,name=classifier,proto3" json:"classifier,omitempty"`
	// Describes the types of responses to which the classification results will come. Classification responses will follow the responses of the specified types.
	Triggers      []RecognitionClassifier_TriggerType `protobuf:"varint,2,rep,packed,name=triggers,proto3,enum=speechkit.stt.v3.RecognitionClassifier_TriggerType" json:"triggers,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RecognitionClassifier) Reset() {
	*x = RecognitionClassifier{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognitionClassifier) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognitionClassifier) ProtoMessage() {}

func (x *RecognitionClassifier) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognitionClassifier.ProtoReflect.Descriptor instead.
func (*RecognitionClassifier) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{4}
}

func (x *RecognitionClassifier) GetClassifier() string {
	if x != nil {
		return x.Classifier
	}
	return ""
}

func (x *RecognitionClassifier) GetTriggers() []RecognitionClassifier_TriggerType {
	if x != nil {
		return x.Triggers
	}
	return nil
}

type RecognitionClassifierOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of classifiers to use. For detailed information and usage example, see [documentation](/docs/speechkit/stt/analysis).
	Classifiers   []*RecognitionClassifier `protobuf:"bytes,1,rep,name=classifiers,proto3" json:"classifiers,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RecognitionClassifierOptions) Reset() {
	*x = RecognitionClassifierOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognitionClassifierOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognitionClassifierOptions) ProtoMessage() {}

func (x *RecognitionClassifierOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognitionClassifierOptions.ProtoReflect.Descriptor instead.
func (*RecognitionClassifierOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{5}
}

func (x *RecognitionClassifierOptions) GetClassifiers() []*RecognitionClassifier {
	if x != nil {
		return x.Classifiers
	}
	return nil
}

type SpeechAnalysisOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Analyse speech for every speaker
	EnableSpeakerAnalysis bool `protobuf:"varint,1,opt,name=enable_speaker_analysis,json=enableSpeakerAnalysis,proto3" json:"enable_speaker_analysis,omitempty"`
	// Analyse conversation of two speakers
	EnableConversationAnalysis bool `protobuf:"varint,2,opt,name=enable_conversation_analysis,json=enableConversationAnalysis,proto3" json:"enable_conversation_analysis,omitempty"`
	// Quantile levels in range (0, 1) for descriptive statistics
	DescriptiveStatisticsQuantiles []float64 `protobuf:"fixed64,3,rep,packed,name=descriptive_statistics_quantiles,json=descriptiveStatisticsQuantiles,proto3" json:"descriptive_statistics_quantiles,omitempty"`
	unknownFields                  protoimpl.UnknownFields
	sizeCache                      protoimpl.SizeCache
}

func (x *SpeechAnalysisOptions) Reset() {
	*x = SpeechAnalysisOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeechAnalysisOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeechAnalysisOptions) ProtoMessage() {}

func (x *SpeechAnalysisOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SpeechAnalysisOptions.ProtoReflect.Descriptor instead.
func (*SpeechAnalysisOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{6}
}

func (x *SpeechAnalysisOptions) GetEnableSpeakerAnalysis() bool {
	if x != nil {
		return x.EnableSpeakerAnalysis
	}
	return false
}

func (x *SpeechAnalysisOptions) GetEnableConversationAnalysis() bool {
	if x != nil {
		return x.EnableConversationAnalysis
	}
	return false
}

func (x *SpeechAnalysisOptions) GetDescriptiveStatisticsQuantiles() []float64 {
	if x != nil {
		return x.DescriptiveStatisticsQuantiles
	}
	return nil
}

// RAW Audio format spec (no container to infer type). Used in AudioFormat options.
type RawAudio struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Type of audio encoding.
	AudioEncoding RawAudio_AudioEncoding `protobuf:"varint,1,opt,name=audio_encoding,json=audioEncoding,proto3,enum=speechkit.stt.v3.RawAudio_AudioEncoding" json:"audio_encoding,omitempty"`
	// PCM sample rate.
	SampleRateHertz int64 `protobuf:"varint,2,opt,name=sample_rate_hertz,json=sampleRateHertz,proto3" json:"sample_rate_hertz,omitempty"`
	// PCM channel count. Currently only single channel audio is supported in real-time recognition.
	AudioChannelCount int64 `protobuf:"varint,3,opt,name=audio_channel_count,json=audioChannelCount,proto3" json:"audio_channel_count,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *RawAudio) Reset() {
	*x = RawAudio{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RawAudio) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RawAudio) ProtoMessage() {}

func (x *RawAudio) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RawAudio.ProtoReflect.Descriptor instead.
func (*RawAudio) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{7}
}

func (x *RawAudio) GetAudioEncoding() RawAudio_AudioEncoding {
	if x != nil {
		return x.AudioEncoding
	}
	return RawAudio_AUDIO_ENCODING_UNSPECIFIED
}

func (x *RawAudio) GetSampleRateHertz() int64 {
	if x != nil {
		return x.SampleRateHertz
	}
	return 0
}

func (x *RawAudio) GetAudioChannelCount() int64 {
	if x != nil {
		return x.AudioChannelCount
	}
	return 0
}

// Audio with fixed type in container. Used in AudioFormat options.
type ContainerAudio struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Type of audio container.
	ContainerAudioType ContainerAudio_ContainerAudioType `protobuf:"varint,1,opt,name=container_audio_type,json=containerAudioType,proto3,enum=speechkit.stt.v3.ContainerAudio_ContainerAudioType" json:"container_audio_type,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *ContainerAudio) Reset() {
	*x = ContainerAudio{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ContainerAudio) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ContainerAudio) ProtoMessage() {}

func (x *ContainerAudio) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ContainerAudio.ProtoReflect.Descriptor instead.
func (*ContainerAudio) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{8}
}

func (x *ContainerAudio) GetContainerAudioType() ContainerAudio_ContainerAudioType {
	if x != nil {
		return x.ContainerAudioType
	}
	return ContainerAudio_CONTAINER_AUDIO_TYPE_UNSPECIFIED
}

// Audio format options.
type AudioFormatOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to AudioFormat:
	//
	//	*AudioFormatOptions_RawAudio
	//	*AudioFormatOptions_ContainerAudio
	AudioFormat   isAudioFormatOptions_AudioFormat `protobuf_oneof:"AudioFormat"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AudioFormatOptions) Reset() {
	*x = AudioFormatOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AudioFormatOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AudioFormatOptions) ProtoMessage() {}

func (x *AudioFormatOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AudioFormatOptions.ProtoReflect.Descriptor instead.
func (*AudioFormatOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{9}
}

func (x *AudioFormatOptions) GetAudioFormat() isAudioFormatOptions_AudioFormat {
	if x != nil {
		return x.AudioFormat
	}
	return nil
}

func (x *AudioFormatOptions) GetRawAudio() *RawAudio {
	if x != nil {
		if x, ok := x.AudioFormat.(*AudioFormatOptions_RawAudio); ok {
			return x.RawAudio
		}
	}
	return nil
}

func (x *AudioFormatOptions) GetContainerAudio() *ContainerAudio {
	if x != nil {
		if x, ok := x.AudioFormat.(*AudioFormatOptions_ContainerAudio); ok {
			return x.ContainerAudio
		}
	}
	return nil
}

type isAudioFormatOptions_AudioFormat interface {
	isAudioFormatOptions_AudioFormat()
}

type AudioFormatOptions_RawAudio struct {
	// RAW audio without container.
	RawAudio *RawAudio `protobuf:"bytes,1,opt,name=raw_audio,json=rawAudio,proto3,oneof"`
}

type AudioFormatOptions_ContainerAudio struct {
	// Audio is wrapped in container.
	ContainerAudio *ContainerAudio `protobuf:"bytes,2,opt,name=container_audio,json=containerAudio,proto3,oneof"`
}

func (*AudioFormatOptions_RawAudio) isAudioFormatOptions_AudioFormat() {}

func (*AudioFormatOptions_ContainerAudio) isAudioFormatOptions_AudioFormat() {}

// Type of restriction for the list of languages expected in the incoming audio.
type LanguageRestrictionOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Language restriction type.
	// All of these restrictions are used by the model as guidelines, not as strict rules.
	// The language is recognized for each sentence. If a sentence has phrases in different languages, all of them will be transcribed in the most probable language.
	RestrictionType LanguageRestrictionOptions_LanguageRestrictionType `protobuf:"varint,1,opt,name=restriction_type,json=restrictionType,proto3,enum=speechkit.stt.v3.LanguageRestrictionOptions_LanguageRestrictionType" json:"restriction_type,omitempty"`
	// The list of [language codes](/docs/speechkit/stt/models) to restrict recognition in the case of an auto model.
	LanguageCode  []string `protobuf:"bytes,2,rep,name=language_code,json=languageCode,proto3" json:"language_code,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LanguageRestrictionOptions) Reset() {
	*x = LanguageRestrictionOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LanguageRestrictionOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LanguageRestrictionOptions) ProtoMessage() {}

func (x *LanguageRestrictionOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LanguageRestrictionOptions.ProtoReflect.Descriptor instead.
func (*LanguageRestrictionOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{10}
}

func (x *LanguageRestrictionOptions) GetRestrictionType() LanguageRestrictionOptions_LanguageRestrictionType {
	if x != nil {
		return x.RestrictionType
	}
	return LanguageRestrictionOptions_LANGUAGE_RESTRICTION_TYPE_UNSPECIFIED
}

func (x *LanguageRestrictionOptions) GetLanguageCode() []string {
	if x != nil {
		return x.LanguageCode
	}
	return nil
}

// Represents the expected structure of the model's response using a JSON Schema.
type JsonSchema struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The JSON Schema that the model's output must conform to.
	Schema        *structpb.Struct `protobuf:"bytes,1,opt,name=schema,proto3" json:"schema,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *JsonSchema) Reset() {
	*x = JsonSchema{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *JsonSchema) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*JsonSchema) ProtoMessage() {}

func (x *JsonSchema) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use JsonSchema.ProtoReflect.Descriptor instead.
func (*JsonSchema) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{11}
}

func (x *JsonSchema) GetSchema() *structpb.Struct {
	if x != nil {
		return x.Schema
	}
	return nil
}

// Represents summarization entry for transcription.
type SummarizationProperty struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Summarization instruction for model.
	Instruction string `protobuf:"bytes,1,opt,name=instruction,proto3" json:"instruction,omitempty"`
	// Specifies the format of the model's response.
	//
	// Types that are valid to be assigned to ResponseFormat:
	//
	//	*SummarizationProperty_JsonObject
	//	*SummarizationProperty_JsonSchema
	ResponseFormat isSummarizationProperty_ResponseFormat `protobuf_oneof:"ResponseFormat"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *SummarizationProperty) Reset() {
	*x = SummarizationProperty{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummarizationProperty) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummarizationProperty) ProtoMessage() {}

func (x *SummarizationProperty) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummarizationProperty.ProtoReflect.Descriptor instead.
func (*SummarizationProperty) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{12}
}

func (x *SummarizationProperty) GetInstruction() string {
	if x != nil {
		return x.Instruction
	}
	return ""
}

func (x *SummarizationProperty) GetResponseFormat() isSummarizationProperty_ResponseFormat {
	if x != nil {
		return x.ResponseFormat
	}
	return nil
}

func (x *SummarizationProperty) GetJsonObject() bool {
	if x != nil {
		if x, ok := x.ResponseFormat.(*SummarizationProperty_JsonObject); ok {
			return x.JsonObject
		}
	}
	return false
}

func (x *SummarizationProperty) GetJsonSchema() *JsonSchema {
	if x != nil {
		if x, ok := x.ResponseFormat.(*SummarizationProperty_JsonSchema); ok {
			return x.JsonSchema
		}
	}
	return nil
}

type isSummarizationProperty_ResponseFormat interface {
	isSummarizationProperty_ResponseFormat()
}

type SummarizationProperty_JsonObject struct {
	// When set to true, the model will return a valid JSON object.
	// Be sure to ask the model explicitly for JSON.
	// Otherwise, it may produce excessive whitespace and run indefinitely until it reaches the token limit.
	JsonObject bool `protobuf:"varint,2,opt,name=json_object,json=jsonObject,proto3,oneof"`
}

type SummarizationProperty_JsonSchema struct {
	// Enforces a specific JSON structure for the model's response based on a provided schema.
	JsonSchema *JsonSchema `protobuf:"bytes,3,opt,name=json_schema,json=jsonSchema,proto3,oneof"`
}

func (*SummarizationProperty_JsonObject) isSummarizationProperty_ResponseFormat() {}

func (*SummarizationProperty_JsonSchema) isSummarizationProperty_ResponseFormat() {}

// Represents transcription summarization options.
type SummarizationOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The [ID of the model](/docs/foundation-models/concepts/yandexgpt/models) to be used for completion generation.
	ModelUri string `protobuf:"bytes,1,opt,name=model_uri,json=modelUri,proto3" json:"model_uri,omitempty"`
	// A list of suimmarizations to perform with transcription.
	Properties    []*SummarizationProperty `protobuf:"bytes,2,rep,name=properties,proto3" json:"properties,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SummarizationOptions) Reset() {
	*x = SummarizationOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummarizationOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummarizationOptions) ProtoMessage() {}

func (x *SummarizationOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummarizationOptions.ProtoReflect.Descriptor instead.
func (*SummarizationOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{13}
}

func (x *SummarizationOptions) GetModelUri() string {
	if x != nil {
		return x.ModelUri
	}
	return ""
}

func (x *SummarizationOptions) GetProperties() []*SummarizationProperty {
	if x != nil {
		return x.Properties
	}
	return nil
}

// Represents summarization response entry for transcription.
type SummarizationPropertyResult struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Summarization response text.
	Response      string `protobuf:"bytes,1,opt,name=response,proto3" json:"response,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SummarizationPropertyResult) Reset() {
	*x = SummarizationPropertyResult{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SummarizationPropertyResult) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SummarizationPropertyResult) ProtoMessage() {}

func (x *SummarizationPropertyResult) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SummarizationPropertyResult.ProtoReflect.Descriptor instead.
func (*SummarizationPropertyResult) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{14}
}

func (x *SummarizationPropertyResult) GetResponse() string {
	if x != nil {
		return x.Response
	}
	return ""
}

type RecognitionModelOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Sets the recognition model for the cloud version of SpeechKit.
	// For `Recognizer.RecognizeStreaming`, possible values are `general`, `general:rc`, `general:deprecated`.
	// For `AsyncRecognizer.RecognizeFile`, possible values are `general`, `general:rc`, `general:deprecated`, `deferred-general`, `deferred-general:rc`, and `deferred-general:deprecated`.
	// The model is ignored for SpeechKit Hybrid.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Specified input audio.
	AudioFormat *AudioFormatOptions `protobuf:"bytes,2,opt,name=audio_format,json=audioFormat,proto3" json:"audio_format,omitempty"`
	// Text normalization options.
	TextNormalization *TextNormalizationOptions `protobuf:"bytes,3,opt,name=text_normalization,json=textNormalization,proto3" json:"text_normalization,omitempty"`
	// Possible languages in audio.
	LanguageRestriction *LanguageRestrictionOptions `protobuf:"bytes,4,opt,name=language_restriction,json=languageRestriction,proto3" json:"language_restriction,omitempty"`
	// For `Recognizer.RecognizeStreaming`, defines the audio data processing mode. Default is `REAL_TIME`.
	// For `AsyncRecognizer.RecognizeFile`, this field is ignored.
	AudioProcessingType RecognitionModelOptions_AudioProcessingType `protobuf:"varint,5,opt,name=audio_processing_type,json=audioProcessingType,proto3,enum=speechkit.stt.v3.RecognitionModelOptions_AudioProcessingType" json:"audio_processing_type,omitempty"`
	unknownFields       protoimpl.UnknownFields
	sizeCache           protoimpl.SizeCache
}

func (x *RecognitionModelOptions) Reset() {
	*x = RecognitionModelOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognitionModelOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognitionModelOptions) ProtoMessage() {}

func (x *RecognitionModelOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognitionModelOptions.ProtoReflect.Descriptor instead.
func (*RecognitionModelOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{15}
}

func (x *RecognitionModelOptions) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *RecognitionModelOptions) GetAudioFormat() *AudioFormatOptions {
	if x != nil {
		return x.AudioFormat
	}
	return nil
}

func (x *RecognitionModelOptions) GetTextNormalization() *TextNormalizationOptions {
	if x != nil {
		return x.TextNormalization
	}
	return nil
}

func (x *RecognitionModelOptions) GetLanguageRestriction() *LanguageRestrictionOptions {
	if x != nil {
		return x.LanguageRestriction
	}
	return nil
}

func (x *RecognitionModelOptions) GetAudioProcessingType() RecognitionModelOptions_AudioProcessingType {
	if x != nil {
		return x.AudioProcessingType
	}
	return RecognitionModelOptions_AUDIO_PROCESSING_TYPE_UNSPECIFIED
}

type SpeakerLabelingOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Specifies the execution of speaker labeling.
	SpeakerLabeling SpeakerLabelingOptions_SpeakerLabeling `protobuf:"varint,1,opt,name=speaker_labeling,json=speakerLabeling,proto3,enum=speechkit.stt.v3.SpeakerLabelingOptions_SpeakerLabeling" json:"speaker_labeling,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *SpeakerLabelingOptions) Reset() {
	*x = SpeakerLabelingOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeakerLabelingOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeakerLabelingOptions) ProtoMessage() {}

func (x *SpeakerLabelingOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SpeakerLabelingOptions.ProtoReflect.Descriptor instead.
func (*SpeakerLabelingOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{16}
}

func (x *SpeakerLabelingOptions) GetSpeakerLabeling() SpeakerLabelingOptions_SpeakerLabeling {
	if x != nil {
		return x.SpeakerLabeling
	}
	return SpeakerLabelingOptions_SPEAKER_LABELING_UNSPECIFIED
}

type StreamingOptions struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Configuration for speech recognition model.
	RecognitionModel *RecognitionModelOptions `protobuf:"bytes,1,opt,name=recognition_model,json=recognitionModel,proto3" json:"recognition_model,omitempty"`
	// Configuration for an end of utterance detection model.
	EouClassifier *EouClassifierOptions `protobuf:"bytes,2,opt,name=eou_classifier,json=eouClassifier,proto3" json:"eou_classifier,omitempty"`
	// Configuration for classifiers over speech recognition.
	RecognitionClassifier *RecognitionClassifierOptions `protobuf:"bytes,3,opt,name=recognition_classifier,json=recognitionClassifier,proto3" json:"recognition_classifier,omitempty"`
	// Configuration for speech analysis over speech recognition.
	SpeechAnalysis *SpeechAnalysisOptions `protobuf:"bytes,4,opt,name=speech_analysis,json=speechAnalysis,proto3" json:"speech_analysis,omitempty"`
	// Configuration for speaker labeling.
	SpeakerLabeling *SpeakerLabelingOptions `protobuf:"bytes,5,opt,name=speaker_labeling,json=speakerLabeling,proto3" json:"speaker_labeling,omitempty"`
	// Summarization options.
	Summarization *SummarizationOptions `protobuf:"bytes,6,opt,name=summarization,proto3" json:"summarization,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingOptions) Reset() {
	*x = StreamingOptions{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingOptions) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingOptions) ProtoMessage() {}

func (x *StreamingOptions) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingOptions.ProtoReflect.Descriptor instead.
func (*StreamingOptions) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{17}
}

func (x *StreamingOptions) GetRecognitionModel() *RecognitionModelOptions {
	if x != nil {
		return x.RecognitionModel
	}
	return nil
}

func (x *StreamingOptions) GetEouClassifier() *EouClassifierOptions {
	if x != nil {
		return x.EouClassifier
	}
	return nil
}

func (x *StreamingOptions) GetRecognitionClassifier() *RecognitionClassifierOptions {
	if x != nil {
		return x.RecognitionClassifier
	}
	return nil
}

func (x *StreamingOptions) GetSpeechAnalysis() *SpeechAnalysisOptions {
	if x != nil {
		return x.SpeechAnalysis
	}
	return nil
}

func (x *StreamingOptions) GetSpeakerLabeling() *SpeakerLabelingOptions {
	if x != nil {
		return x.SpeakerLabeling
	}
	return nil
}

func (x *StreamingOptions) GetSummarization() *SummarizationOptions {
	if x != nil {
		return x.Summarization
	}
	return nil
}

// Data chunk with audio.
type AudioChunk struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Bytes with audio data.
	Data          []byte `protobuf:"bytes,1,opt,name=data,proto3" json:"data,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AudioChunk) Reset() {
	*x = AudioChunk{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AudioChunk) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AudioChunk) ProtoMessage() {}

func (x *AudioChunk) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AudioChunk.ProtoReflect.Descriptor instead.
func (*AudioChunk) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{18}
}

func (x *AudioChunk) GetData() []byte {
	if x != nil {
		return x.Data
	}
	return nil
}

// Data chunk with silence.
type SilenceChunk struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Duration of silence chunk in ms.
	DurationMs    int64 `protobuf:"varint,1,opt,name=duration_ms,json=durationMs,proto3" json:"duration_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SilenceChunk) Reset() {
	*x = SilenceChunk{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SilenceChunk) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SilenceChunk) ProtoMessage() {}

func (x *SilenceChunk) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SilenceChunk.ProtoReflect.Descriptor instead.
func (*SilenceChunk) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{19}
}

func (x *SilenceChunk) GetDurationMs() int64 {
	if x != nil {
		return x.DurationMs
	}
	return 0
}

// Force EOU.
type Eou struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Eou) Reset() {
	*x = Eou{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Eou) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Eou) ProtoMessage() {}

func (x *Eou) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Eou.ProtoReflect.Descriptor instead.
func (*Eou) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{20}
}

// Streaming audio request.
// Events are control messages from user. First message should be session options. The next messages are audio data chunks or control messages.
type StreamingRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to Event:
	//
	//	*StreamingRequest_SessionOptions
	//	*StreamingRequest_Chunk
	//	*StreamingRequest_SilenceChunk
	//	*StreamingRequest_Eou
	Event         isStreamingRequest_Event `protobuf_oneof:"Event"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingRequest) Reset() {
	*x = StreamingRequest{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingRequest) ProtoMessage() {}

func (x *StreamingRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingRequest.ProtoReflect.Descriptor instead.
func (*StreamingRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{21}
}

func (x *StreamingRequest) GetEvent() isStreamingRequest_Event {
	if x != nil {
		return x.Event
	}
	return nil
}

func (x *StreamingRequest) GetSessionOptions() *StreamingOptions {
	if x != nil {
		if x, ok := x.Event.(*StreamingRequest_SessionOptions); ok {
			return x.SessionOptions
		}
	}
	return nil
}

func (x *StreamingRequest) GetChunk() *AudioChunk {
	if x != nil {
		if x, ok := x.Event.(*StreamingRequest_Chunk); ok {
			return x.Chunk
		}
	}
	return nil
}

func (x *StreamingRequest) GetSilenceChunk() *SilenceChunk {
	if x != nil {
		if x, ok := x.Event.(*StreamingRequest_SilenceChunk); ok {
			return x.SilenceChunk
		}
	}
	return nil
}

func (x *StreamingRequest) GetEou() *Eou {
	if x != nil {
		if x, ok := x.Event.(*StreamingRequest_Eou); ok {
			return x.Eou
		}
	}
	return nil
}

type isStreamingRequest_Event interface {
	isStreamingRequest_Event()
}

type StreamingRequest_SessionOptions struct {
	// Session options. Should be the first message from user.
	SessionOptions *StreamingOptions `protobuf:"bytes,1,opt,name=session_options,json=sessionOptions,proto3,oneof"`
}

type StreamingRequest_Chunk struct {
	// Chunk with audio data.
	Chunk *AudioChunk `protobuf:"bytes,2,opt,name=chunk,proto3,oneof"`
}

type StreamingRequest_SilenceChunk struct {
	// Chunk with silence.
	SilenceChunk *SilenceChunk `protobuf:"bytes,3,opt,name=silence_chunk,json=silenceChunk,proto3,oneof"`
}

type StreamingRequest_Eou struct {
	// Request to end current utterance. Works only with external EOU detector.
	Eou *Eou `protobuf:"bytes,4,opt,name=eou,proto3,oneof"`
}

func (*StreamingRequest_SessionOptions) isStreamingRequest_Event() {}

func (*StreamingRequest_Chunk) isStreamingRequest_Event() {}

func (*StreamingRequest_SilenceChunk) isStreamingRequest_Event() {}

func (*StreamingRequest_Eou) isStreamingRequest_Event() {}

type RecognizeFileRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to AudioSource:
	//
	//	*RecognizeFileRequest_Content
	//	*RecognizeFileRequest_Uri
	AudioSource isRecognizeFileRequest_AudioSource `protobuf_oneof:"AudioSource"`
	// Configuration for speech recognition model.
	RecognitionModel *RecognitionModelOptions `protobuf:"bytes,3,opt,name=recognition_model,json=recognitionModel,proto3" json:"recognition_model,omitempty"`
	// Configuration for classifiers over speech recognition.
	RecognitionClassifier *RecognitionClassifierOptions `protobuf:"bytes,4,opt,name=recognition_classifier,json=recognitionClassifier,proto3" json:"recognition_classifier,omitempty"`
	// Configuration for speech analysis over speech recognition.
	SpeechAnalysis *SpeechAnalysisOptions `protobuf:"bytes,5,opt,name=speech_analysis,json=speechAnalysis,proto3" json:"speech_analysis,omitempty"`
	// Configuration for speaker labeling
	SpeakerLabeling *SpeakerLabelingOptions `protobuf:"bytes,6,opt,name=speaker_labeling,json=speakerLabeling,proto3" json:"speaker_labeling,omitempty"`
	// Summarization options
	Summarization *SummarizationOptions `protobuf:"bytes,7,opt,name=summarization,proto3" json:"summarization,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RecognizeFileRequest) Reset() {
	*x = RecognizeFileRequest{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[22]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognizeFileRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognizeFileRequest) ProtoMessage() {}

func (x *RecognizeFileRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[22]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognizeFileRequest.ProtoReflect.Descriptor instead.
func (*RecognizeFileRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{22}
}

func (x *RecognizeFileRequest) GetAudioSource() isRecognizeFileRequest_AudioSource {
	if x != nil {
		return x.AudioSource
	}
	return nil
}

func (x *RecognizeFileRequest) GetContent() []byte {
	if x != nil {
		if x, ok := x.AudioSource.(*RecognizeFileRequest_Content); ok {
			return x.Content
		}
	}
	return nil
}

func (x *RecognizeFileRequest) GetUri() string {
	if x != nil {
		if x, ok := x.AudioSource.(*RecognizeFileRequest_Uri); ok {
			return x.Uri
		}
	}
	return ""
}

func (x *RecognizeFileRequest) GetRecognitionModel() *RecognitionModelOptions {
	if x != nil {
		return x.RecognitionModel
	}
	return nil
}

func (x *RecognizeFileRequest) GetRecognitionClassifier() *RecognitionClassifierOptions {
	if x != nil {
		return x.RecognitionClassifier
	}
	return nil
}

func (x *RecognizeFileRequest) GetSpeechAnalysis() *SpeechAnalysisOptions {
	if x != nil {
		return x.SpeechAnalysis
	}
	return nil
}

func (x *RecognizeFileRequest) GetSpeakerLabeling() *SpeakerLabelingOptions {
	if x != nil {
		return x.SpeakerLabeling
	}
	return nil
}

func (x *RecognizeFileRequest) GetSummarization() *SummarizationOptions {
	if x != nil {
		return x.Summarization
	}
	return nil
}

type isRecognizeFileRequest_AudioSource interface {
	isRecognizeFileRequest_AudioSource()
}

type RecognizeFileRequest_Content struct {
	// Bytes with data
	Content []byte `protobuf:"bytes,1,opt,name=content,proto3,oneof"`
}

type RecognizeFileRequest_Uri struct {
	// S3 data URL
	Uri string `protobuf:"bytes,2,opt,name=uri,proto3,oneof"`
}

func (*RecognizeFileRequest_Content) isRecognizeFileRequest_AudioSource() {}

func (*RecognizeFileRequest_Uri) isRecognizeFileRequest_AudioSource() {}

// Recognized word.
type Word struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Word text.
	Text string `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	// Estimation of word start time in ms.
	StartTimeMs int64 `protobuf:"varint,2,opt,name=start_time_ms,json=startTimeMs,proto3" json:"start_time_ms,omitempty"`
	// Estimation of word end time in ms.
	EndTimeMs     int64 `protobuf:"varint,3,opt,name=end_time_ms,json=endTimeMs,proto3" json:"end_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Word) Reset() {
	*x = Word{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[23]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Word) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Word) ProtoMessage() {}

func (x *Word) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[23]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Word.ProtoReflect.Descriptor instead.
func (*Word) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{23}
}

func (x *Word) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *Word) GetStartTimeMs() int64 {
	if x != nil {
		return x.StartTimeMs
	}
	return 0
}

func (x *Word) GetEndTimeMs() int64 {
	if x != nil {
		return x.EndTimeMs
	}
	return 0
}

// Estimation of language and its probability.
type LanguageEstimation struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Language tag in IETF BCP 47 format, consisting of ISO 639-1 language code and ISO 3166-1 country code (e.g., en-US, ru-RU).
	LanguageCode string `protobuf:"bytes,1,opt,name=language_code,json=languageCode,proto3" json:"language_code,omitempty"`
	// Estimation of language probability.
	Probability   float64 `protobuf:"fixed64,2,opt,name=probability,proto3" json:"probability,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LanguageEstimation) Reset() {
	*x = LanguageEstimation{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LanguageEstimation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LanguageEstimation) ProtoMessage() {}

func (x *LanguageEstimation) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LanguageEstimation.ProtoReflect.Descriptor instead.
func (*LanguageEstimation) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{24}
}

func (x *LanguageEstimation) GetLanguageCode() string {
	if x != nil {
		return x.LanguageCode
	}
	return ""
}

func (x *LanguageEstimation) GetProbability() float64 {
	if x != nil {
		return x.Probability
	}
	return 0
}

// Recognition of specific time frame.
type Alternative struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Words in time frame.
	Words []*Word `protobuf:"bytes,1,rep,name=words,proto3" json:"words,omitempty"`
	// Text in time frame.
	Text string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	// Start of time frame.
	StartTimeMs int64 `protobuf:"varint,3,opt,name=start_time_ms,json=startTimeMs,proto3" json:"start_time_ms,omitempty"`
	// End of time frame.
	EndTimeMs int64 `protobuf:"varint,4,opt,name=end_time_ms,json=endTimeMs,proto3" json:"end_time_ms,omitempty"`
	// The hypothesis confidence. Currently is not used.
	Confidence float64 `protobuf:"fixed64,5,opt,name=confidence,proto3" json:"confidence,omitempty"`
	// Distribution over possible languages.
	Languages     []*LanguageEstimation `protobuf:"bytes,6,rep,name=languages,proto3" json:"languages,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Alternative) Reset() {
	*x = Alternative{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[25]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Alternative) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Alternative) ProtoMessage() {}

func (x *Alternative) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[25]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Alternative.ProtoReflect.Descriptor instead.
func (*Alternative) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{25}
}

func (x *Alternative) GetWords() []*Word {
	if x != nil {
		return x.Words
	}
	return nil
}

func (x *Alternative) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *Alternative) GetStartTimeMs() int64 {
	if x != nil {
		return x.StartTimeMs
	}
	return 0
}

func (x *Alternative) GetEndTimeMs() int64 {
	if x != nil {
		return x.EndTimeMs
	}
	return 0
}

func (x *Alternative) GetConfidence() float64 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

func (x *Alternative) GetLanguages() []*LanguageEstimation {
	if x != nil {
		return x.Languages
	}
	return nil
}

// Update information for external End of Utterance.
type EouUpdate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// EOU estimated time.
	TimeMs        int64 `protobuf:"varint,2,opt,name=time_ms,json=timeMs,proto3" json:"time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EouUpdate) Reset() {
	*x = EouUpdate{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[26]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EouUpdate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EouUpdate) ProtoMessage() {}

func (x *EouUpdate) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[26]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EouUpdate.ProtoReflect.Descriptor instead.
func (*EouUpdate) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{26}
}

func (x *EouUpdate) GetTimeMs() int64 {
	if x != nil {
		return x.TimeMs
	}
	return 0
}

// Update of hypothesis.
type AlternativeUpdate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// List of hypothesis for timeframes.
	Alternatives []*Alternative `protobuf:"bytes,1,rep,name=alternatives,proto3" json:"alternatives,omitempty"`
	// Deprecated: Marked as deprecated in yandex/cloud/ai/stt/v3/stt.proto.
	ChannelTag    string `protobuf:"bytes,2,opt,name=channel_tag,json=channelTag,proto3" json:"channel_tag,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AlternativeUpdate) Reset() {
	*x = AlternativeUpdate{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[27]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AlternativeUpdate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AlternativeUpdate) ProtoMessage() {}

func (x *AlternativeUpdate) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[27]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AlternativeUpdate.ProtoReflect.Descriptor instead.
func (*AlternativeUpdate) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{27}
}

func (x *AlternativeUpdate) GetAlternatives() []*Alternative {
	if x != nil {
		return x.Alternatives
	}
	return nil
}

// Deprecated: Marked as deprecated in yandex/cloud/ai/stt/v3/stt.proto.
func (x *AlternativeUpdate) GetChannelTag() string {
	if x != nil {
		return x.ChannelTag
	}
	return ""
}

// AudioCursors are state of ASR recognition stream.
type AudioCursors struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Amount of audio chunks server received. This cursor is moved after each audio chunk was received by server.
	ReceivedDataMs int64 `protobuf:"varint,1,opt,name=received_data_ms,json=receivedDataMs,proto3" json:"received_data_ms,omitempty"`
	// Input stream reset data.
	ResetTimeMs int64 `protobuf:"varint,2,opt,name=reset_time_ms,json=resetTimeMs,proto3" json:"reset_time_ms,omitempty"`
	// How much audio was processed. This time includes trimming silences as well.
	// This cursor is moved after server received enough data to update recognition results (includes silence as well).
	PartialTimeMs int64 `protobuf:"varint,3,opt,name=partial_time_ms,json=partialTimeMs,proto3" json:"partial_time_ms,omitempty"`
	// Time of last final. This cursor is moved when server decides that recognition from start of audio until `final_time_ms` will not change anymore
	// usually this event is followed by EOU detection. This behavior could change in future.
	FinalTimeMs int64 `protobuf:"varint,4,opt,name=final_time_ms,json=finalTimeMs,proto3" json:"final_time_ms,omitempty"`
	// This is index of last final server send. Incremented after each new final.
	FinalIndex int64 `protobuf:"varint,5,opt,name=final_index,json=finalIndex,proto3" json:"final_index,omitempty"`
	// Estimated time of EOU. Cursor is updated after each new EOU is sent.
	// For external classifier this equals to [received_data_ms] at the moment EOU event arrives.
	// For internal classifier this is estimation of time. The time is not exact and has the same guarantees as word timings.
	EouTimeMs     int64 `protobuf:"varint,6,opt,name=eou_time_ms,json=eouTimeMs,proto3" json:"eou_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AudioCursors) Reset() {
	*x = AudioCursors{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[28]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AudioCursors) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AudioCursors) ProtoMessage() {}

func (x *AudioCursors) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[28]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AudioCursors.ProtoReflect.Descriptor instead.
func (*AudioCursors) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{28}
}

func (x *AudioCursors) GetReceivedDataMs() int64 {
	if x != nil {
		return x.ReceivedDataMs
	}
	return 0
}

func (x *AudioCursors) GetResetTimeMs() int64 {
	if x != nil {
		return x.ResetTimeMs
	}
	return 0
}

func (x *AudioCursors) GetPartialTimeMs() int64 {
	if x != nil {
		return x.PartialTimeMs
	}
	return 0
}

func (x *AudioCursors) GetFinalTimeMs() int64 {
	if x != nil {
		return x.FinalTimeMs
	}
	return 0
}

func (x *AudioCursors) GetFinalIndex() int64 {
	if x != nil {
		return x.FinalIndex
	}
	return 0
}

func (x *AudioCursors) GetEouTimeMs() int64 {
	if x != nil {
		return x.EouTimeMs
	}
	return 0
}

// Refinement for final hypo. For example, text normalization is refinement.
type FinalRefinement struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Index of final for which server sends additional information.
	FinalIndex int64 `protobuf:"varint,1,opt,name=final_index,json=finalIndex,proto3" json:"final_index,omitempty"`
	// Type of refinement.
	//
	// Types that are valid to be assigned to Type:
	//
	//	*FinalRefinement_NormalizedText
	Type          isFinalRefinement_Type `protobuf_oneof:"Type"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *FinalRefinement) Reset() {
	*x = FinalRefinement{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[29]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *FinalRefinement) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*FinalRefinement) ProtoMessage() {}

func (x *FinalRefinement) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[29]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use FinalRefinement.ProtoReflect.Descriptor instead.
func (*FinalRefinement) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{29}
}

func (x *FinalRefinement) GetFinalIndex() int64 {
	if x != nil {
		return x.FinalIndex
	}
	return 0
}

func (x *FinalRefinement) GetType() isFinalRefinement_Type {
	if x != nil {
		return x.Type
	}
	return nil
}

func (x *FinalRefinement) GetNormalizedText() *AlternativeUpdate {
	if x != nil {
		if x, ok := x.Type.(*FinalRefinement_NormalizedText); ok {
			return x.NormalizedText
		}
	}
	return nil
}

type isFinalRefinement_Type interface {
	isFinalRefinement_Type()
}

type FinalRefinement_NormalizedText struct {
	// Normalized text instead of raw one.
	NormalizedText *AlternativeUpdate `protobuf:"bytes,2,opt,name=normalized_text,json=normalizedText,proto3,oneof"`
}

func (*FinalRefinement_NormalizedText) isFinalRefinement_Type() {}

// Status message.
type StatusCode struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Code type.
	CodeType CodeType `protobuf:"varint,1,opt,name=code_type,json=codeType,proto3,enum=speechkit.stt.v3.CodeType" json:"code_type,omitempty"`
	// Human readable message.
	Message       string `protobuf:"bytes,2,opt,name=message,proto3" json:"message,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StatusCode) Reset() {
	*x = StatusCode{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[30]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StatusCode) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StatusCode) ProtoMessage() {}

func (x *StatusCode) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[30]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StatusCode.ProtoReflect.Descriptor instead.
func (*StatusCode) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{30}
}

func (x *StatusCode) GetCodeType() CodeType {
	if x != nil {
		return x.CodeType
	}
	return CodeType_CODE_TYPE_UNSPECIFIED
}

func (x *StatusCode) GetMessage() string {
	if x != nil {
		return x.Message
	}
	return ""
}

// Session identifier.
type SessionUuid struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Internal session identifier.
	Uuid string `protobuf:"bytes,1,opt,name=uuid,proto3" json:"uuid,omitempty"`
	// User session identifier.
	UserRequestId string `protobuf:"bytes,2,opt,name=user_request_id,json=userRequestId,proto3" json:"user_request_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SessionUuid) Reset() {
	*x = SessionUuid{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[31]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SessionUuid) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SessionUuid) ProtoMessage() {}

func (x *SessionUuid) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[31]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SessionUuid.ProtoReflect.Descriptor instead.
func (*SessionUuid) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{31}
}

func (x *SessionUuid) GetUuid() string {
	if x != nil {
		return x.Uuid
	}
	return ""
}

func (x *SessionUuid) GetUserRequestId() string {
	if x != nil {
		return x.UserRequestId
	}
	return ""
}

type PhraseHighlight struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Text transcription of the highlighted audio segment.
	Text string `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	// Start time of the highlighted audio segment.
	StartTimeMs int64 `protobuf:"varint,2,opt,name=start_time_ms,json=startTimeMs,proto3" json:"start_time_ms,omitempty"`
	// End time of the highlighted audio segment.
	EndTimeMs     int64 `protobuf:"varint,3,opt,name=end_time_ms,json=endTimeMs,proto3" json:"end_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PhraseHighlight) Reset() {
	*x = PhraseHighlight{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[32]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PhraseHighlight) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PhraseHighlight) ProtoMessage() {}

func (x *PhraseHighlight) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[32]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PhraseHighlight.ProtoReflect.Descriptor instead.
func (*PhraseHighlight) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{32}
}

func (x *PhraseHighlight) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *PhraseHighlight) GetStartTimeMs() int64 {
	if x != nil {
		return x.StartTimeMs
	}
	return 0
}

func (x *PhraseHighlight) GetEndTimeMs() int64 {
	if x != nil {
		return x.EndTimeMs
	}
	return 0
}

type RecognitionClassifierLabel struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The label of the class predicted by the classifier.
	Label string `protobuf:"bytes,1,opt,name=label,proto3" json:"label,omitempty"`
	// The prediction confidence.
	Confidence    float64 `protobuf:"fixed64,2,opt,name=confidence,proto3" json:"confidence,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RecognitionClassifierLabel) Reset() {
	*x = RecognitionClassifierLabel{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[33]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognitionClassifierLabel) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognitionClassifierLabel) ProtoMessage() {}

func (x *RecognitionClassifierLabel) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[33]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognitionClassifierLabel.ProtoReflect.Descriptor instead.
func (*RecognitionClassifierLabel) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{33}
}

func (x *RecognitionClassifierLabel) GetLabel() string {
	if x != nil {
		return x.Label
	}
	return ""
}

func (x *RecognitionClassifierLabel) GetConfidence() float64 {
	if x != nil {
		return x.Confidence
	}
	return 0
}

type RecognitionClassifierResult struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Name of the triggered classifier.
	Classifier string `protobuf:"bytes,1,opt,name=classifier,proto3" json:"classifier,omitempty"`
	// List of highlights, i.e. parts of phrase that determine the result of the classification.
	Highlights []*PhraseHighlight `protobuf:"bytes,2,rep,name=highlights,proto3" json:"highlights,omitempty"`
	// Classifier predictions.
	Labels        []*RecognitionClassifierLabel `protobuf:"bytes,3,rep,name=labels,proto3" json:"labels,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RecognitionClassifierResult) Reset() {
	*x = RecognitionClassifierResult{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[34]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognitionClassifierResult) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognitionClassifierResult) ProtoMessage() {}

func (x *RecognitionClassifierResult) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[34]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognitionClassifierResult.ProtoReflect.Descriptor instead.
func (*RecognitionClassifierResult) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{34}
}

func (x *RecognitionClassifierResult) GetClassifier() string {
	if x != nil {
		return x.Classifier
	}
	return ""
}

func (x *RecognitionClassifierResult) GetHighlights() []*PhraseHighlight {
	if x != nil {
		return x.Highlights
	}
	return nil
}

func (x *RecognitionClassifierResult) GetLabels() []*RecognitionClassifierLabel {
	if x != nil {
		return x.Labels
	}
	return nil
}

type RecognitionClassifierUpdate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Response window type.
	WindowType RecognitionClassifierUpdate_WindowType `protobuf:"varint,1,opt,name=window_type,json=windowType,proto3,enum=speechkit.stt.v3.RecognitionClassifierUpdate_WindowType" json:"window_type,omitempty"`
	// Start time of the audio segment used for classification.
	StartTimeMs int64 `protobuf:"varint,2,opt,name=start_time_ms,json=startTimeMs,proto3" json:"start_time_ms,omitempty"`
	// End time of the audio segment used for classification.
	EndTimeMs int64 `protobuf:"varint,3,opt,name=end_time_ms,json=endTimeMs,proto3" json:"end_time_ms,omitempty"`
	// Result for dictionary-based classifier.
	ClassifierResult *RecognitionClassifierResult `protobuf:"bytes,4,opt,name=classifier_result,json=classifierResult,proto3" json:"classifier_result,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *RecognitionClassifierUpdate) Reset() {
	*x = RecognitionClassifierUpdate{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[35]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RecognitionClassifierUpdate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RecognitionClassifierUpdate) ProtoMessage() {}

func (x *RecognitionClassifierUpdate) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[35]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RecognitionClassifierUpdate.ProtoReflect.Descriptor instead.
func (*RecognitionClassifierUpdate) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{35}
}

func (x *RecognitionClassifierUpdate) GetWindowType() RecognitionClassifierUpdate_WindowType {
	if x != nil {
		return x.WindowType
	}
	return RecognitionClassifierUpdate_WINDOW_TYPE_UNSPECIFIED
}

func (x *RecognitionClassifierUpdate) GetStartTimeMs() int64 {
	if x != nil {
		return x.StartTimeMs
	}
	return 0
}

func (x *RecognitionClassifierUpdate) GetEndTimeMs() int64 {
	if x != nil {
		return x.EndTimeMs
	}
	return 0
}

func (x *RecognitionClassifierUpdate) GetClassifierResult() *RecognitionClassifierResult {
	if x != nil {
		return x.ClassifierResult
	}
	return nil
}

type DescriptiveStatistics struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Minimum observed value.
	Min float64 `protobuf:"fixed64,1,opt,name=min,proto3" json:"min,omitempty"`
	// Maximum observed value.
	Max float64 `protobuf:"fixed64,2,opt,name=max,proto3" json:"max,omitempty"`
	// Estimated mean of distribution.
	Mean float64 `protobuf:"fixed64,3,opt,name=mean,proto3" json:"mean,omitempty"`
	// Estimated standard deviation of distribution.
	Std float64 `protobuf:"fixed64,4,opt,name=std,proto3" json:"std,omitempty"`
	// List of evaluated quantiles.
	Quantiles     []*DescriptiveStatistics_Quantile `protobuf:"bytes,5,rep,name=quantiles,proto3" json:"quantiles,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DescriptiveStatistics) Reset() {
	*x = DescriptiveStatistics{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[36]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DescriptiveStatistics) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DescriptiveStatistics) ProtoMessage() {}

func (x *DescriptiveStatistics) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[36]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DescriptiveStatistics.ProtoReflect.Descriptor instead.
func (*DescriptiveStatistics) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{36}
}

func (x *DescriptiveStatistics) GetMin() float64 {
	if x != nil {
		return x.Min
	}
	return 0
}

func (x *DescriptiveStatistics) GetMax() float64 {
	if x != nil {
		return x.Max
	}
	return 0
}

func (x *DescriptiveStatistics) GetMean() float64 {
	if x != nil {
		return x.Mean
	}
	return 0
}

func (x *DescriptiveStatistics) GetStd() float64 {
	if x != nil {
		return x.Std
	}
	return 0
}

func (x *DescriptiveStatistics) GetQuantiles() []*DescriptiveStatistics_Quantile {
	if x != nil {
		return x.Quantiles
	}
	return nil
}

type AudioSegmentBoundaries struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Audio segment start time.
	StartTimeMs int64 `protobuf:"varint,1,opt,name=start_time_ms,json=startTimeMs,proto3" json:"start_time_ms,omitempty"`
	// Audio segment end time.
	EndTimeMs     int64 `protobuf:"varint,2,opt,name=end_time_ms,json=endTimeMs,proto3" json:"end_time_ms,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AudioSegmentBoundaries) Reset() {
	*x = AudioSegmentBoundaries{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[37]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AudioSegmentBoundaries) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AudioSegmentBoundaries) ProtoMessage() {}

func (x *AudioSegmentBoundaries) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[37]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AudioSegmentBoundaries.ProtoReflect.Descriptor instead.
func (*AudioSegmentBoundaries) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{37}
}

func (x *AudioSegmentBoundaries) GetStartTimeMs() int64 {
	if x != nil {
		return x.StartTimeMs
	}
	return 0
}

func (x *AudioSegmentBoundaries) GetEndTimeMs() int64 {
	if x != nil {
		return x.EndTimeMs
	}
	return 0
}

type SpeakerAnalysis struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Speaker tag.
	SpeakerTag string `protobuf:"bytes,1,opt,name=speaker_tag,json=speakerTag,proto3" json:"speaker_tag,omitempty"`
	// Response window type.
	WindowType SpeakerAnalysis_WindowType `protobuf:"varint,2,opt,name=window_type,json=windowType,proto3,enum=speechkit.stt.v3.SpeakerAnalysis_WindowType" json:"window_type,omitempty"`
	// Audio segment boundaries.
	SpeechBoundaries *AudioSegmentBoundaries `protobuf:"bytes,3,opt,name=speech_boundaries,json=speechBoundaries,proto3" json:"speech_boundaries,omitempty"`
	// Total speech duration.
	TotalSpeechMs int64 `protobuf:"varint,4,opt,name=total_speech_ms,json=totalSpeechMs,proto3" json:"total_speech_ms,omitempty"`
	// Speech ratio within audio segment.
	SpeechRatio float64 `protobuf:"fixed64,5,opt,name=speech_ratio,json=speechRatio,proto3" json:"speech_ratio,omitempty"`
	// Total duration of silence.
	TotalSilenceMs int64 `protobuf:"varint,6,opt,name=total_silence_ms,json=totalSilenceMs,proto3" json:"total_silence_ms,omitempty"`
	// Silence ratio within audio segment.
	SilenceRatio float64 `protobuf:"fixed64,7,opt,name=silence_ratio,json=silenceRatio,proto3" json:"silence_ratio,omitempty"`
	// Number of words in recognized speech.
	WordsCount int64 `protobuf:"varint,8,opt,name=words_count,json=wordsCount,proto3" json:"words_count,omitempty"`
	// Number of letters in recognized speech.
	LettersCount int64 `protobuf:"varint,9,opt,name=letters_count,json=lettersCount,proto3" json:"letters_count,omitempty"`
	// Descriptive statistics for words per second distribution.
	WordsPerSecond *DescriptiveStatistics `protobuf:"bytes,10,opt,name=words_per_second,json=wordsPerSecond,proto3" json:"words_per_second,omitempty"`
	// Descriptive statistics for letters per second distribution.
	LettersPerSecond *DescriptiveStatistics `protobuf:"bytes,11,opt,name=letters_per_second,json=lettersPerSecond,proto3" json:"letters_per_second,omitempty"`
	// Descriptive statistics for words per utterance distribution.
	WordsPerUtterance *DescriptiveStatistics `protobuf:"bytes,12,opt,name=words_per_utterance,json=wordsPerUtterance,proto3" json:"words_per_utterance,omitempty"`
	// Descriptive statistics for letters per utterance distribution.
	LettersPerUtterance *DescriptiveStatistics `protobuf:"bytes,13,opt,name=letters_per_utterance,json=lettersPerUtterance,proto3" json:"letters_per_utterance,omitempty"`
	// Number of utterances
	UtteranceCount int64 `protobuf:"varint,14,opt,name=utterance_count,json=utteranceCount,proto3" json:"utterance_count,omitempty"`
	// Descriptive statistics for utterance duration distribution
	UtteranceDurationEstimation *DescriptiveStatistics `protobuf:"bytes,15,opt,name=utterance_duration_estimation,json=utteranceDurationEstimation,proto3" json:"utterance_duration_estimation,omitempty"`
	unknownFields               protoimpl.UnknownFields
	sizeCache                   protoimpl.SizeCache
}

func (x *SpeakerAnalysis) Reset() {
	*x = SpeakerAnalysis{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[38]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeakerAnalysis) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeakerAnalysis) ProtoMessage() {}

func (x *SpeakerAnalysis) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[38]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SpeakerAnalysis.ProtoReflect.Descriptor instead.
func (*SpeakerAnalysis) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{38}
}

func (x *SpeakerAnalysis) GetSpeakerTag() string {
	if x != nil {
		return x.SpeakerTag
	}
	return ""
}

func (x *SpeakerAnalysis) GetWindowType() SpeakerAnalysis_WindowType {
	if x != nil {
		return x.WindowType
	}
	return SpeakerAnalysis_WINDOW_TYPE_UNSPECIFIED
}

func (x *SpeakerAnalysis) GetSpeechBoundaries() *AudioSegmentBoundaries {
	if x != nil {
		return x.SpeechBoundaries
	}
	return nil
}

func (x *SpeakerAnalysis) GetTotalSpeechMs() int64 {
	if x != nil {
		return x.TotalSpeechMs
	}
	return 0
}

func (x *SpeakerAnalysis) GetSpeechRatio() float64 {
	if x != nil {
		return x.SpeechRatio
	}
	return 0
}

func (x *SpeakerAnalysis) GetTotalSilenceMs() int64 {
	if x != nil {
		return x.TotalSilenceMs
	}
	return 0
}

func (x *SpeakerAnalysis) GetSilenceRatio() float64 {
	if x != nil {
		return x.SilenceRatio
	}
	return 0
}

func (x *SpeakerAnalysis) GetWordsCount() int64 {
	if x != nil {
		return x.WordsCount
	}
	return 0
}

func (x *SpeakerAnalysis) GetLettersCount() int64 {
	if x != nil {
		return x.LettersCount
	}
	return 0
}

func (x *SpeakerAnalysis) GetWordsPerSecond() *DescriptiveStatistics {
	if x != nil {
		return x.WordsPerSecond
	}
	return nil
}

func (x *SpeakerAnalysis) GetLettersPerSecond() *DescriptiveStatistics {
	if x != nil {
		return x.LettersPerSecond
	}
	return nil
}

func (x *SpeakerAnalysis) GetWordsPerUtterance() *DescriptiveStatistics {
	if x != nil {
		return x.WordsPerUtterance
	}
	return nil
}

func (x *SpeakerAnalysis) GetLettersPerUtterance() *DescriptiveStatistics {
	if x != nil {
		return x.LettersPerUtterance
	}
	return nil
}

func (x *SpeakerAnalysis) GetUtteranceCount() int64 {
	if x != nil {
		return x.UtteranceCount
	}
	return 0
}

func (x *SpeakerAnalysis) GetUtteranceDurationEstimation() *DescriptiveStatistics {
	if x != nil {
		return x.UtteranceDurationEstimation
	}
	return nil
}

type ConversationAnalysis struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Audio segment boundaries.
	ConversationBoundaries *AudioSegmentBoundaries `protobuf:"bytes,1,opt,name=conversation_boundaries,json=conversationBoundaries,proto3" json:"conversation_boundaries,omitempty"`
	// Total simultaneous silence duration.
	TotalSimultaneousSilenceDurationMs int64 `protobuf:"varint,2,opt,name=total_simultaneous_silence_duration_ms,json=totalSimultaneousSilenceDurationMs,proto3" json:"total_simultaneous_silence_duration_ms,omitempty"`
	// Simultaneous silence ratio within audio segment.
	TotalSimultaneousSilenceRatio float64 `protobuf:"fixed64,3,opt,name=total_simultaneous_silence_ratio,json=totalSimultaneousSilenceRatio,proto3" json:"total_simultaneous_silence_ratio,omitempty"`
	// Descriptive statistics for simultaneous silence duration distribution.
	SimultaneousSilenceDurationEstimation *DescriptiveStatistics `protobuf:"bytes,4,opt,name=simultaneous_silence_duration_estimation,json=simultaneousSilenceDurationEstimation,proto3" json:"simultaneous_silence_duration_estimation,omitempty"`
	// Total simultaneous speech duration.
	TotalSimultaneousSpeechDurationMs int64 `protobuf:"varint,5,opt,name=total_simultaneous_speech_duration_ms,json=totalSimultaneousSpeechDurationMs,proto3" json:"total_simultaneous_speech_duration_ms,omitempty"`
	// Simultaneous speech ratio within audio segment.
	TotalSimultaneousSpeechRatio float64 `protobuf:"fixed64,6,opt,name=total_simultaneous_speech_ratio,json=totalSimultaneousSpeechRatio,proto3" json:"total_simultaneous_speech_ratio,omitempty"`
	// Descriptive statistics for simultaneous speech duration distribution.
	SimultaneousSpeechDurationEstimation *DescriptiveStatistics `protobuf:"bytes,7,opt,name=simultaneous_speech_duration_estimation,json=simultaneousSpeechDurationEstimation,proto3" json:"simultaneous_speech_duration_estimation,omitempty"`
	// Interrupts description for every speaker.
	SpeakerInterrupts []*ConversationAnalysis_InterruptsEvaluation `protobuf:"bytes,8,rep,name=speaker_interrupts,json=speakerInterrupts,proto3" json:"speaker_interrupts,omitempty"`
	// Total speech duration, including both simultaneous and separate speech.
	TotalSpeechDurationMs int64 `protobuf:"varint,9,opt,name=total_speech_duration_ms,json=totalSpeechDurationMs,proto3" json:"total_speech_duration_ms,omitempty"`
	// Total speech ratio within audio segment.
	TotalSpeechRatio float64 `protobuf:"fixed64,10,opt,name=total_speech_ratio,json=totalSpeechRatio,proto3" json:"total_speech_ratio,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ConversationAnalysis) Reset() {
	*x = ConversationAnalysis{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[39]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ConversationAnalysis) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ConversationAnalysis) ProtoMessage() {}

func (x *ConversationAnalysis) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[39]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ConversationAnalysis.ProtoReflect.Descriptor instead.
func (*ConversationAnalysis) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{39}
}

func (x *ConversationAnalysis) GetConversationBoundaries() *AudioSegmentBoundaries {
	if x != nil {
		return x.ConversationBoundaries
	}
	return nil
}

func (x *ConversationAnalysis) GetTotalSimultaneousSilenceDurationMs() int64 {
	if x != nil {
		return x.TotalSimultaneousSilenceDurationMs
	}
	return 0
}

func (x *ConversationAnalysis) GetTotalSimultaneousSilenceRatio() float64 {
	if x != nil {
		return x.TotalSimultaneousSilenceRatio
	}
	return 0
}

func (x *ConversationAnalysis) GetSimultaneousSilenceDurationEstimation() *DescriptiveStatistics {
	if x != nil {
		return x.SimultaneousSilenceDurationEstimation
	}
	return nil
}

func (x *ConversationAnalysis) GetTotalSimultaneousSpeechDurationMs() int64 {
	if x != nil {
		return x.TotalSimultaneousSpeechDurationMs
	}
	return 0
}

func (x *ConversationAnalysis) GetTotalSimultaneousSpeechRatio() float64 {
	if x != nil {
		return x.TotalSimultaneousSpeechRatio
	}
	return 0
}

func (x *ConversationAnalysis) GetSimultaneousSpeechDurationEstimation() *DescriptiveStatistics {
	if x != nil {
		return x.SimultaneousSpeechDurationEstimation
	}
	return nil
}

func (x *ConversationAnalysis) GetSpeakerInterrupts() []*ConversationAnalysis_InterruptsEvaluation {
	if x != nil {
		return x.SpeakerInterrupts
	}
	return nil
}

func (x *ConversationAnalysis) GetTotalSpeechDurationMs() int64 {
	if x != nil {
		return x.TotalSpeechDurationMs
	}
	return 0
}

func (x *ConversationAnalysis) GetTotalSpeechRatio() float64 {
	if x != nil {
		return x.TotalSpeechRatio
	}
	return 0
}

// An object representing the number of content [tokens](/docs/foundation-models/concepts/yandexgpt/tokens) used by the completion model.
type ContentUsage struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The number of tokens in the textual part of the model input.
	InputTextTokens int64 `protobuf:"varint,1,opt,name=input_text_tokens,json=inputTextTokens,proto3" json:"input_text_tokens,omitempty"`
	// The number of tokens in the generated completion.
	CompletionTokens int64 `protobuf:"varint,2,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	// The total number of tokens, including all input tokens and all generated tokens.
	TotalTokens   int64 `protobuf:"varint,3,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ContentUsage) Reset() {
	*x = ContentUsage{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[40]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ContentUsage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ContentUsage) ProtoMessage() {}

func (x *ContentUsage) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[40]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ContentUsage.ProtoReflect.Descriptor instead.
func (*ContentUsage) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{40}
}

func (x *ContentUsage) GetInputTextTokens() int64 {
	if x != nil {
		return x.InputTextTokens
	}
	return 0
}

func (x *ContentUsage) GetCompletionTokens() int64 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *ContentUsage) GetTotalTokens() int64 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

type Summarization struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// A list of summarizations of transcription.
	Results []*SummarizationPropertyResult `protobuf:"bytes,1,rep,name=results,proto3" json:"results,omitempty"`
	// A set of statistics describing the number of content tokens used by the completion model.
	ContentUsage  *ContentUsage `protobuf:"bytes,2,opt,name=content_usage,json=contentUsage,proto3" json:"content_usage,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Summarization) Reset() {
	*x = Summarization{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[41]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Summarization) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Summarization) ProtoMessage() {}

func (x *Summarization) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[41]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Summarization.ProtoReflect.Descriptor instead.
func (*Summarization) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{41}
}

func (x *Summarization) GetResults() []*SummarizationPropertyResult {
	if x != nil {
		return x.Results
	}
	return nil
}

func (x *Summarization) GetContentUsage() *ContentUsage {
	if x != nil {
		return x.ContentUsage
	}
	return nil
}

// Responses from server.
// Each response contains session UUID, AudioCursors, and specific event.
type StreamingResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Session identifier.
	SessionUuid *SessionUuid `protobuf:"bytes,1,opt,name=session_uuid,json=sessionUuid,proto3" json:"session_uuid,omitempty"`
	// Progress bar for stream session recognition: how many data we obtained; final and partial times; etc.
	AudioCursors *AudioCursors `protobuf:"bytes,2,opt,name=audio_cursors,json=audioCursors,proto3" json:"audio_cursors,omitempty"`
	// Wall clock on server side. This is time when server wrote results to stream.
	ResponseWallTimeMs int64 `protobuf:"varint,3,opt,name=response_wall_time_ms,json=responseWallTimeMs,proto3" json:"response_wall_time_ms,omitempty"`
	// Types that are valid to be assigned to Event:
	//
	//	*StreamingResponse_Partial
	//	*StreamingResponse_Final
	//	*StreamingResponse_EouUpdate
	//	*StreamingResponse_FinalRefinement
	//	*StreamingResponse_StatusCode
	//	*StreamingResponse_ClassifierUpdate
	//	*StreamingResponse_SpeakerAnalysis
	//	*StreamingResponse_ConversationAnalysis
	//	*StreamingResponse_Summarization
	Event isStreamingResponse_Event `protobuf_oneof:"Event"`
	// Tag for distinguish audio channels.
	ChannelTag    string `protobuf:"bytes,9,opt,name=channel_tag,json=channelTag,proto3" json:"channel_tag,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingResponse) Reset() {
	*x = StreamingResponse{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[42]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingResponse) ProtoMessage() {}

func (x *StreamingResponse) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[42]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingResponse.ProtoReflect.Descriptor instead.
func (*StreamingResponse) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{42}
}

func (x *StreamingResponse) GetSessionUuid() *SessionUuid {
	if x != nil {
		return x.SessionUuid
	}
	return nil
}

func (x *StreamingResponse) GetAudioCursors() *AudioCursors {
	if x != nil {
		return x.AudioCursors
	}
	return nil
}

func (x *StreamingResponse) GetResponseWallTimeMs() int64 {
	if x != nil {
		return x.ResponseWallTimeMs
	}
	return 0
}

func (x *StreamingResponse) GetEvent() isStreamingResponse_Event {
	if x != nil {
		return x.Event
	}
	return nil
}

func (x *StreamingResponse) GetPartial() *AlternativeUpdate {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_Partial); ok {
			return x.Partial
		}
	}
	return nil
}

func (x *StreamingResponse) GetFinal() *AlternativeUpdate {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_Final); ok {
			return x.Final
		}
	}
	return nil
}

func (x *StreamingResponse) GetEouUpdate() *EouUpdate {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_EouUpdate); ok {
			return x.EouUpdate
		}
	}
	return nil
}

func (x *StreamingResponse) GetFinalRefinement() *FinalRefinement {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_FinalRefinement); ok {
			return x.FinalRefinement
		}
	}
	return nil
}

func (x *StreamingResponse) GetStatusCode() *StatusCode {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_StatusCode); ok {
			return x.StatusCode
		}
	}
	return nil
}

func (x *StreamingResponse) GetClassifierUpdate() *RecognitionClassifierUpdate {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_ClassifierUpdate); ok {
			return x.ClassifierUpdate
		}
	}
	return nil
}

func (x *StreamingResponse) GetSpeakerAnalysis() *SpeakerAnalysis {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_SpeakerAnalysis); ok {
			return x.SpeakerAnalysis
		}
	}
	return nil
}

func (x *StreamingResponse) GetConversationAnalysis() *ConversationAnalysis {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_ConversationAnalysis); ok {
			return x.ConversationAnalysis
		}
	}
	return nil
}

func (x *StreamingResponse) GetSummarization() *Summarization {
	if x != nil {
		if x, ok := x.Event.(*StreamingResponse_Summarization); ok {
			return x.Summarization
		}
	}
	return nil
}

func (x *StreamingResponse) GetChannelTag() string {
	if x != nil {
		return x.ChannelTag
	}
	return ""
}

type isStreamingResponse_Event interface {
	isStreamingResponse_Event()
}

type StreamingResponse_Partial struct {
	// Partial results, server will send them regularly after enough audio data was received from user.
	// This is the current text estimation from `final_time_ms` to `partial_time_ms`. Could change after new data will arrive.
	Partial *AlternativeUpdate `protobuf:"bytes,4,opt,name=partial,proto3,oneof"`
}

type StreamingResponse_Final struct {
	// Final results, the recognition is now fixed until `final_time_ms`. For now, final is sent only if the EOU event was triggered. This behavior could be changed in future releases.
	Final *AlternativeUpdate `protobuf:"bytes,5,opt,name=final,proto3,oneof"`
}

type StreamingResponse_EouUpdate struct {
	// After EOU classifier, send the message with final, send the EouUpdate with time of EOU
	// before eou_update we send final with the same time. there could be several finals before eou update.
	EouUpdate *EouUpdate `protobuf:"bytes,6,opt,name=eou_update,json=eouUpdate,proto3,oneof"`
}

type StreamingResponse_FinalRefinement struct {
	// For each final, if normalization is enabled, sent the normalized text (or some other advanced post-processing).
	// Final normalization will introduce additional latency.
	FinalRefinement *FinalRefinement `protobuf:"bytes,7,opt,name=final_refinement,json=finalRefinement,proto3,oneof"`
}

type StreamingResponse_StatusCode struct {
	// Status messages, send by server with fixed interval (keep-alive).
	StatusCode *StatusCode `protobuf:"bytes,8,opt,name=status_code,json=statusCode,proto3,oneof"`
}

type StreamingResponse_ClassifierUpdate struct {
	// Result of the triggered classifier.
	ClassifierUpdate *RecognitionClassifierUpdate `protobuf:"bytes,10,opt,name=classifier_update,json=classifierUpdate,proto3,oneof"`
}

type StreamingResponse_SpeakerAnalysis struct {
	// Speech statistics for every speaker.
	SpeakerAnalysis *SpeakerAnalysis `protobuf:"bytes,11,opt,name=speaker_analysis,json=speakerAnalysis,proto3,oneof"`
}

type StreamingResponse_ConversationAnalysis struct {
	// Conversation statistics.
	ConversationAnalysis *ConversationAnalysis `protobuf:"bytes,12,opt,name=conversation_analysis,json=conversationAnalysis,proto3,oneof"`
}

type StreamingResponse_Summarization struct {
	// Summary.
	Summarization *Summarization `protobuf:"bytes,13,opt,name=summarization,proto3,oneof"`
}

func (*StreamingResponse_Partial) isStreamingResponse_Event() {}

func (*StreamingResponse_Final) isStreamingResponse_Event() {}

func (*StreamingResponse_EouUpdate) isStreamingResponse_Event() {}

func (*StreamingResponse_FinalRefinement) isStreamingResponse_Event() {}

func (*StreamingResponse_StatusCode) isStreamingResponse_Event() {}

func (*StreamingResponse_ClassifierUpdate) isStreamingResponse_Event() {}

func (*StreamingResponse_SpeakerAnalysis) isStreamingResponse_Event() {}

func (*StreamingResponse_ConversationAnalysis) isStreamingResponse_Event() {}

func (*StreamingResponse_Summarization) isStreamingResponse_Event() {}

type DeleteRecognitionRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	OperationId   string                 `protobuf:"bytes,1,opt,name=operation_id,json=operationId,proto3" json:"operation_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DeleteRecognitionRequest) Reset() {
	*x = DeleteRecognitionRequest{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[43]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DeleteRecognitionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DeleteRecognitionRequest) ProtoMessage() {}

func (x *DeleteRecognitionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[43]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DeleteRecognitionRequest.ProtoReflect.Descriptor instead.
func (*DeleteRecognitionRequest) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{43}
}

func (x *DeleteRecognitionRequest) GetOperationId() string {
	if x != nil {
		return x.OperationId
	}
	return ""
}

type StreamingResponseList struct {
	state              protoimpl.MessageState `protogen:"open.v1"`
	StreamingResponses []*StreamingResponse   `protobuf:"bytes,1,rep,name=streaming_responses,json=streamingResponses,proto3" json:"streaming_responses,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *StreamingResponseList) Reset() {
	*x = StreamingResponseList{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[44]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingResponseList) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingResponseList) ProtoMessage() {}

func (x *StreamingResponseList) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[44]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingResponseList.ProtoReflect.Descriptor instead.
func (*StreamingResponseList) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{44}
}

func (x *StreamingResponseList) GetStreamingResponses() []*StreamingResponse {
	if x != nil {
		return x.StreamingResponses
	}
	return nil
}

type DescriptiveStatistics_Quantile struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Quantile level in range (0, 1).
	Level float64 `protobuf:"fixed64,1,opt,name=level,proto3" json:"level,omitempty"`
	// Quantile value.
	Value         float64 `protobuf:"fixed64,2,opt,name=value,proto3" json:"value,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DescriptiveStatistics_Quantile) Reset() {
	*x = DescriptiveStatistics_Quantile{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[45]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DescriptiveStatistics_Quantile) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DescriptiveStatistics_Quantile) ProtoMessage() {}

func (x *DescriptiveStatistics_Quantile) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[45]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DescriptiveStatistics_Quantile.ProtoReflect.Descriptor instead.
func (*DescriptiveStatistics_Quantile) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{36, 0}
}

func (x *DescriptiveStatistics_Quantile) GetLevel() float64 {
	if x != nil {
		return x.Level
	}
	return 0
}

func (x *DescriptiveStatistics_Quantile) GetValue() float64 {
	if x != nil {
		return x.Value
	}
	return 0
}

type ConversationAnalysis_InterruptsEvaluation struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Speaker tag.
	SpeakerTag string `protobuf:"bytes,1,opt,name=speaker_tag,json=speakerTag,proto3" json:"speaker_tag,omitempty"`
	// Number of interrupts made by the speaker.
	InterruptsCount int64 `protobuf:"varint,2,opt,name=interrupts_count,json=interruptsCount,proto3" json:"interrupts_count,omitempty"`
	// Total duration of all interrupts.
	InterruptsDurationMs int64 `protobuf:"varint,3,opt,name=interrupts_duration_ms,json=interruptsDurationMs,proto3" json:"interrupts_duration_ms,omitempty"`
	// Boundaries for every interrupt.
	Interrupts    []*AudioSegmentBoundaries `protobuf:"bytes,4,rep,name=interrupts,proto3" json:"interrupts,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ConversationAnalysis_InterruptsEvaluation) Reset() {
	*x = ConversationAnalysis_InterruptsEvaluation{}
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[46]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ConversationAnalysis_InterruptsEvaluation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ConversationAnalysis_InterruptsEvaluation) ProtoMessage() {}

func (x *ConversationAnalysis_InterruptsEvaluation) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[46]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ConversationAnalysis_InterruptsEvaluation.ProtoReflect.Descriptor instead.
func (*ConversationAnalysis_InterruptsEvaluation) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP(), []int{39, 0}
}

func (x *ConversationAnalysis_InterruptsEvaluation) GetSpeakerTag() string {
	if x != nil {
		return x.SpeakerTag
	}
	return ""
}

func (x *ConversationAnalysis_InterruptsEvaluation) GetInterruptsCount() int64 {
	if x != nil {
		return x.InterruptsCount
	}
	return 0
}

func (x *ConversationAnalysis_InterruptsEvaluation) GetInterruptsDurationMs() int64 {
	if x != nil {
		return x.InterruptsDurationMs
	}
	return 0
}

func (x *ConversationAnalysis_InterruptsEvaluation) GetInterrupts() []*AudioSegmentBoundaries {
	if x != nil {
		return x.Interrupts
	}
	return nil
}

var File_yandex_cloud_ai_stt_v3_stt_proto protoreflect.FileDescriptor

const file_yandex_cloud_ai_stt_v3_stt_proto_rawDesc = "" +
	"\n" +
	" yandex/cloud/ai/stt/v3/stt.proto\x12\x10speechkit.stt.v3\x1a\x1cgoogle/protobuf/struct.proto\"\xab\x04\n" +
	"\x18TextNormalizationOptions\x12k\n" +
	"\x12text_normalization\x18\x01 \x01(\x0e2<.speechkit.stt.v3.TextNormalizationOptions.TextNormalizationR\x11textNormalization\x12)\n" +
	"\x10profanity_filter\x18\x02 \x01(\bR\x0fprofanityFilter\x12'\n" +
	"\x0fliterature_text\x18\x03 \x01(\bR\x0eliteratureText\x12r\n" +
	"\x15phone_formatting_mode\x18\x04 \x01(\x0e2>.speechkit.stt.v3.TextNormalizationOptions.PhoneFormattingModeR\x13phoneFormattingMode\"x\n" +
	"\x11TextNormalization\x12\"\n" +
	"\x1eTEXT_NORMALIZATION_UNSPECIFIED\x10\x00\x12\x1e\n" +
	"\x1aTEXT_NORMALIZATION_ENABLED\x10\x01\x12\x1f\n" +
	"\x1bTEXT_NORMALIZATION_DISABLED\x10\x02\"`\n" +
	"\x13PhoneFormattingMode\x12%\n" +
	"!PHONE_FORMATTING_MODE_UNSPECIFIED\x10\x00\x12\"\n" +
	"\x1ePHONE_FORMATTING_MODE_DISABLED\x10\x01\"\xf0\x01\n" +
	"\x14DefaultEouClassifier\x12I\n" +
	"\x04type\x18\x01 \x01(\x0e25.speechkit.stt.v3.DefaultEouClassifier.EouSensitivityR\x04type\x12C\n" +
	"\x1fmax_pause_between_words_hint_ms\x18\x02 \x01(\x03R\x1amaxPauseBetweenWordsHintMs\"H\n" +
	"\x0eEouSensitivity\x12\x1f\n" +
	"\x1bEOU_SENSITIVITY_UNSPECIFIED\x10\x00\x12\v\n" +
	"\aDEFAULT\x10\x01\x12\b\n" +
	"\x04HIGH\x10\x02\"\x17\n" +
	"\x15ExternalEouClassifier\"\xd9\x01\n" +
	"\x14EouClassifierOptions\x12W\n" +
	"\x12default_classifier\x18\x01 \x01(\v2&.speechkit.stt.v3.DefaultEouClassifierH\x00R\x11defaultClassifier\x12Z\n" +
	"\x13external_classifier\x18\x02 \x01(\v2'.speechkit.stt.v3.ExternalEouClassifierH\x00R\x12externalClassifierB\f\n" +
	"\n" +
	"Classifier\"\xe9\x01\n" +
	"\x15RecognitionClassifier\x12\x1e\n" +
	"\n" +
	"classifier\x18\x01 \x01(\tR\n" +
	"classifier\x12O\n" +
	"\btriggers\x18\x02 \x03(\x0e23.speechkit.stt.v3.RecognitionClassifier.TriggerTypeR\btriggers\"_\n" +
	"\vTriggerType\x12 \n" +
	"\x18TRIGGER_TYPE_UNSPECIFIED\x10\x00\x1a\x02\b\x01\x12\x10\n" +
	"\fON_UTTERANCE\x10\x01\x12\f\n" +
	"\bON_FINAL\x10\x02\x12\x0e\n" +
	"\n" +
	"ON_PARTIAL\x10\x03\"i\n" +
	"\x1cRecognitionClassifierOptions\x12I\n" +
	"\vclassifiers\x18\x01 \x03(\v2'.speechkit.stt.v3.RecognitionClassifierR\vclassifiers\"\xdb\x01\n" +
	"\x15SpeechAnalysisOptions\x126\n" +
	"\x17enable_speaker_analysis\x18\x01 \x01(\bR\x15enableSpeakerAnalysis\x12@\n" +
	"\x1cenable_conversation_analysis\x18\x02 \x01(\bR\x1aenableConversationAnalysis\x12H\n" +
	" descriptive_statistics_quantiles\x18\x03 \x03(\x01R\x1edescriptiveStatisticsQuantiles\"\xfa\x01\n" +
	"\bRawAudio\x12O\n" +
	"\x0eaudio_encoding\x18\x01 \x01(\x0e2(.speechkit.stt.v3.RawAudio.AudioEncodingR\raudioEncoding\x12*\n" +
	"\x11sample_rate_hertz\x18\x02 \x01(\x03R\x0fsampleRateHertz\x12.\n" +
	"\x13audio_channel_count\x18\x03 \x01(\x03R\x11audioChannelCount\"A\n" +
	"\rAudioEncoding\x12\x1e\n" +
	"\x1aAUDIO_ENCODING_UNSPECIFIED\x10\x00\x12\x10\n" +
	"\fLINEAR16_PCM\x10\x01\"\xd3\x01\n" +
	"\x0eContainerAudio\x12e\n" +
	"\x14container_audio_type\x18\x01 \x01(\x0e23.speechkit.stt.v3.ContainerAudio.ContainerAudioTypeR\x12containerAudioType\"Z\n" +
	"\x12ContainerAudioType\x12$\n" +
	" CONTAINER_AUDIO_TYPE_UNSPECIFIED\x10\x00\x12\a\n" +
	"\x03WAV\x10\x01\x12\f\n" +
	"\bOGG_OPUS\x10\x02\x12\a\n" +
	"\x03MP3\x10\x03\"\xab\x01\n" +
	"\x12AudioFormatOptions\x129\n" +
	"\traw_audio\x18\x01 \x01(\v2\x1a.speechkit.stt.v3.RawAudioH\x00R\brawAudio\x12K\n" +
	"\x0fcontainer_audio\x18\x02 \x01(\v2 .speechkit.stt.v3.ContainerAudioH\x00R\x0econtainerAudioB\r\n" +
	"\vAudioFormat\"\x96\x02\n" +
	"\x1aLanguageRestrictionOptions\x12o\n" +
	"\x10restriction_type\x18\x01 \x01(\x0e2D.speechkit.stt.v3.LanguageRestrictionOptions.LanguageRestrictionTypeR\x0frestrictionType\x12#\n" +
	"\rlanguage_code\x18\x02 \x03(\tR\flanguageCode\"b\n" +
	"\x17LanguageRestrictionType\x12)\n" +
	"%LANGUAGE_RESTRICTION_TYPE_UNSPECIFIED\x10\x00\x12\r\n" +
	"\tWHITELIST\x10\x01\x12\r\n" +
	"\tBLACKLIST\x10\x02\"=\n" +
	"\n" +
	"JsonSchema\x12/\n" +
	"\x06schema\x18\x01 \x01(\v2\x17.google.protobuf.StructR\x06schema\"\xaf\x01\n" +
	"\x15SummarizationProperty\x12 \n" +
	"\vinstruction\x18\x01 \x01(\tR\vinstruction\x12!\n" +
	"\vjson_object\x18\x02 \x01(\bH\x00R\n" +
	"jsonObject\x12?\n" +
	"\vjson_schema\x18\x03 \x01(\v2\x1c.speechkit.stt.v3.JsonSchemaH\x00R\n" +
	"jsonSchemaB\x10\n" +
	"\x0eResponseFormat\"|\n" +
	"\x14SummarizationOptions\x12\x1b\n" +
	"\tmodel_uri\x18\x01 \x01(\tR\bmodelUri\x12G\n" +
	"\n" +
	"properties\x18\x02 \x03(\v2'.speechkit.stt.v3.SummarizationPropertyR\n" +
	"properties\"9\n" +
	"\x1bSummarizationPropertyResult\x12\x1a\n" +
	"\bresponse\x18\x01 \x01(\tR\bresponse\"\x83\x04\n" +
	"\x17RecognitionModelOptions\x12\x14\n" +
	"\x05model\x18\x01 \x01(\tR\x05model\x12G\n" +
	"\faudio_format\x18\x02 \x01(\v2$.speechkit.stt.v3.AudioFormatOptionsR\vaudioFormat\x12Y\n" +
	"\x12text_normalization\x18\x03 \x01(\v2*.speechkit.stt.v3.TextNormalizationOptionsR\x11textNormalization\x12_\n" +
	"\x14language_restriction\x18\x04 \x01(\v2,.speechkit.stt.v3.LanguageRestrictionOptionsR\x13languageRestriction\x12q\n" +
	"\x15audio_processing_type\x18\x05 \x01(\x0e2=.speechkit.stt.v3.RecognitionModelOptions.AudioProcessingTypeR\x13audioProcessingType\"Z\n" +
	"\x13AudioProcessingType\x12%\n" +
	"!AUDIO_PROCESSING_TYPE_UNSPECIFIED\x10\x00\x12\r\n" +
	"\tREAL_TIME\x10\x01\x12\r\n" +
	"\tFULL_DATA\x10\x02\"\xef\x01\n" +
	"\x16SpeakerLabelingOptions\x12c\n" +
	"\x10speaker_labeling\x18\x01 \x01(\x0e28.speechkit.stt.v3.SpeakerLabelingOptions.SpeakerLabelingR\x0fspeakerLabeling\"p\n" +
	"\x0fSpeakerLabeling\x12 \n" +
	"\x1cSPEAKER_LABELING_UNSPECIFIED\x10\x00\x12\x1c\n" +
	"\x18SPEAKER_LABELING_ENABLED\x10\x01\x12\x1d\n" +
	"\x19SPEAKER_LABELING_DISABLED\x10\x02\"\x95\x04\n" +
	"\x10StreamingOptions\x12V\n" +
	"\x11recognition_model\x18\x01 \x01(\v2).speechkit.stt.v3.RecognitionModelOptionsR\x10recognitionModel\x12M\n" +
	"\x0eeou_classifier\x18\x02 \x01(\v2&.speechkit.stt.v3.EouClassifierOptionsR\reouClassifier\x12e\n" +
	"\x16recognition_classifier\x18\x03 \x01(\v2..speechkit.stt.v3.RecognitionClassifierOptionsR\x15recognitionClassifier\x12P\n" +
	"\x0fspeech_analysis\x18\x04 \x01(\v2'.speechkit.stt.v3.SpeechAnalysisOptionsR\x0espeechAnalysis\x12S\n" +
	"\x10speaker_labeling\x18\x05 \x01(\v2(.speechkit.stt.v3.SpeakerLabelingOptionsR\x0fspeakerLabeling\x12L\n" +
	"\rsummarization\x18\x06 \x01(\v2&.speechkit.stt.v3.SummarizationOptionsR\rsummarization\" \n" +
	"\n" +
	"AudioChunk\x12\x12\n" +
	"\x04data\x18\x01 \x01(\fR\x04data\"/\n" +
	"\fSilenceChunk\x12\x1f\n" +
	"\vduration_ms\x18\x01 \x01(\x03R\n" +
	"durationMs\"\x05\n" +
	"\x03Eou\"\x92\x02\n" +
	"\x10StreamingRequest\x12M\n" +
	"\x0fsession_options\x18\x01 \x01(\v2\".speechkit.stt.v3.StreamingOptionsH\x00R\x0esessionOptions\x124\n" +
	"\x05chunk\x18\x02 \x01(\v2\x1c.speechkit.stt.v3.AudioChunkH\x00R\x05chunk\x12E\n" +
	"\rsilence_chunk\x18\x03 \x01(\v2\x1e.speechkit.stt.v3.SilenceChunkH\x00R\fsilenceChunk\x12)\n" +
	"\x03eou\x18\x04 \x01(\v2\x15.speechkit.stt.v3.EouH\x00R\x03eouB\a\n" +
	"\x05Event\"\x89\x04\n" +
	"\x14RecognizeFileRequest\x12\x1a\n" +
	"\acontent\x18\x01 \x01(\fH\x00R\acontent\x12\x12\n" +
	"\x03uri\x18\x02 \x01(\tH\x00R\x03uri\x12V\n" +
	"\x11recognition_model\x18\x03 \x01(\v2).speechkit.stt.v3.RecognitionModelOptionsR\x10recognitionModel\x12e\n" +
	"\x16recognition_classifier\x18\x04 \x01(\v2..speechkit.stt.v3.RecognitionClassifierOptionsR\x15recognitionClassifier\x12P\n" +
	"\x0fspeech_analysis\x18\x05 \x01(\v2'.speechkit.stt.v3.SpeechAnalysisOptionsR\x0espeechAnalysis\x12S\n" +
	"\x10speaker_labeling\x18\x06 \x01(\v2(.speechkit.stt.v3.SpeakerLabelingOptionsR\x0fspeakerLabeling\x12L\n" +
	"\rsummarization\x18\a \x01(\v2&.speechkit.stt.v3.SummarizationOptionsR\rsummarizationB\r\n" +
	"\vAudioSource\"^\n" +
	"\x04Word\x12\x12\n" +
	"\x04text\x18\x01 \x01(\tR\x04text\x12\"\n" +
	"\rstart_time_ms\x18\x02 \x01(\x03R\vstartTimeMs\x12\x1e\n" +
	"\vend_time_ms\x18\x03 \x01(\x03R\tendTimeMs\"[\n" +
	"\x12LanguageEstimation\x12#\n" +
	"\rlanguage_code\x18\x01 \x01(\tR\flanguageCode\x12 \n" +
	"\vprobability\x18\x02 \x01(\x01R\vprobability\"\xf7\x01\n" +
	"\vAlternative\x12,\n" +
	"\x05words\x18\x01 \x03(\v2\x16.speechkit.stt.v3.WordR\x05words\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\x12\"\n" +
	"\rstart_time_ms\x18\x03 \x01(\x03R\vstartTimeMs\x12\x1e\n" +
	"\vend_time_ms\x18\x04 \x01(\x03R\tendTimeMs\x12\x1e\n" +
	"\n" +
	"confidence\x18\x05 \x01(\x01R\n" +
	"confidence\x12B\n" +
	"\tlanguages\x18\x06 \x03(\v2$.speechkit.stt.v3.LanguageEstimationR\tlanguages\"$\n" +
	"\tEouUpdate\x12\x17\n" +
	"\atime_ms\x18\x02 \x01(\x03R\x06timeMs\"{\n" +
	"\x11AlternativeUpdate\x12A\n" +
	"\falternatives\x18\x01 \x03(\v2\x1d.speechkit.stt.v3.AlternativeR\falternatives\x12#\n" +
	"\vchannel_tag\x18\x02 \x01(\tB\x02\x18\x01R\n" +
	"channelTag\"\xe9\x01\n" +
	"\fAudioCursors\x12(\n" +
	"\x10received_data_ms\x18\x01 \x01(\x03R\x0ereceivedDataMs\x12\"\n" +
	"\rreset_time_ms\x18\x02 \x01(\x03R\vresetTimeMs\x12&\n" +
	"\x0fpartial_time_ms\x18\x03 \x01(\x03R\rpartialTimeMs\x12\"\n" +
	"\rfinal_time_ms\x18\x04 \x01(\x03R\vfinalTimeMs\x12\x1f\n" +
	"\vfinal_index\x18\x05 \x01(\x03R\n" +
	"finalIndex\x12\x1e\n" +
	"\veou_time_ms\x18\x06 \x01(\x03R\teouTimeMs\"\x8a\x01\n" +
	"\x0fFinalRefinement\x12\x1f\n" +
	"\vfinal_index\x18\x01 \x01(\x03R\n" +
	"finalIndex\x12N\n" +
	"\x0fnormalized_text\x18\x02 \x01(\v2#.speechkit.stt.v3.AlternativeUpdateH\x00R\x0enormalizedTextB\x06\n" +
	"\x04Type\"_\n" +
	"\n" +
	"StatusCode\x127\n" +
	"\tcode_type\x18\x01 \x01(\x0e2\x1a.speechkit.stt.v3.CodeTypeR\bcodeType\x12\x18\n" +
	"\amessage\x18\x02 \x01(\tR\amessage\"I\n" +
	"\vSessionUuid\x12\x12\n" +
	"\x04uuid\x18\x01 \x01(\tR\x04uuid\x12&\n" +
	"\x0fuser_request_id\x18\x02 \x01(\tR\ruserRequestId\"i\n" +
	"\x0fPhraseHighlight\x12\x12\n" +
	"\x04text\x18\x01 \x01(\tR\x04text\x12\"\n" +
	"\rstart_time_ms\x18\x02 \x01(\x03R\vstartTimeMs\x12\x1e\n" +
	"\vend_time_ms\x18\x03 \x01(\x03R\tendTimeMs\"R\n" +
	"\x1aRecognitionClassifierLabel\x12\x14\n" +
	"\x05label\x18\x01 \x01(\tR\x05label\x12\x1e\n" +
	"\n" +
	"confidence\x18\x02 \x01(\x01R\n" +
	"confidence\"\xc6\x01\n" +
	"\x1bRecognitionClassifierResult\x12\x1e\n" +
	"\n" +
	"classifier\x18\x01 \x01(\tR\n" +
	"classifier\x12A\n" +
	"\n" +
	"highlights\x18\x02 \x03(\v2!.speechkit.stt.v3.PhraseHighlightR\n" +
	"highlights\x12D\n" +
	"\x06labels\x18\x03 \x03(\v2,.speechkit.stt.v3.RecognitionClassifierLabelR\x06labels\"\xfd\x02\n" +
	"\x1bRecognitionClassifierUpdate\x12Y\n" +
	"\vwindow_type\x18\x01 \x01(\x0e28.speechkit.stt.v3.RecognitionClassifierUpdate.WindowTypeR\n" +
	"windowType\x12\"\n" +
	"\rstart_time_ms\x18\x02 \x01(\x03R\vstartTimeMs\x12\x1e\n" +
	"\vend_time_ms\x18\x03 \x01(\x03R\tendTimeMs\x12Z\n" +
	"\x11classifier_result\x18\x04 \x01(\v2-.speechkit.stt.v3.RecognitionClassifierResultR\x10classifierResult\"c\n" +
	"\n" +
	"WindowType\x12\x1f\n" +
	"\x17WINDOW_TYPE_UNSPECIFIED\x10\x00\x1a\x02\b\x01\x12\x12\n" +
	"\x0eLAST_UTTERANCE\x10\x01\x12\x0e\n" +
	"\n" +
	"LAST_FINAL\x10\x02\x12\x10\n" +
	"\fLAST_PARTIAL\x10\x03\"\xe9\x01\n" +
	"\x15DescriptiveStatistics\x12\x10\n" +
	"\x03min\x18\x01 \x01(\x01R\x03min\x12\x10\n" +
	"\x03max\x18\x02 \x01(\x01R\x03max\x12\x12\n" +
	"\x04mean\x18\x03 \x01(\x01R\x04mean\x12\x10\n" +
	"\x03std\x18\x04 \x01(\x01R\x03std\x12N\n" +
	"\tquantiles\x18\x05 \x03(\v20.speechkit.stt.v3.DescriptiveStatistics.QuantileR\tquantiles\x1a6\n" +
	"\bQuantile\x12\x14\n" +
	"\x05level\x18\x01 \x01(\x01R\x05level\x12\x14\n" +
	"\x05value\x18\x02 \x01(\x01R\x05value\"\\\n" +
	"\x16AudioSegmentBoundaries\x12\"\n" +
	"\rstart_time_ms\x18\x01 \x01(\x03R\vstartTimeMs\x12\x1e\n" +
	"\vend_time_ms\x18\x02 \x01(\x03R\tendTimeMs\"\xfc\a\n" +
	"\x0fSpeakerAnalysis\x12\x1f\n" +
	"\vspeaker_tag\x18\x01 \x01(\tR\n" +
	"speakerTag\x12M\n" +
	"\vwindow_type\x18\x02 \x01(\x0e2,.speechkit.stt.v3.SpeakerAnalysis.WindowTypeR\n" +
	"windowType\x12U\n" +
	"\x11speech_boundaries\x18\x03 \x01(\v2(.speechkit.stt.v3.AudioSegmentBoundariesR\x10speechBoundaries\x12&\n" +
	"\x0ftotal_speech_ms\x18\x04 \x01(\x03R\rtotalSpeechMs\x12!\n" +
	"\fspeech_ratio\x18\x05 \x01(\x01R\vspeechRatio\x12(\n" +
	"\x10total_silence_ms\x18\x06 \x01(\x03R\x0etotalSilenceMs\x12#\n" +
	"\rsilence_ratio\x18\a \x01(\x01R\fsilenceRatio\x12\x1f\n" +
	"\vwords_count\x18\b \x01(\x03R\n" +
	"wordsCount\x12#\n" +
	"\rletters_count\x18\t \x01(\x03R\flettersCount\x12Q\n" +
	"\x10words_per_second\x18\n" +
	" \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR\x0ewordsPerSecond\x12U\n" +
	"\x12letters_per_second\x18\v \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR\x10lettersPerSecond\x12W\n" +
	"\x13words_per_utterance\x18\f \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR\x11wordsPerUtterance\x12[\n" +
	"\x15letters_per_utterance\x18\r \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR\x13lettersPerUtterance\x12'\n" +
	"\x0futterance_count\x18\x0e \x01(\x03R\x0eutteranceCount\x12k\n" +
	"\x1dutterance_duration_estimation\x18\x0f \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR\x1butteranceDurationEstimation\"L\n" +
	"\n" +
	"WindowType\x12\x1f\n" +
	"\x17WINDOW_TYPE_UNSPECIFIED\x10\x00\x1a\x02\b\x01\x12\t\n" +
	"\x05TOTAL\x10\x01\x12\x12\n" +
	"\x0eLAST_UTTERANCE\x10\x02\"\xea\b\n" +
	"\x14ConversationAnalysis\x12a\n" +
	"\x17conversation_boundaries\x18\x01 \x01(\v2(.speechkit.stt.v3.AudioSegmentBoundariesR\x16conversationBoundaries\x12R\n" +
	"&total_simultaneous_silence_duration_ms\x18\x02 \x01(\x03R\"totalSimultaneousSilenceDurationMs\x12G\n" +
	" total_simultaneous_silence_ratio\x18\x03 \x01(\x01R\x1dtotalSimultaneousSilenceRatio\x12\x80\x01\n" +
	"(simultaneous_silence_duration_estimation\x18\x04 \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR%simultaneousSilenceDurationEstimation\x12P\n" +
	"%total_simultaneous_speech_duration_ms\x18\x05 \x01(\x03R!totalSimultaneousSpeechDurationMs\x12E\n" +
	"\x1ftotal_simultaneous_speech_ratio\x18\x06 \x01(\x01R\x1ctotalSimultaneousSpeechRatio\x12~\n" +
	"'simultaneous_speech_duration_estimation\x18\a \x01(\v2'.speechkit.stt.v3.DescriptiveStatisticsR$simultaneousSpeechDurationEstimation\x12j\n" +
	"\x12speaker_interrupts\x18\b \x03(\v2;.speechkit.stt.v3.ConversationAnalysis.InterruptsEvaluationR\x11speakerInterrupts\x127\n" +
	"\x18total_speech_duration_ms\x18\t \x01(\x03R\x15totalSpeechDurationMs\x12,\n" +
	"\x12total_speech_ratio\x18\n" +
	" \x01(\x01R\x10totalSpeechRatio\x1a\xe2\x01\n" +
	"\x14InterruptsEvaluation\x12\x1f\n" +
	"\vspeaker_tag\x18\x01 \x01(\tR\n" +
	"speakerTag\x12)\n" +
	"\x10interrupts_count\x18\x02 \x01(\x03R\x0finterruptsCount\x124\n" +
	"\x16interrupts_duration_ms\x18\x03 \x01(\x03R\x14interruptsDurationMs\x12H\n" +
	"\n" +
	"interrupts\x18\x04 \x03(\v2(.speechkit.stt.v3.AudioSegmentBoundariesR\n" +
	"interrupts\"\x8a\x01\n" +
	"\fContentUsage\x12*\n" +
	"\x11input_text_tokens\x18\x01 \x01(\x03R\x0finputTextTokens\x12+\n" +
	"\x11completion_tokens\x18\x02 \x01(\x03R\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\x03 \x01(\x03R\vtotalTokens\"\x9d\x01\n" +
	"\rSummarization\x12G\n" +
	"\aresults\x18\x01 \x03(\v2-.speechkit.stt.v3.SummarizationPropertyResultR\aresults\x12C\n" +
	"\rcontent_usage\x18\x02 \x01(\v2\x1e.speechkit.stt.v3.ContentUsageR\fcontentUsage\"\x9a\a\n" +
	"\x11StreamingResponse\x12@\n" +
	"\fsession_uuid\x18\x01 \x01(\v2\x1d.speechkit.stt.v3.SessionUuidR\vsessionUuid\x12C\n" +
	"\raudio_cursors\x18\x02 \x01(\v2\x1e.speechkit.stt.v3.AudioCursorsR\faudioCursors\x121\n" +
	"\x15response_wall_time_ms\x18\x03 \x01(\x03R\x12responseWallTimeMs\x12?\n" +
	"\apartial\x18\x04 \x01(\v2#.speechkit.stt.v3.AlternativeUpdateH\x00R\apartial\x12;\n" +
	"\x05final\x18\x05 \x01(\v2#.speechkit.stt.v3.AlternativeUpdateH\x00R\x05final\x12<\n" +
	"\n" +
	"eou_update\x18\x06 \x01(\v2\x1b.speechkit.stt.v3.EouUpdateH\x00R\teouUpdate\x12N\n" +
	"\x10final_refinement\x18\a \x01(\v2!.speechkit.stt.v3.FinalRefinementH\x00R\x0ffinalRefinement\x12?\n" +
	"\vstatus_code\x18\b \x01(\v2\x1c.speechkit.stt.v3.StatusCodeH\x00R\n" +
	"statusCode\x12\\\n" +
	"\x11classifier_update\x18\n" +
	" \x01(\v2-.speechkit.stt.v3.RecognitionClassifierUpdateH\x00R\x10classifierUpdate\x12N\n" +
	"\x10speaker_analysis\x18\v \x01(\v2!.speechkit.stt.v3.SpeakerAnalysisH\x00R\x0fspeakerAnalysis\x12]\n" +
	"\x15conversation_analysis\x18\f \x01(\v2&.speechkit.stt.v3.ConversationAnalysisH\x00R\x14conversationAnalysis\x12G\n" +
	"\rsummarization\x18\r \x01(\v2\x1f.speechkit.stt.v3.SummarizationH\x00R\rsummarization\x12\x1f\n" +
	"\vchannel_tag\x18\t \x01(\tR\n" +
	"channelTagB\a\n" +
	"\x05Event\"=\n" +
	"\x18DeleteRecognitionRequest\x12!\n" +
	"\foperation_id\x18\x01 \x01(\tR\voperationId\"m\n" +
	"\x15StreamingResponseList\x12T\n" +
	"\x13streaming_responses\x18\x01 \x03(\v2#.speechkit.stt.v3.StreamingResponseR\x12streamingResponses*O\n" +
	"\bCodeType\x12\x1d\n" +
	"\x15CODE_TYPE_UNSPECIFIED\x10\x00\x1a\x02\b\x01\x12\v\n" +
	"\aWORKING\x10\x01\x12\v\n" +
	"\aWARNING\x10\x02\x12\n" +
	"\n" +
	"\x06CLOSED\x10\x03B\\\n" +
	"\x1ayandex.cloud.api.ai.stt.v3Z>github.com/yandex-cloud/go-genproto/yandex/cloud/ai/stt/v3;sttb\x06proto3"

var (
	file_yandex_cloud_ai_stt_v3_stt_proto_rawDescOnce sync.Once
	file_yandex_cloud_ai_stt_v3_stt_proto_rawDescData []byte
)

func file_yandex_cloud_ai_stt_v3_stt_proto_rawDescGZIP() []byte {
	file_yandex_cloud_ai_stt_v3_stt_proto_rawDescOnce.Do(func() {
		file_yandex_cloud_ai_stt_v3_stt_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_stt_v3_stt_proto_rawDesc), len(file_yandex_cloud_ai_stt_v3_stt_proto_rawDesc)))
	})
	return file_yandex_cloud_ai_stt_v3_stt_proto_rawDescData
}

var file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes = make([]protoimpl.EnumInfo, 12)
var file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes = make([]protoimpl.MessageInfo, 47)
var file_yandex_cloud_ai_stt_v3_stt_proto_goTypes = []any{
	(CodeType)(0), // 0: speechkit.stt.v3.CodeType
	(TextNormalizationOptions_TextNormalization)(0),         // 1: speechkit.stt.v3.TextNormalizationOptions.TextNormalization
	(TextNormalizationOptions_PhoneFormattingMode)(0),       // 2: speechkit.stt.v3.TextNormalizationOptions.PhoneFormattingMode
	(DefaultEouClassifier_EouSensitivity)(0),                // 3: speechkit.stt.v3.DefaultEouClassifier.EouSensitivity
	(RecognitionClassifier_TriggerType)(0),                  // 4: speechkit.stt.v3.RecognitionClassifier.TriggerType
	(RawAudio_AudioEncoding)(0),                             // 5: speechkit.stt.v3.RawAudio.AudioEncoding
	(ContainerAudio_ContainerAudioType)(0),                  // 6: speechkit.stt.v3.ContainerAudio.ContainerAudioType
	(LanguageRestrictionOptions_LanguageRestrictionType)(0), // 7: speechkit.stt.v3.LanguageRestrictionOptions.LanguageRestrictionType
	(RecognitionModelOptions_AudioProcessingType)(0),        // 8: speechkit.stt.v3.RecognitionModelOptions.AudioProcessingType
	(SpeakerLabelingOptions_SpeakerLabeling)(0),             // 9: speechkit.stt.v3.SpeakerLabelingOptions.SpeakerLabeling
	(RecognitionClassifierUpdate_WindowType)(0),             // 10: speechkit.stt.v3.RecognitionClassifierUpdate.WindowType
	(SpeakerAnalysis_WindowType)(0),                         // 11: speechkit.stt.v3.SpeakerAnalysis.WindowType
	(*TextNormalizationOptions)(nil),                        // 12: speechkit.stt.v3.TextNormalizationOptions
	(*DefaultEouClassifier)(nil),                            // 13: speechkit.stt.v3.DefaultEouClassifier
	(*ExternalEouClassifier)(nil),                           // 14: speechkit.stt.v3.ExternalEouClassifier
	(*EouClassifierOptions)(nil),                            // 15: speechkit.stt.v3.EouClassifierOptions
	(*RecognitionClassifier)(nil),                           // 16: speechkit.stt.v3.RecognitionClassifier
	(*RecognitionClassifierOptions)(nil),                    // 17: speechkit.stt.v3.RecognitionClassifierOptions
	(*SpeechAnalysisOptions)(nil),                           // 18: speechkit.stt.v3.SpeechAnalysisOptions
	(*RawAudio)(nil),                                        // 19: speechkit.stt.v3.RawAudio
	(*ContainerAudio)(nil),                                  // 20: speechkit.stt.v3.ContainerAudio
	(*AudioFormatOptions)(nil),                              // 21: speechkit.stt.v3.AudioFormatOptions
	(*LanguageRestrictionOptions)(nil),                      // 22: speechkit.stt.v3.LanguageRestrictionOptions
	(*JsonSchema)(nil),                                      // 23: speechkit.stt.v3.JsonSchema
	(*SummarizationProperty)(nil),                           // 24: speechkit.stt.v3.SummarizationProperty
	(*SummarizationOptions)(nil),                            // 25: speechkit.stt.v3.SummarizationOptions
	(*SummarizationPropertyResult)(nil),                     // 26: speechkit.stt.v3.SummarizationPropertyResult
	(*RecognitionModelOptions)(nil),                         // 27: speechkit.stt.v3.RecognitionModelOptions
	(*SpeakerLabelingOptions)(nil),                          // 28: speechkit.stt.v3.SpeakerLabelingOptions
	(*StreamingOptions)(nil),                                // 29: speechkit.stt.v3.StreamingOptions
	(*AudioChunk)(nil),                                      // 30: speechkit.stt.v3.AudioChunk
	(*SilenceChunk)(nil),                                    // 31: speechkit.stt.v3.SilenceChunk
	(*Eou)(nil),                                             // 32: speechkit.stt.v3.Eou
	(*StreamingRequest)(nil),                                // 33: speechkit.stt.v3.StreamingRequest
	(*RecognizeFileRequest)(nil),                            // 34: speechkit.stt.v3.RecognizeFileRequest
	(*Word)(nil),                                            // 35: speechkit.stt.v3.Word
	(*LanguageEstimation)(nil),                              // 36: speechkit.stt.v3.LanguageEstimation
	(*Alternative)(nil),                                     // 37: speechkit.stt.v3.Alternative
	(*EouUpdate)(nil),                                       // 38: speechkit.stt.v3.EouUpdate
	(*AlternativeUpdate)(nil),                               // 39: speechkit.stt.v3.AlternativeUpdate
	(*AudioCursors)(nil),                                    // 40: speechkit.stt.v3.AudioCursors
	(*FinalRefinement)(nil),                                 // 41: speechkit.stt.v3.FinalRefinement
	(*StatusCode)(nil),                                      // 42: speechkit.stt.v3.StatusCode
	(*SessionUuid)(nil),                                     // 43: speechkit.stt.v3.SessionUuid
	(*PhraseHighlight)(nil),                                 // 44: speechkit.stt.v3.PhraseHighlight
	(*RecognitionClassifierLabel)(nil),                      // 45: speechkit.stt.v3.RecognitionClassifierLabel
	(*RecognitionClassifierResult)(nil),                     // 46: speechkit.stt.v3.RecognitionClassifierResult
	(*RecognitionClassifierUpdate)(nil),                     // 47: speechkit.stt.v3.RecognitionClassifierUpdate
	(*DescriptiveStatistics)(nil),                           // 48: speechkit.stt.v3.DescriptiveStatistics
	(*AudioSegmentBoundaries)(nil),                          // 49: speechkit.stt.v3.AudioSegmentBoundaries
	(*SpeakerAnalysis)(nil),                                 // 50: speechkit.stt.v3.SpeakerAnalysis
	(*ConversationAnalysis)(nil),                            // 51: speechkit.stt.v3.ConversationAnalysis
	(*ContentUsage)(nil),                                    // 52: speechkit.stt.v3.ContentUsage
	(*Summarization)(nil),                                   // 53: speechkit.stt.v3.Summarization
	(*StreamingResponse)(nil),                               // 54: speechkit.stt.v3.StreamingResponse
	(*DeleteRecognitionRequest)(nil),                        // 55: speechkit.stt.v3.DeleteRecognitionRequest
	(*StreamingResponseList)(nil),                           // 56: speechkit.stt.v3.StreamingResponseList
	(*DescriptiveStatistics_Quantile)(nil),                  // 57: speechkit.stt.v3.DescriptiveStatistics.Quantile
	(*ConversationAnalysis_InterruptsEvaluation)(nil),       // 58: speechkit.stt.v3.ConversationAnalysis.InterruptsEvaluation
	(*structpb.Struct)(nil),                                 // 59: google.protobuf.Struct
}
var file_yandex_cloud_ai_stt_v3_stt_proto_depIdxs = []int32{
	1,  // 0: speechkit.stt.v3.TextNormalizationOptions.text_normalization:type_name -> speechkit.stt.v3.TextNormalizationOptions.TextNormalization
	2,  // 1: speechkit.stt.v3.TextNormalizationOptions.phone_formatting_mode:type_name -> speechkit.stt.v3.TextNormalizationOptions.PhoneFormattingMode
	3,  // 2: speechkit.stt.v3.DefaultEouClassifier.type:type_name -> speechkit.stt.v3.DefaultEouClassifier.EouSensitivity
	13, // 3: speechkit.stt.v3.EouClassifierOptions.default_classifier:type_name -> speechkit.stt.v3.DefaultEouClassifier
	14, // 4: speechkit.stt.v3.EouClassifierOptions.external_classifier:type_name -> speechkit.stt.v3.ExternalEouClassifier
	4,  // 5: speechkit.stt.v3.RecognitionClassifier.triggers:type_name -> speechkit.stt.v3.RecognitionClassifier.TriggerType
	16, // 6: speechkit.stt.v3.RecognitionClassifierOptions.classifiers:type_name -> speechkit.stt.v3.RecognitionClassifier
	5,  // 7: speechkit.stt.v3.RawAudio.audio_encoding:type_name -> speechkit.stt.v3.RawAudio.AudioEncoding
	6,  // 8: speechkit.stt.v3.ContainerAudio.container_audio_type:type_name -> speechkit.stt.v3.ContainerAudio.ContainerAudioType
	19, // 9: speechkit.stt.v3.AudioFormatOptions.raw_audio:type_name -> speechkit.stt.v3.RawAudio
	20, // 10: speechkit.stt.v3.AudioFormatOptions.container_audio:type_name -> speechkit.stt.v3.ContainerAudio
	7,  // 11: speechkit.stt.v3.LanguageRestrictionOptions.restriction_type:type_name -> speechkit.stt.v3.LanguageRestrictionOptions.LanguageRestrictionType
	59, // 12: speechkit.stt.v3.JsonSchema.schema:type_name -> google.protobuf.Struct
	23, // 13: speechkit.stt.v3.SummarizationProperty.json_schema:type_name -> speechkit.stt.v3.JsonSchema
	24, // 14: speechkit.stt.v3.SummarizationOptions.properties:type_name -> speechkit.stt.v3.SummarizationProperty
	21, // 15: speechkit.stt.v3.RecognitionModelOptions.audio_format:type_name -> speechkit.stt.v3.AudioFormatOptions
	12, // 16: speechkit.stt.v3.RecognitionModelOptions.text_normalization:type_name -> speechkit.stt.v3.TextNormalizationOptions
	22, // 17: speechkit.stt.v3.RecognitionModelOptions.language_restriction:type_name -> speechkit.stt.v3.LanguageRestrictionOptions
	8,  // 18: speechkit.stt.v3.RecognitionModelOptions.audio_processing_type:type_name -> speechkit.stt.v3.RecognitionModelOptions.AudioProcessingType
	9,  // 19: speechkit.stt.v3.SpeakerLabelingOptions.speaker_labeling:type_name -> speechkit.stt.v3.SpeakerLabelingOptions.SpeakerLabeling
	27, // 20: speechkit.stt.v3.StreamingOptions.recognition_model:type_name -> speechkit.stt.v3.RecognitionModelOptions
	15, // 21: speechkit.stt.v3.StreamingOptions.eou_classifier:type_name -> speechkit.stt.v3.EouClassifierOptions
	17, // 22: speechkit.stt.v3.StreamingOptions.recognition_classifier:type_name -> speechkit.stt.v3.RecognitionClassifierOptions
	18, // 23: speechkit.stt.v3.StreamingOptions.speech_analysis:type_name -> speechkit.stt.v3.SpeechAnalysisOptions
	28, // 24: speechkit.stt.v3.StreamingOptions.speaker_labeling:type_name -> speechkit.stt.v3.SpeakerLabelingOptions
	25, // 25: speechkit.stt.v3.StreamingOptions.summarization:type_name -> speechkit.stt.v3.SummarizationOptions
	29, // 26: speechkit.stt.v3.StreamingRequest.session_options:type_name -> speechkit.stt.v3.StreamingOptions
	30, // 27: speechkit.stt.v3.StreamingRequest.chunk:type_name -> speechkit.stt.v3.AudioChunk
	31, // 28: speechkit.stt.v3.StreamingRequest.silence_chunk:type_name -> speechkit.stt.v3.SilenceChunk
	32, // 29: speechkit.stt.v3.StreamingRequest.eou:type_name -> speechkit.stt.v3.Eou
	27, // 30: speechkit.stt.v3.RecognizeFileRequest.recognition_model:type_name -> speechkit.stt.v3.RecognitionModelOptions
	17, // 31: speechkit.stt.v3.RecognizeFileRequest.recognition_classifier:type_name -> speechkit.stt.v3.RecognitionClassifierOptions
	18, // 32: speechkit.stt.v3.RecognizeFileRequest.speech_analysis:type_name -> speechkit.stt.v3.SpeechAnalysisOptions
	28, // 33: speechkit.stt.v3.RecognizeFileRequest.speaker_labeling:type_name -> speechkit.stt.v3.SpeakerLabelingOptions
	25, // 34: speechkit.stt.v3.RecognizeFileRequest.summarization:type_name -> speechkit.stt.v3.SummarizationOptions
	35, // 35: speechkit.stt.v3.Alternative.words:type_name -> speechkit.stt.v3.Word
	36, // 36: speechkit.stt.v3.Alternative.languages:type_name -> speechkit.stt.v3.LanguageEstimation
	37, // 37: speechkit.stt.v3.AlternativeUpdate.alternatives:type_name -> speechkit.stt.v3.Alternative
	39, // 38: speechkit.stt.v3.FinalRefinement.normalized_text:type_name -> speechkit.stt.v3.AlternativeUpdate
	0,  // 39: speechkit.stt.v3.StatusCode.code_type:type_name -> speechkit.stt.v3.CodeType
	44, // 40: speechkit.stt.v3.RecognitionClassifierResult.highlights:type_name -> speechkit.stt.v3.PhraseHighlight
	45, // 41: speechkit.stt.v3.RecognitionClassifierResult.labels:type_name -> speechkit.stt.v3.RecognitionClassifierLabel
	10, // 42: speechkit.stt.v3.RecognitionClassifierUpdate.window_type:type_name -> speechkit.stt.v3.RecognitionClassifierUpdate.WindowType
	46, // 43: speechkit.stt.v3.RecognitionClassifierUpdate.classifier_result:type_name -> speechkit.stt.v3.RecognitionClassifierResult
	57, // 44: speechkit.stt.v3.DescriptiveStatistics.quantiles:type_name -> speechkit.stt.v3.DescriptiveStatistics.Quantile
	11, // 45: speechkit.stt.v3.SpeakerAnalysis.window_type:type_name -> speechkit.stt.v3.SpeakerAnalysis.WindowType
	49, // 46: speechkit.stt.v3.SpeakerAnalysis.speech_boundaries:type_name -> speechkit.stt.v3.AudioSegmentBoundaries
	48, // 47: speechkit.stt.v3.SpeakerAnalysis.words_per_second:type_name -> speechkit.stt.v3.DescriptiveStatistics
	48, // 48: speechkit.stt.v3.SpeakerAnalysis.letters_per_second:type_name -> speechkit.stt.v3.DescriptiveStatistics
	48, // 49: speechkit.stt.v3.SpeakerAnalysis.words_per_utterance:type_name -> speechkit.stt.v3.DescriptiveStatistics
	48, // 50: speechkit.stt.v3.SpeakerAnalysis.letters_per_utterance:type_name -> speechkit.stt.v3.DescriptiveStatistics
	48, // 51: speechkit.stt.v3.SpeakerAnalysis.utterance_duration_estimation:type_name -> speechkit.stt.v3.DescriptiveStatistics
	49, // 52: speechkit.stt.v3.ConversationAnalysis.conversation_boundaries:type_name -> speechkit.stt.v3.AudioSegmentBoundaries
	48, // 53: speechkit.stt.v3.ConversationAnalysis.simultaneous_silence_duration_estimation:type_name -> speechkit.stt.v3.DescriptiveStatistics
	48, // 54: speechkit.stt.v3.ConversationAnalysis.simultaneous_speech_duration_estimation:type_name -> speechkit.stt.v3.DescriptiveStatistics
	58, // 55: speechkit.stt.v3.ConversationAnalysis.speaker_interrupts:type_name -> speechkit.stt.v3.ConversationAnalysis.InterruptsEvaluation
	26, // 56: speechkit.stt.v3.Summarization.results:type_name -> speechkit.stt.v3.SummarizationPropertyResult
	52, // 57: speechkit.stt.v3.Summarization.content_usage:type_name -> speechkit.stt.v3.ContentUsage
	43, // 58: speechkit.stt.v3.StreamingResponse.session_uuid:type_name -> speechkit.stt.v3.SessionUuid
	40, // 59: speechkit.stt.v3.StreamingResponse.audio_cursors:type_name -> speechkit.stt.v3.AudioCursors
	39, // 60: speechkit.stt.v3.StreamingResponse.partial:type_name -> speechkit.stt.v3.AlternativeUpdate
	39, // 61: speechkit.stt.v3.StreamingResponse.final:type_name -> speechkit.stt.v3.AlternativeUpdate
	38, // 62: speechkit.stt.v3.StreamingResponse.eou_update:type_name -> speechkit.stt.v3.EouUpdate
	41, // 63: speechkit.stt.v3.StreamingResponse.final_refinement:type_name -> speechkit.stt.v3.FinalRefinement
	42, // 64: speechkit.stt.v3.StreamingResponse.status_code:type_name -> speechkit.stt.v3.StatusCode
	47, // 65: speechkit.stt.v3.StreamingResponse.classifier_update:type_name -> speechkit.stt.v3.RecognitionClassifierUpdate
	50, // 66: speechkit.stt.v3.StreamingResponse.speaker_analysis:type_name -> speechkit.stt.v3.SpeakerAnalysis
	51, // 67: speechkit.stt.v3.StreamingResponse.conversation_analysis:type_name -> speechkit.stt.v3.ConversationAnalysis
	53, // 68: speechkit.stt.v3.StreamingResponse.summarization:type_name -> speechkit.stt.v3.Summarization
	54, // 69: speechkit.stt.v3.StreamingResponseList.streaming_responses:type_name -> speechkit.stt.v3.StreamingResponse
	49, // 70: speechkit.stt.v3.ConversationAnalysis.InterruptsEvaluation.interrupts:type_name -> speechkit.stt.v3.AudioSegmentBoundaries
	71, // [71:71] is the sub-list for method output_type
	71, // [71:71] is the sub-list for method input_type
	71, // [71:71] is the sub-list for extension type_name
	71, // [71:71] is the sub-list for extension extendee
	0,  // [0:71] is the sub-list for field type_name
}

func init() { file_yandex_cloud_ai_stt_v3_stt_proto_init() }
func file_yandex_cloud_ai_stt_v3_stt_proto_init() {
	if File_yandex_cloud_ai_stt_v3_stt_proto != nil {
		return
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[3].OneofWrappers = []any{
		(*EouClassifierOptions_DefaultClassifier)(nil),
		(*EouClassifierOptions_ExternalClassifier)(nil),
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[9].OneofWrappers = []any{
		(*AudioFormatOptions_RawAudio)(nil),
		(*AudioFormatOptions_ContainerAudio)(nil),
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[12].OneofWrappers = []any{
		(*SummarizationProperty_JsonObject)(nil),
		(*SummarizationProperty_JsonSchema)(nil),
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[21].OneofWrappers = []any{
		(*StreamingRequest_SessionOptions)(nil),
		(*StreamingRequest_Chunk)(nil),
		(*StreamingRequest_SilenceChunk)(nil),
		(*StreamingRequest_Eou)(nil),
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[22].OneofWrappers = []any{
		(*RecognizeFileRequest_Content)(nil),
		(*RecognizeFileRequest_Uri)(nil),
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[29].OneofWrappers = []any{
		(*FinalRefinement_NormalizedText)(nil),
	}
	file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes[42].OneofWrappers = []any{
		(*StreamingResponse_Partial)(nil),
		(*StreamingResponse_Final)(nil),
		(*StreamingResponse_EouUpdate)(nil),
		(*StreamingResponse_FinalRefinement)(nil),
		(*StreamingResponse_StatusCode)(nil),
		(*StreamingResponse_ClassifierUpdate)(nil),
		(*StreamingResponse_SpeakerAnalysis)(nil),
		(*StreamingResponse_ConversationAnalysis)(nil),
		(*StreamingResponse_Summarization)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_yandex_cloud_ai_stt_v3_stt_proto_rawDesc), len(file_yandex_cloud_ai_stt_v3_stt_proto_rawDesc)),
			NumEnums:      12,
			NumMessages:   47,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_yandex_cloud_ai_stt_v3_stt_proto_goTypes,
		DependencyIndexes: file_yandex_cloud_ai_stt_v3_stt_proto_depIdxs,
		EnumInfos:         file_yandex_cloud_ai_stt_v3_stt_proto_enumTypes,
		MessageInfos:      file_yandex_cloud_ai_stt_v3_stt_proto_msgTypes,
	}.Build()
	File_yandex_cloud_ai_stt_v3_stt_proto = out.File
	file_yandex_cloud_ai_stt_v3_stt_proto_goTypes = nil
	file_yandex_cloud_ai_stt_v3_stt_proto_depIdxs = nil
}

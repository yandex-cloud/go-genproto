// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v3.21.12
// source: yandex/cloud/spark/v1/job.proto

package spark

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

type Job_Status int32

const (
	Job_STATUS_UNSPECIFIED Job_Status = 0
	// Job created and is waiting to acquire.
	Job_PROVISIONING Job_Status = 1
	// Job acquired and is waiting for execution.
	Job_PENDING Job_Status = 2
	// Job is running.
	Job_RUNNING Job_Status = 3
	// Job failed.
	Job_ERROR Job_Status = 4
	// Job finished.
	Job_DONE Job_Status = 5
	// Job cancelled.
	Job_CANCELLED Job_Status = 6
	// Job is waiting for cancellation.
	Job_CANCELLING Job_Status = 7
)

// Enum value maps for Job_Status.
var (
	Job_Status_name = map[int32]string{
		0: "STATUS_UNSPECIFIED",
		1: "PROVISIONING",
		2: "PENDING",
		3: "RUNNING",
		4: "ERROR",
		5: "DONE",
		6: "CANCELLED",
		7: "CANCELLING",
	}
	Job_Status_value = map[string]int32{
		"STATUS_UNSPECIFIED": 0,
		"PROVISIONING":       1,
		"PENDING":            2,
		"RUNNING":            3,
		"ERROR":              4,
		"DONE":               5,
		"CANCELLED":          6,
		"CANCELLING":         7,
	}
)

func (x Job_Status) Enum() *Job_Status {
	p := new(Job_Status)
	*p = x
	return p
}

func (x Job_Status) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Job_Status) Descriptor() protoreflect.EnumDescriptor {
	return file_yandex_cloud_spark_v1_job_proto_enumTypes[0].Descriptor()
}

func (Job_Status) Type() protoreflect.EnumType {
	return &file_yandex_cloud_spark_v1_job_proto_enumTypes[0]
}

func (x Job_Status) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Job_Status.Descriptor instead.
func (Job_Status) EnumDescriptor() ([]byte, []int) {
	return file_yandex_cloud_spark_v1_job_proto_rawDescGZIP(), []int{0, 0}
}

// Spark job.
type Job struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. Unique ID of the Spark job.
	// This ID is assigned by MDB in the process of creating Spark job.
	Id string `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// Required. Unique ID of the Spark cluster.
	ClusterId string `protobuf:"bytes,2,opt,name=cluster_id,json=clusterId,proto3" json:"cluster_id,omitempty"`
	// The time when the Spark job was created.
	CreatedAt *timestamppb.Timestamp `protobuf:"bytes,3,opt,name=created_at,json=createdAt,proto3" json:"created_at,omitempty"`
	// The time when the Spark job was started.
	StartedAt *timestamppb.Timestamp `protobuf:"bytes,4,opt,name=started_at,json=startedAt,proto3" json:"started_at,omitempty"`
	// The time when the Spark job was finished.
	FinishedAt *timestamppb.Timestamp `protobuf:"bytes,5,opt,name=finished_at,json=finishedAt,proto3" json:"finished_at,omitempty"`
	// Name of the Spark job.
	Name string `protobuf:"bytes,6,opt,name=name,proto3" json:"name,omitempty"`
	// The id of the user who created the job
	CreatedBy string `protobuf:"bytes,7,opt,name=created_by,json=createdBy,proto3" json:"created_by,omitempty"`
	// Status.
	Status Job_Status `protobuf:"varint,8,opt,name=status,proto3,enum=yandex.cloud.spark.v1.Job_Status" json:"status,omitempty"`
	// Job specification.
	//
	// Types that are valid to be assigned to JobSpec:
	//
	//	*Job_SparkJob
	//	*Job_PysparkJob
	//	*Job_SparkConnectJob
	JobSpec isJob_JobSpec `protobuf_oneof:"job_spec"`
	// Spark UI Url.
	UiUrl string `protobuf:"bytes,12,opt,name=ui_url,json=uiUrl,proto3" json:"ui_url,omitempty"`
	// Service account used to access Cloud resources.
	ServiceAccountId string `protobuf:"bytes,13,opt,name=service_account_id,json=serviceAccountId,proto3" json:"service_account_id,omitempty"`
	// Spark Connect Url.
	ConnectUrl    string `protobuf:"bytes,14,opt,name=connect_url,json=connectUrl,proto3" json:"connect_url,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Job) Reset() {
	*x = Job{}
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Job) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Job) ProtoMessage() {}

func (x *Job) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Job.ProtoReflect.Descriptor instead.
func (*Job) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_spark_v1_job_proto_rawDescGZIP(), []int{0}
}

func (x *Job) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *Job) GetClusterId() string {
	if x != nil {
		return x.ClusterId
	}
	return ""
}

func (x *Job) GetCreatedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.CreatedAt
	}
	return nil
}

func (x *Job) GetStartedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.StartedAt
	}
	return nil
}

func (x *Job) GetFinishedAt() *timestamppb.Timestamp {
	if x != nil {
		return x.FinishedAt
	}
	return nil
}

func (x *Job) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *Job) GetCreatedBy() string {
	if x != nil {
		return x.CreatedBy
	}
	return ""
}

func (x *Job) GetStatus() Job_Status {
	if x != nil {
		return x.Status
	}
	return Job_STATUS_UNSPECIFIED
}

func (x *Job) GetJobSpec() isJob_JobSpec {
	if x != nil {
		return x.JobSpec
	}
	return nil
}

func (x *Job) GetSparkJob() *SparkJob {
	if x != nil {
		if x, ok := x.JobSpec.(*Job_SparkJob); ok {
			return x.SparkJob
		}
	}
	return nil
}

func (x *Job) GetPysparkJob() *PysparkJob {
	if x != nil {
		if x, ok := x.JobSpec.(*Job_PysparkJob); ok {
			return x.PysparkJob
		}
	}
	return nil
}

func (x *Job) GetSparkConnectJob() *SparkConnectJob {
	if x != nil {
		if x, ok := x.JobSpec.(*Job_SparkConnectJob); ok {
			return x.SparkConnectJob
		}
	}
	return nil
}

func (x *Job) GetUiUrl() string {
	if x != nil {
		return x.UiUrl
	}
	return ""
}

func (x *Job) GetServiceAccountId() string {
	if x != nil {
		return x.ServiceAccountId
	}
	return ""
}

func (x *Job) GetConnectUrl() string {
	if x != nil {
		return x.ConnectUrl
	}
	return ""
}

type isJob_JobSpec interface {
	isJob_JobSpec()
}

type Job_SparkJob struct {
	SparkJob *SparkJob `protobuf:"bytes,9,opt,name=spark_job,json=sparkJob,proto3,oneof"`
}

type Job_PysparkJob struct {
	PysparkJob *PysparkJob `protobuf:"bytes,10,opt,name=pyspark_job,json=pysparkJob,proto3,oneof"`
}

type Job_SparkConnectJob struct {
	SparkConnectJob *SparkConnectJob `protobuf:"bytes,20,opt,name=spark_connect_job,json=sparkConnectJob,proto3,oneof"`
}

func (*Job_SparkJob) isJob_JobSpec() {}

func (*Job_PysparkJob) isJob_JobSpec() {}

func (*Job_SparkConnectJob) isJob_JobSpec() {}

type SparkJob struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional arguments to pass to the driver.
	Args []string `protobuf:"bytes,1,rep,name=args,proto3" json:"args,omitempty"`
	// Jar file URIs to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `protobuf:"bytes,2,rep,name=jar_file_uris,json=jarFileUris,proto3" json:"jar_file_uris,omitempty"`
	// URIs of files to be copied to the working directory of Spark drivers and distributed tasks.
	FileUris []string `protobuf:"bytes,3,rep,name=file_uris,json=fileUris,proto3" json:"file_uris,omitempty"`
	// URIs of archives to be extracted in the working directory of Spark drivers and tasks.
	ArchiveUris []string `protobuf:"bytes,4,rep,name=archive_uris,json=archiveUris,proto3" json:"archive_uris,omitempty"`
	// A mapping of property names to values, used to configure Spark.
	Properties map[string]string `protobuf:"bytes,5,rep,name=properties,proto3" json:"properties,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// URI of the jar file containing the main class.
	MainJarFileUri string `protobuf:"bytes,6,opt,name=main_jar_file_uri,json=mainJarFileUri,proto3" json:"main_jar_file_uri,omitempty"`
	// The name of the driver's main class.
	MainClass string `protobuf:"bytes,7,opt,name=main_class,json=mainClass,proto3" json:"main_class,omitempty"`
	// List of maven coordinates of jars to include on the driver and executor classpaths.
	Packages []string `protobuf:"bytes,8,rep,name=packages,proto3" json:"packages,omitempty"`
	// List of additional remote repositories to search for the maven coordinates given with --packages.
	Repositories []string `protobuf:"bytes,9,rep,name=repositories,proto3" json:"repositories,omitempty"`
	// List of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.
	ExcludePackages []string `protobuf:"bytes,10,rep,name=exclude_packages,json=excludePackages,proto3" json:"exclude_packages,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *SparkJob) Reset() {
	*x = SparkJob{}
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SparkJob) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SparkJob) ProtoMessage() {}

func (x *SparkJob) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SparkJob.ProtoReflect.Descriptor instead.
func (*SparkJob) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_spark_v1_job_proto_rawDescGZIP(), []int{1}
}

func (x *SparkJob) GetArgs() []string {
	if x != nil {
		return x.Args
	}
	return nil
}

func (x *SparkJob) GetJarFileUris() []string {
	if x != nil {
		return x.JarFileUris
	}
	return nil
}

func (x *SparkJob) GetFileUris() []string {
	if x != nil {
		return x.FileUris
	}
	return nil
}

func (x *SparkJob) GetArchiveUris() []string {
	if x != nil {
		return x.ArchiveUris
	}
	return nil
}

func (x *SparkJob) GetProperties() map[string]string {
	if x != nil {
		return x.Properties
	}
	return nil
}

func (x *SparkJob) GetMainJarFileUri() string {
	if x != nil {
		return x.MainJarFileUri
	}
	return ""
}

func (x *SparkJob) GetMainClass() string {
	if x != nil {
		return x.MainClass
	}
	return ""
}

func (x *SparkJob) GetPackages() []string {
	if x != nil {
		return x.Packages
	}
	return nil
}

func (x *SparkJob) GetRepositories() []string {
	if x != nil {
		return x.Repositories
	}
	return nil
}

func (x *SparkJob) GetExcludePackages() []string {
	if x != nil {
		return x.ExcludePackages
	}
	return nil
}

type SparkConnectJob struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Jar file URIs to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `protobuf:"bytes,1,rep,name=jar_file_uris,json=jarFileUris,proto3" json:"jar_file_uris,omitempty"`
	// URIs of files to be copied to the working directory of Spark drivers and distributed tasks.
	FileUris []string `protobuf:"bytes,2,rep,name=file_uris,json=fileUris,proto3" json:"file_uris,omitempty"`
	// URIs of archives to be extracted in the working directory of Spark drivers and tasks.
	ArchiveUris []string `protobuf:"bytes,3,rep,name=archive_uris,json=archiveUris,proto3" json:"archive_uris,omitempty"`
	// A mapping of property names to values, used to configure Spark.
	Properties map[string]string `protobuf:"bytes,4,rep,name=properties,proto3" json:"properties,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// List of maven coordinates of jars to include on the driver and executor classpaths.
	Packages []string `protobuf:"bytes,5,rep,name=packages,proto3" json:"packages,omitempty"`
	// List of additional remote repositories to search for the maven coordinates given with --packages.
	Repositories []string `protobuf:"bytes,6,rep,name=repositories,proto3" json:"repositories,omitempty"`
	// List of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.
	ExcludePackages []string `protobuf:"bytes,7,rep,name=exclude_packages,json=excludePackages,proto3" json:"exclude_packages,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *SparkConnectJob) Reset() {
	*x = SparkConnectJob{}
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SparkConnectJob) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SparkConnectJob) ProtoMessage() {}

func (x *SparkConnectJob) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SparkConnectJob.ProtoReflect.Descriptor instead.
func (*SparkConnectJob) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_spark_v1_job_proto_rawDescGZIP(), []int{2}
}

func (x *SparkConnectJob) GetJarFileUris() []string {
	if x != nil {
		return x.JarFileUris
	}
	return nil
}

func (x *SparkConnectJob) GetFileUris() []string {
	if x != nil {
		return x.FileUris
	}
	return nil
}

func (x *SparkConnectJob) GetArchiveUris() []string {
	if x != nil {
		return x.ArchiveUris
	}
	return nil
}

func (x *SparkConnectJob) GetProperties() map[string]string {
	if x != nil {
		return x.Properties
	}
	return nil
}

func (x *SparkConnectJob) GetPackages() []string {
	if x != nil {
		return x.Packages
	}
	return nil
}

func (x *SparkConnectJob) GetRepositories() []string {
	if x != nil {
		return x.Repositories
	}
	return nil
}

func (x *SparkConnectJob) GetExcludePackages() []string {
	if x != nil {
		return x.ExcludePackages
	}
	return nil
}

type PysparkJob struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional arguments to pass to the driver.
	Args []string `protobuf:"bytes,1,rep,name=args,proto3" json:"args,omitempty"`
	// Jar file URIs to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `protobuf:"bytes,2,rep,name=jar_file_uris,json=jarFileUris,proto3" json:"jar_file_uris,omitempty"`
	// URIs of files to be copied to the working directory of Spark drivers and distributed tasks.
	FileUris []string `protobuf:"bytes,3,rep,name=file_uris,json=fileUris,proto3" json:"file_uris,omitempty"`
	// URIs of archives to be extracted in the working directory of Spark drivers and tasks.
	ArchiveUris []string `protobuf:"bytes,4,rep,name=archive_uris,json=archiveUris,proto3" json:"archive_uris,omitempty"`
	// A mapping of property names to values, used to configure Spark.
	Properties map[string]string `protobuf:"bytes,5,rep,name=properties,proto3" json:"properties,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri string `protobuf:"bytes,6,opt,name=main_python_file_uri,json=mainPythonFileUri,proto3" json:"main_python_file_uri,omitempty"`
	// URIs of Python files to pass to the PySpark framework.
	PythonFileUris []string `protobuf:"bytes,7,rep,name=python_file_uris,json=pythonFileUris,proto3" json:"python_file_uris,omitempty"`
	// List of maven coordinates of jars to include on the driver and executor classpaths.
	Packages []string `protobuf:"bytes,8,rep,name=packages,proto3" json:"packages,omitempty"`
	// List of additional remote repositories to search for the maven coordinates given with --packages.
	Repositories []string `protobuf:"bytes,9,rep,name=repositories,proto3" json:"repositories,omitempty"`
	// List of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.
	ExcludePackages []string `protobuf:"bytes,10,rep,name=exclude_packages,json=excludePackages,proto3" json:"exclude_packages,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *PysparkJob) Reset() {
	*x = PysparkJob{}
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PysparkJob) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PysparkJob) ProtoMessage() {}

func (x *PysparkJob) ProtoReflect() protoreflect.Message {
	mi := &file_yandex_cloud_spark_v1_job_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PysparkJob.ProtoReflect.Descriptor instead.
func (*PysparkJob) Descriptor() ([]byte, []int) {
	return file_yandex_cloud_spark_v1_job_proto_rawDescGZIP(), []int{3}
}

func (x *PysparkJob) GetArgs() []string {
	if x != nil {
		return x.Args
	}
	return nil
}

func (x *PysparkJob) GetJarFileUris() []string {
	if x != nil {
		return x.JarFileUris
	}
	return nil
}

func (x *PysparkJob) GetFileUris() []string {
	if x != nil {
		return x.FileUris
	}
	return nil
}

func (x *PysparkJob) GetArchiveUris() []string {
	if x != nil {
		return x.ArchiveUris
	}
	return nil
}

func (x *PysparkJob) GetProperties() map[string]string {
	if x != nil {
		return x.Properties
	}
	return nil
}

func (x *PysparkJob) GetMainPythonFileUri() string {
	if x != nil {
		return x.MainPythonFileUri
	}
	return ""
}

func (x *PysparkJob) GetPythonFileUris() []string {
	if x != nil {
		return x.PythonFileUris
	}
	return nil
}

func (x *PysparkJob) GetPackages() []string {
	if x != nil {
		return x.Packages
	}
	return nil
}

func (x *PysparkJob) GetRepositories() []string {
	if x != nil {
		return x.Repositories
	}
	return nil
}

func (x *PysparkJob) GetExcludePackages() []string {
	if x != nil {
		return x.ExcludePackages
	}
	return nil
}

var File_yandex_cloud_spark_v1_job_proto protoreflect.FileDescriptor

const file_yandex_cloud_spark_v1_job_proto_rawDesc = "" +
	"\n" +
	"\x1fyandex/cloud/spark/v1/job.proto\x12\x15yandex.cloud.spark.v1\x1a\x1fgoogle/protobuf/timestamp.proto\"\xb2\x06\n" +
	"\x03Job\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x1d\n" +
	"\n" +
	"cluster_id\x18\x02 \x01(\tR\tclusterId\x129\n" +
	"\n" +
	"created_at\x18\x03 \x01(\v2\x1a.google.protobuf.TimestampR\tcreatedAt\x129\n" +
	"\n" +
	"started_at\x18\x04 \x01(\v2\x1a.google.protobuf.TimestampR\tstartedAt\x12;\n" +
	"\vfinished_at\x18\x05 \x01(\v2\x1a.google.protobuf.TimestampR\n" +
	"finishedAt\x12\x12\n" +
	"\x04name\x18\x06 \x01(\tR\x04name\x12\x1d\n" +
	"\n" +
	"created_by\x18\a \x01(\tR\tcreatedBy\x129\n" +
	"\x06status\x18\b \x01(\x0e2!.yandex.cloud.spark.v1.Job.StatusR\x06status\x12>\n" +
	"\tspark_job\x18\t \x01(\v2\x1f.yandex.cloud.spark.v1.SparkJobH\x00R\bsparkJob\x12D\n" +
	"\vpyspark_job\x18\n" +
	" \x01(\v2!.yandex.cloud.spark.v1.PysparkJobH\x00R\n" +
	"pysparkJob\x12T\n" +
	"\x11spark_connect_job\x18\x14 \x01(\v2&.yandex.cloud.spark.v1.SparkConnectJobH\x00R\x0fsparkConnectJob\x12\x15\n" +
	"\x06ui_url\x18\f \x01(\tR\x05uiUrl\x12,\n" +
	"\x12service_account_id\x18\r \x01(\tR\x10serviceAccountId\x12\x1f\n" +
	"\vconnect_url\x18\x0e \x01(\tR\n" +
	"connectUrl\"\x80\x01\n" +
	"\x06Status\x12\x16\n" +
	"\x12STATUS_UNSPECIFIED\x10\x00\x12\x10\n" +
	"\fPROVISIONING\x10\x01\x12\v\n" +
	"\aPENDING\x10\x02\x12\v\n" +
	"\aRUNNING\x10\x03\x12\t\n" +
	"\x05ERROR\x10\x04\x12\b\n" +
	"\x04DONE\x10\x05\x12\r\n" +
	"\tCANCELLED\x10\x06\x12\x0e\n" +
	"\n" +
	"CANCELLING\x10\aB\n" +
	"\n" +
	"\bjob_specJ\x04\b\v\x10\fJ\x04\b\x0f\x10\x14\"\xc7\x03\n" +
	"\bSparkJob\x12\x12\n" +
	"\x04args\x18\x01 \x03(\tR\x04args\x12\"\n" +
	"\rjar_file_uris\x18\x02 \x03(\tR\vjarFileUris\x12\x1b\n" +
	"\tfile_uris\x18\x03 \x03(\tR\bfileUris\x12!\n" +
	"\farchive_uris\x18\x04 \x03(\tR\varchiveUris\x12O\n" +
	"\n" +
	"properties\x18\x05 \x03(\v2/.yandex.cloud.spark.v1.SparkJob.PropertiesEntryR\n" +
	"properties\x12)\n" +
	"\x11main_jar_file_uri\x18\x06 \x01(\tR\x0emainJarFileUri\x12\x1d\n" +
	"\n" +
	"main_class\x18\a \x01(\tR\tmainClass\x12\x1a\n" +
	"\bpackages\x18\b \x03(\tR\bpackages\x12\"\n" +
	"\frepositories\x18\t \x03(\tR\frepositories\x12)\n" +
	"\x10exclude_packages\x18\n" +
	" \x03(\tR\x0fexcludePackages\x1a=\n" +
	"\x0fPropertiesEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xf7\x02\n" +
	"\x0fSparkConnectJob\x12\"\n" +
	"\rjar_file_uris\x18\x01 \x03(\tR\vjarFileUris\x12\x1b\n" +
	"\tfile_uris\x18\x02 \x03(\tR\bfileUris\x12!\n" +
	"\farchive_uris\x18\x03 \x03(\tR\varchiveUris\x12V\n" +
	"\n" +
	"properties\x18\x04 \x03(\v26.yandex.cloud.spark.v1.SparkConnectJob.PropertiesEntryR\n" +
	"properties\x12\x1a\n" +
	"\bpackages\x18\x05 \x03(\tR\bpackages\x12\"\n" +
	"\frepositories\x18\x06 \x03(\tR\frepositories\x12)\n" +
	"\x10exclude_packages\x18\a \x03(\tR\x0fexcludePackages\x1a=\n" +
	"\x0fPropertiesEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01\"\xdc\x03\n" +
	"\n" +
	"PysparkJob\x12\x12\n" +
	"\x04args\x18\x01 \x03(\tR\x04args\x12\"\n" +
	"\rjar_file_uris\x18\x02 \x03(\tR\vjarFileUris\x12\x1b\n" +
	"\tfile_uris\x18\x03 \x03(\tR\bfileUris\x12!\n" +
	"\farchive_uris\x18\x04 \x03(\tR\varchiveUris\x12Q\n" +
	"\n" +
	"properties\x18\x05 \x03(\v21.yandex.cloud.spark.v1.PysparkJob.PropertiesEntryR\n" +
	"properties\x12/\n" +
	"\x14main_python_file_uri\x18\x06 \x01(\tR\x11mainPythonFileUri\x12(\n" +
	"\x10python_file_uris\x18\a \x03(\tR\x0epythonFileUris\x12\x1a\n" +
	"\bpackages\x18\b \x03(\tR\bpackages\x12\"\n" +
	"\frepositories\x18\t \x03(\tR\frepositories\x12)\n" +
	"\x10exclude_packages\x18\n" +
	" \x03(\tR\x0fexcludePackages\x1a=\n" +
	"\x0fPropertiesEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01B\\\n" +
	"\x19yandex.cloud.api.spark.v1Z?github.com/yandex-cloud/go-genproto/yandex/cloud/spark/v1;sparkb\x06proto3"

var (
	file_yandex_cloud_spark_v1_job_proto_rawDescOnce sync.Once
	file_yandex_cloud_spark_v1_job_proto_rawDescData []byte
)

func file_yandex_cloud_spark_v1_job_proto_rawDescGZIP() []byte {
	file_yandex_cloud_spark_v1_job_proto_rawDescOnce.Do(func() {
		file_yandex_cloud_spark_v1_job_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_yandex_cloud_spark_v1_job_proto_rawDesc), len(file_yandex_cloud_spark_v1_job_proto_rawDesc)))
	})
	return file_yandex_cloud_spark_v1_job_proto_rawDescData
}

var file_yandex_cloud_spark_v1_job_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_yandex_cloud_spark_v1_job_proto_msgTypes = make([]protoimpl.MessageInfo, 7)
var file_yandex_cloud_spark_v1_job_proto_goTypes = []any{
	(Job_Status)(0),               // 0: yandex.cloud.spark.v1.Job.Status
	(*Job)(nil),                   // 1: yandex.cloud.spark.v1.Job
	(*SparkJob)(nil),              // 2: yandex.cloud.spark.v1.SparkJob
	(*SparkConnectJob)(nil),       // 3: yandex.cloud.spark.v1.SparkConnectJob
	(*PysparkJob)(nil),            // 4: yandex.cloud.spark.v1.PysparkJob
	nil,                           // 5: yandex.cloud.spark.v1.SparkJob.PropertiesEntry
	nil,                           // 6: yandex.cloud.spark.v1.SparkConnectJob.PropertiesEntry
	nil,                           // 7: yandex.cloud.spark.v1.PysparkJob.PropertiesEntry
	(*timestamppb.Timestamp)(nil), // 8: google.protobuf.Timestamp
}
var file_yandex_cloud_spark_v1_job_proto_depIdxs = []int32{
	8,  // 0: yandex.cloud.spark.v1.Job.created_at:type_name -> google.protobuf.Timestamp
	8,  // 1: yandex.cloud.spark.v1.Job.started_at:type_name -> google.protobuf.Timestamp
	8,  // 2: yandex.cloud.spark.v1.Job.finished_at:type_name -> google.protobuf.Timestamp
	0,  // 3: yandex.cloud.spark.v1.Job.status:type_name -> yandex.cloud.spark.v1.Job.Status
	2,  // 4: yandex.cloud.spark.v1.Job.spark_job:type_name -> yandex.cloud.spark.v1.SparkJob
	4,  // 5: yandex.cloud.spark.v1.Job.pyspark_job:type_name -> yandex.cloud.spark.v1.PysparkJob
	3,  // 6: yandex.cloud.spark.v1.Job.spark_connect_job:type_name -> yandex.cloud.spark.v1.SparkConnectJob
	5,  // 7: yandex.cloud.spark.v1.SparkJob.properties:type_name -> yandex.cloud.spark.v1.SparkJob.PropertiesEntry
	6,  // 8: yandex.cloud.spark.v1.SparkConnectJob.properties:type_name -> yandex.cloud.spark.v1.SparkConnectJob.PropertiesEntry
	7,  // 9: yandex.cloud.spark.v1.PysparkJob.properties:type_name -> yandex.cloud.spark.v1.PysparkJob.PropertiesEntry
	10, // [10:10] is the sub-list for method output_type
	10, // [10:10] is the sub-list for method input_type
	10, // [10:10] is the sub-list for extension type_name
	10, // [10:10] is the sub-list for extension extendee
	0,  // [0:10] is the sub-list for field type_name
}

func init() { file_yandex_cloud_spark_v1_job_proto_init() }
func file_yandex_cloud_spark_v1_job_proto_init() {
	if File_yandex_cloud_spark_v1_job_proto != nil {
		return
	}
	file_yandex_cloud_spark_v1_job_proto_msgTypes[0].OneofWrappers = []any{
		(*Job_SparkJob)(nil),
		(*Job_PysparkJob)(nil),
		(*Job_SparkConnectJob)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_yandex_cloud_spark_v1_job_proto_rawDesc), len(file_yandex_cloud_spark_v1_job_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   7,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_yandex_cloud_spark_v1_job_proto_goTypes,
		DependencyIndexes: file_yandex_cloud_spark_v1_job_proto_depIdxs,
		EnumInfos:         file_yandex_cloud_spark_v1_job_proto_enumTypes,
		MessageInfos:      file_yandex_cloud_spark_v1_job_proto_msgTypes,
	}.Build()
	File_yandex_cloud_spark_v1_job_proto = out.File
	file_yandex_cloud_spark_v1_job_proto_goTypes = nil
	file_yandex_cloud_spark_v1_job_proto_depIdxs = nil
}
